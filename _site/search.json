[
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html",
    "href": "Take-home_Exe/Take-home_Ex_01.html",
    "title": "Take-Home Exercise 01",
    "section": "",
    "text": "This quota document shows the visual analytics conducted for the Take Home Exercise 01, The data set chosen is the Heart Attack in Japan Youth Vs Adult from Kaggle."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#tasks-required",
    "href": "Take-home_Exe/Take-home_Ex_01.html#tasks-required",
    "title": "Take-Home Exercise 01",
    "section": "Tasks Required",
    "text": "Tasks Required\nThe main task is to develop graphical visuals for a media company on an article for the possible factors leading to heart attack trends in Japanese Youth.\nThe plan is to carry out exploratory and confirmatory data analysis to confirm the observed trend before doing up the visuals required for the graphic.\nThe goal is to find out if there are any observable trends and health or lifestyle factors leading to the occurrence of heart attack in Japanese youth.\nFor the purpose of this analysis, I define Japanese youth as individuals under the age of 35, following the definition provided in the article on Youth Employment Policies in Japan. The data set is catogrised into 4 main groups as follows:\n\nYouths who have experienced a heart attack (interest group),\nYouths who have not experienced a heart attack,\nAdults who have experienced a heart attack,\nAdults who have not experienced a heart attack.\n\nPopulation with age above 65, they will be omitted from this study as age above 65 and above are considered as elderly based on the Wikipedia."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#loading-of-packages",
    "href": "Take-home_Exe/Take-home_Ex_01.html#loading-of-packages",
    "title": "Take-Home Exercise 01",
    "section": "1.0 Loading of Packages",
    "text": "1.0 Loading of Packages\nThe following code chunk indicates the list of packages used for this Take Home Exercise 01.\n\npacman::p_load(haven, SmartEDA, tidyverse, tidymodels, ggdist, ggridges, ggthemes,\n               colorspace, gridExtra, ggstatsplot, GGally, \n               readxl, performance, parameters, car,aod, ggExtra, ggiraph, corrplot)"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#loading-of-data",
    "href": "Take-home_Exe/Take-home_Ex_01.html#loading-of-data",
    "title": "Take-Home Exercise 01",
    "section": "",
    "text": "As the data is in csv format, the data is loaded using read_csv function.\n\nheart &lt;- read_csv(\"data/Ex01/japan_heart_attack_dataset.csv\")\n\n\n\nsummary and head functions are used to view and examin the data type before further processing is carried out.\n\nsummary(heart)\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:30000       Length:30000       Length:30000      \n 1st Qu.:33.00   Class :character   Class :character   Class :character  \n Median :48.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :48.49                                                           \n 3rd Qu.:64.00                                                           \n Max.   :79.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:30000       Length:30000         Min.   : 80.02    Length:30000      \n Class :character   Class :character     1st Qu.:179.55    Class :character  \n Mode  :character   Mode  :character     Median :199.77    Mode  :character  \n                                         Mean   :199.90                      \n                                         3rd Qu.:220.16                      \n                                         Max.   :336.86                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:30000       Length:30000        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.644   1st Qu.:21.63  \n Mode  :character   Mode  :character    Median : 4.993   Median :24.96  \n                                        Mean   : 5.002   Mean   :25.00  \n                                        3rd Qu.: 6.353   3rd Qu.:28.36  \n                                        Max.   :10.000   Max.   :46.10  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 30.03   Min.   : 56.23   Min.   : 39.95   Length:30000      \n 1st Qu.: 63.25   1st Qu.:109.79   1st Qu.: 73.26   Class :character  \n Median : 69.95   Median :119.90   Median : 80.12   Mode  :character  \n Mean   : 69.98   Mean   :119.91   Mean   : 80.03                     \n 3rd Qu.: 76.66   3rd Qu.:130.02   3rd Qu.: 86.76                     \n Max.   :108.78   Max.   :178.77   Max.   :117.67                     \n Heart_Attack_Occurrence Extra_Column_1     Extra_Column_2     \n Length:30000            Min.   :0.000007   Min.   :0.0000052  \n Class :character        1st Qu.:0.253308   1st Qu.:0.2473606  \n Mode  :character        Median :0.500820   Median :0.4961980  \n                         Mean   :0.501939   Mean   :0.4978940  \n                         3rd Qu.:0.750529   3rd Qu.:0.7473954  \n                         Max.   :0.999965   Max.   :0.9999894  \n Extra_Column_3      Extra_Column_4      Extra_Column_5     Extra_Column_6     \n Min.   :0.0000227   Min.   :0.0000934   Min.   :0.000105   Min.   :0.0000531  \n 1st Qu.:0.2483093   1st Qu.:0.2522110   1st Qu.:0.251803   1st Qu.:0.2559989  \n Median :0.4976104   Median :0.4976175   Median :0.501987   Median :0.5017726  \n Mean   :0.4981949   Mean   :0.5005952   Mean   :0.501410   Mean   :0.5027631  \n 3rd Qu.:0.7476807   3rd Qu.:0.7505662   3rd Qu.:0.753657   3rd Qu.:0.7511886  \n Max.   :0.9999694   Max.   :0.9999869   Max.   :0.999995   Max.   :0.9998892  \n Extra_Column_7      Extra_Column_8      Extra_Column_9     \n Min.   :0.0000678   Min.   :0.0000449   Min.   :0.0000305  \n 1st Qu.:0.2482839   1st Qu.:0.2509790   1st Qu.:0.2502452  \n Median :0.4988157   Median :0.4985698   Median :0.4984491  \n Mean   :0.4980753   Mean   :0.5003557   Mean   :0.5002292  \n 3rd Qu.:0.7456378   3rd Qu.:0.7507293   3rd Qu.:0.7512186  \n Max.   :0.9999900   Max.   :0.9999300   Max.   :0.9999852  \n Extra_Column_10     Extra_Column_11     Extra_Column_12    \n Min.   :0.0000133   Min.   :0.0000008   Min.   :0.0000713  \n 1st Qu.:0.2484256   1st Qu.:0.2538092   1st Qu.:0.2505341  \n Median :0.5031040   Median :0.5067589   Median :0.5038609  \n Mean   :0.5010694   Mean   :0.5044949   Mean   :0.5008624  \n 3rd Qu.:0.7522686   3rd Qu.:0.7556257   3rd Qu.:0.7511780  \n Max.   :0.9999928   Max.   :0.9999578   Max.   :0.9999484  \n Extra_Column_13     Extra_Column_14     Extra_Column_15    \n Min.   :0.0000204   Min.   :0.0000025   Min.   :0.0000241  \n 1st Qu.:0.2473108   1st Qu.:0.2482152   1st Qu.:0.2482573  \n Median :0.5041162   Median :0.4943841   Median :0.5009406  \n Mean   :0.5004557   Mean   :0.4976507   Mean   :0.4999634  \n 3rd Qu.:0.7497094   3rd Qu.:0.7456212   3rd Qu.:0.7487379  \n Max.   :0.9999451   Max.   :0.9999779   Max.   :0.9999913  \n\n\n\nhead (heart)\n\n# A tibble: 6 × 32\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    69 Male   Urban  No              No               No                  \n3    46 Male   Rural  Yes             No               No                  \n4    32 Female Urban  No              No               No                  \n5    60 Female Rural  No              No               No                  \n6    25 Female Rural  No              No               No                  \n# ℹ 26 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;,\n#   Extra_Column_5 &lt;dbl&gt;, Extra_Column_6 &lt;dbl&gt;, Extra_Column_7 &lt;dbl&gt;,\n#   Extra_Column_8 &lt;dbl&gt;, Extra_Column_9 &lt;dbl&gt;, Extra_Column_10 &lt;dbl&gt;, …\n\n\n\n\n\n\ncolSums(is.na(heart))\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence          Extra_Column_1 \n                      0                       0                       0 \n         Extra_Column_2          Extra_Column_3          Extra_Column_4 \n                      0                       0                       0 \n         Extra_Column_5          Extra_Column_6          Extra_Column_7 \n                      0                       0                       0 \n         Extra_Column_8          Extra_Column_9         Extra_Column_10 \n                      0                       0                       0 \n        Extra_Column_11         Extra_Column_12         Extra_Column_13 \n                      0                       0                       0 \n        Extra_Column_14         Extra_Column_15 \n                      0                       0 \n\n\nAs the metadata did not specify the data is of the extra columns hence we will not be able the data in the “Extra_column 1 to 15”\nThe first step of data preparation is to remove them first.\n\nHA &lt;- heart %&gt;% select (1:17) %&gt;% filter (Age &lt; 64)\n\nAfter removing the Extra columns, we continue by examining the data using the summary and head (HA) function.\n\nsummary(HA)\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:22158       Length:22158       Length:22158      \n 1st Qu.:29.00   Class :character   Class :character   Class :character  \n Median :40.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :40.37                                                           \n 3rd Qu.:52.00                                                           \n Max.   :63.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:22158       Length:22158         Min.   : 80.02    Length:22158      \n Class :character   Class :character     1st Qu.:179.50    Class :character  \n Mode  :character   Mode  :character     Median :199.78    Mode  :character  \n                                         Mean   :199.90                      \n                                         3rd Qu.:220.15                      \n                                         Max.   :311.24                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:22158       Length:22158        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.636   1st Qu.:21.63  \n Mode  :character   Mode  :character    Median : 4.973   Median :24.96  \n                                        Mean   : 4.987   Mean   :25.00  \n                                        3rd Qu.: 6.336   3rd Qu.:28.39  \n                                        Max.   :10.000   Max.   :44.12  \n   Heart_Rate      Systolic_BP     Diastolic_BP    Family_History    \n Min.   : 30.59   Min.   : 63.1   Min.   : 39.95   Length:22158      \n 1st Qu.: 63.22   1st Qu.:109.8   1st Qu.: 73.24   Class :character  \n Median : 69.92   Median :119.9   Median : 80.10   Mode  :character  \n Mean   : 69.94   Mean   :120.0   Mean   : 80.03                     \n 3rd Qu.: 76.64   3rd Qu.:130.2   3rd Qu.: 86.76                     \n Max.   :108.78   Max.   :176.6   Max.   :117.66                     \n Heart_Attack_Occurrence\n Length:22158           \n Class :character       \n Mode  :character       \n                        \n                        \n                        \n\n\n\nhead(HA)\n\n# A tibble: 6 × 17\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    46 Male   Rural  Yes             No               No                  \n3    32 Female Urban  No              No               No                  \n4    60 Female Rural  No              No               No                  \n5    25 Female Rural  No              No               No                  \n6    38 Female Urban  Yes             No               No                  \n# ℹ 11 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;\n\n\n\n\nIt was observed that the data consist two main types: namely continous and categorical. The following steps we will examine the distibution of the each type of data in details.\n\nHA &lt;- HA %&gt;%\n  mutate(Age_Grp = ifelse(Age &gt; 35, \"Adult\", \"Youth\")) %&gt;%\n  mutate(Event_Grp = ifelse(Age_Grp == \"Adult\" & Heart_Attack_Occurrence == \"Yes\", \"Adult_HA\",\n                    ifelse(Age_Grp == \"Adult\" & Heart_Attack_Occurrence == \"No\", \"Adult_No_HA\",\n                    ifelse(Age_Grp == \"Youth\" & Heart_Attack_Occurrence == \"Yes\", \"Youth_HA\",\n                    ifelse(Age_Grp == \"Youth\" & Heart_Attack_Occurrence == \"No\", \"Youth_No_HA\",NA)))))\n\nHA &lt;- HA %&gt;%\n  mutate(BMI_cat = case_when( \n    BMI &lt; 18.5 ~ \"Underweight\",\n    BMI &gt;= 18.5 & BMI &lt; 24.9 ~ \"Normal\",\n    BMI &gt;= 25 & BMI &lt; 29.9 ~ \"Overweight\",\n    BMI &gt;= 30 ~ \"Obese\",\n    TRUE ~ NA_character_\n  ))\n\nHA &lt;- HA %&gt;%\n  mutate(BP_cat = case_when(\n    Systolic_BP &lt; 120 & Diastolic_BP &lt; 80 ~ \"Normal\",\n    (Systolic_BP &gt;= 120 & Systolic_BP &lt; 130) & Diastolic_BP &lt; 80 ~ \"Elevated\",\n    (Systolic_BP &gt;= 130 & Systolic_BP &lt; 140) | (Diastolic_BP &gt;= 80 & Diastolic_BP &lt; 90) ~ \"High Stage 1\",\n    Systolic_BP &gt;= 140 | Diastolic_BP &gt;= 90 ~ \"High Stage 2\",\n    TRUE ~ NA_character_\n  ))\nHA &lt;- HA %&gt;%\n  mutate(Cholesterol_Level_cat = case_when(\n    Cholesterol_Level &lt; 200 ~ \"Desirable\",\n    Cholesterol_Level &gt;= 200 & Cholesterol_Level &lt; 239 ~ \"Borderline High\",\n    Cholesterol_Level &gt;= 240 ~ \"High\",\n    TRUE ~ NA_character_\n  ))\n\nHA &lt;- HA %&gt;%\n  mutate(Stress_Level_cat = case_when(\n    Stress_Levels &lt; 3 ~ \"Low\",\n    Stress_Levels &gt;= 3 & Stress_Levels &lt; 6 ~ \"Moderate\",\n    Stress_Levels &gt;= 6 ~ \"High\",\n    TRUE ~ NA_character_\n  ))\n\n\nHA &lt;- HA %&gt;%\n  mutate(Heart_Attack_Occurrence_num = ifelse(Heart_Attack_Occurrence == \"Yes\", 1,\n                                              ifelse(Heart_Attack_Occurrence == \"No\", 0, NA)))\n\nHA &lt;- HA %&gt;% \n  mutate(Heart_Rate_cat = case_when(\n    Heart_Rate &lt; 70 ~ \"Below Normal\",\n    Heart_Rate &gt;=  70 & Heart_Rate &lt;= 100 ~ \"Normal\",\n    Heart_Rate  &gt; 100 ~ \"Elevated\",\n    TRUE ~ NA_character_\n  ))\n\n\n# Removal of NA data \n\nHA &lt;- HA %&gt;%\n  filter(!is.na(BMI_cat) & !is.na(Cholesterol_Level_cat))\n\n\nHA_Yes &lt;- filter(HA, Heart_Attack_Occurrence == \"Yes\")\nHA_No &lt;- filter(HA, Heart_Attack_Occurrence == \"No\")\nHA_youth &lt;- filter(HA, Age_Grp == \"Youth\")\n\n\n\n\n\nThe aim of this task is to examine if there are any trends in categorical data in the occurance of heart attack in Japnaese population.\nThe code below is extract our the categorical data and summarise into the occurance of heart attack\n\ncategorical_columns &lt;- HA %&gt;% select(where(~is.character(.x) || is.factor(.x)))\n\ncount_occurrences &lt;- categorical_columns %&gt;%\n  pivot_longer(cols= - c(Heart_Attack_Occurrence, Age_Grp, Event_Grp), names_to = \"Category\", values_to = \"Value\") %&gt;%\n  group_by(Category, Value, Age_Grp, Heart_Attack_Occurrence, Event_Grp) %&gt;%\n  summarise(Occurrences = n(), .groups = 'drop')\n\nprint(count_occurrences)\n\n# A tibble: 156 × 6\n   Category           Value Age_Grp Heart_Attack_Occurre…¹ Event_Grp Occurrences\n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;                  &lt;chr&gt;           &lt;int&gt;\n 1 Alcohol_Consumpti… High  Adult   No                     Adult_No…        2308\n 2 Alcohol_Consumpti… High  Adult   Yes                    Adult_HA          277\n 3 Alcohol_Consumpti… High  Youth   No                     Youth_No…        1525\n 4 Alcohol_Consumpti… High  Youth   Yes                    Youth_HA          157\n 5 Alcohol_Consumpti… Low   Adult   No                     Adult_No…        3572\n 6 Alcohol_Consumpti… Low   Adult   Yes                    Adult_HA          402\n 7 Alcohol_Consumpti… Low   Youth   No                     Youth_No…        2372\n 8 Alcohol_Consumpti… Low   Youth   Yes                    Youth_HA          262\n 9 Alcohol_Consumpti… Mode… Adult   No                     Adult_No…        4804\n10 Alcohol_Consumpti… Mode… Adult   Yes                    Adult_HA          526\n# ℹ 146 more rows\n# ℹ abbreviated name: ¹​Heart_Attack_Occurrence\n\n\n\n# Create bar plots for each categorical field\nbar_plots &lt;- count_occurrences %&gt;%\n  ggplot(aes(x = Value, y = Occurrences, fill = Event_Grp)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +  # Use position_dodge for side-by-side bars\n  facet_wrap(~ Category, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Counts of Unique Values in Categorical Fields\",\n       x = \"Unique Values\",\n       y = \"Count\") \n\n# Print the bar plots\nprint(bar_plots)\n\n\n\n\n\n\n\n\n\n\n\ncount_occurrences_youth &lt;- filter(count_occurrences, Age_Grp == \"Youth\")\n\n# Create bar plots for each categorical field\nbar_plots &lt;- count_occurrences_youth %&gt;%\n  ggplot(aes(x = Value, y = Occurrences, fill = Heart_Attack_Occurrence)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +  # Use position_dodge for side-by-side bars\n  facet_wrap(~ Category, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Counts of Unique Values in Categorical Fields\",\n       x = \"Unique Values\",\n       y = \"Count\") \n\n# Print the bar plots\nprint(bar_plots)\n\n\n\n\n\n\n\n\n\n#pm &lt;- ggpairs(HA, columns = 1:17, ggplot2::aes(colour = Heart_Attack_Occurrence ))\n#print (pm)"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#visualising-continuous-data",
    "href": "Take-home_Exe/Take-home_Ex_01.html#visualising-continuous-data",
    "title": "Take-Home Exercise 01",
    "section": "visualising continuous data",
    "text": "visualising continuous data\n\n# Select only the continuous columns (numeric)\ncontin_col_HA &lt;- HA %&gt;% select(where(is.numeric), Heart_Attack_Occurrence, Age_Grp)\n\n\ncontin_col_HA_Long&lt;-contin_col_HA %&gt;%\n  pivot_longer(cols = -c(Heart_Attack_Occurrence, Age_Grp), names_to = \"Field\", values_to = \"Value\")\n               \ncontin_col_HA_Long_Yes &lt;- filter(contin_col_HA_Long, Heart_Attack_Occurrence == \"Yes\")              \n\n\n# Create histograms with density plots for continuous variables\nhistograms &lt;- ggplot(contin_col_HA_Long, aes(x = Value, fill = Heart_Attack_Occurrence )) + \n  geom_histogram(binwidth = 1, alpha = 0.8) + \n  theme_minimal() + \n  labs(title = \"Histograms of Continuous Fields\", x = \"Value\", y = \"Frequency\")+\n  facet_wrap(~ Field, scales = \"free\") \n\n# Print the histograms\nprint(histograms)\n\n\n\n\n\n\n\n\n\n# Create histograms with density plots for continuous variables\nhistograms &lt;- ggplot(contin_col_HA_Long_Yes, aes(x = Value, fill = Age_Grp )) + \n  geom_histogram(binwidth = 1, alpha = 0.8) + \n  theme_minimal() + \n  labs(title = \"Histograms of Continuous Fields\", x = \"Value\", y = \"Frequency\")+\n  facet_wrap(~ Field, scales = \"free\") \n\n# Print the histograms\nprint(histograms)\n\n\n\n\n\n\n\n\n\nvisualising using density plot\n\n# Adding density plot\nggplot(HA, aes(x = Stress_Levels)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = \"blue\", alpha = 0.5) +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram and Density Plot of Stress Levels\", x = \"Stress Levels\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nChi Square\n\nggbarstats(HA, \n           x=Age_Grp,\n           y=Heart_Attack_Occurrence)\n\n\n\n\n\n\n\n\n\nggbarstats(HA, \n           x=Event_Grp,\n           y=Region)\n\n\n\n\n\n\n\n\n\nggbarstats(HA_youth, \n           x= Heart_Attack_Occurrence,\n           y=BMI_cat)\n\n\n\n\n\n\n\n\n\n# Select categorical columns\ncategorical_columns &lt;- HA_youth %&gt;% select(where(~is.character(.) || is.factor(.)))\n\n# Function to perform Chi-Square test and visualize the results\nanalyze_association &lt;- function(data, x, y) {\n  # Perform Chi-Square test\n  table &lt;- table(data[[x]], data[[y]])\n  chi_test &lt;- chisq.test(table)\n  \n  # Print Chi-Square test results\n  print(paste(\"Chi-Square Test between\", x, \"and\", y))\n  print(chi_test)\n  \n  # Visualize the results using bar plot\n  plot &lt;- ggplot(HA_youth, aes_string(x = x, fill = y)) +\n    geom_bar(position = \"dodge\") +\n    labs(title = paste(\"Bar Plot of\", x, \"vs\", y, \"\\nChi-Square p-value:\", format(chi_test$p.value, digits=4)),\n         x = x,\n         y = \"Count\") +\n    theme_bw()\n  \n  print(plot)\n}\n\n# Analyze association for each categorical column with Event_Grp\nfor (column in colnames(categorical_columns)) {\n  if (column != \"Heart_Attack_Occurrence\") { # Skip Event_Grp column itself\n    analyze_association(HA_youth, \"Heart_Attack_Occurrence\", column)\n  }\n}\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Gender\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.0027295, df = 1, p-value = 0.9583\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Region\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.12335, df = 1, p-value = 0.7254\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Smoking_History\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.084801, df = 1, p-value = 0.7709\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Diabetes_History\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 7.2967, df = 1, p-value = 0.006908\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Hypertension_History\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 1.3112, df = 1, p-value = 0.2522\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Physical_Activity\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.27734, df = 2, p-value = 0.8705\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Diet_Quality\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 2.9799, df = 2, p-value = 0.2254\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Alcohol_Consumption\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 3.9517, df = 3, p-value = 0.2667\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Family_History\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.52143, df = 1, p-value = 0.4702\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Age_Grp\"\n\n    Chi-squared test for given probabilities\n\ndata:  table\nX-squared = 5658.3, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Event_Grp\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 8591.3, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and BMI_cat\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 8.312, df = 3, p-value = 0.03998\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and BP_cat\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 2.718, df = 3, p-value = 0.4372\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Cholesterol_Level_cat\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 3.6477, df = 2, p-value = 0.1614\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Stress_Level_cat\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 5.3458, df = 2, p-value = 0.06905\n\n\n\n\n\n\n\n\n\n[1] \"Chi-Square Test between Heart_Attack_Occurrence and Heart_Rate_cat\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.59882, df = 2, p-value = 0.7413\n\n\n\n\n\n\n\n\n\n\nggplot(HA, aes(x = Stress_Levels, fill = Heart_Attack_Occurrence)) +  # Map fill to Heart_Attack_Occurrence\n  geom_histogram(aes(y = ..density..), binwidth = 1, alpha = 0.5, position = \"identity\") +  # Adjust position for overlap\n  geom_density(color = \"red\", size = 1, alpha = 0.3) +  # Density plot with transparency\n  labs(title = \"Histogram and Density Plot of Stress Levels\",\n       x = \"Stress Levels\",\n       y = \"Density\") +\n  theme_minimal(base_size = 14) +  # Improve overall theme and font size\n  scale_fill_brewer(palette = \"Set2\", name = \"Heart Attack Occurrence\") +  # Use a color palette for filling\n  theme(legend.position = \"top\")  # Position the legend at the top\n\n\n\n\n\n\n\n\n\nsummary(HA_Yes)\n\n      Age          Gender             Region          Smoking_History   \n Min.   :18.0   Length:2156        Length:2156        Length:2156       \n 1st Qu.:30.0   Class :character   Class :character   Class :character  \n Median :40.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   :40.9                                                           \n 3rd Qu.:52.0                                                           \n Max.   :63.0                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:2156        Length:2156          Min.   :105.2     Length:2156       \n Class :character   Class :character     1st Qu.:179.1     Class :character  \n Mode  :character   Mode  :character     Median :199.5     Mode  :character  \n                                         Mean   :198.9                       \n                                         3rd Qu.:219.5                       \n                                         Max.   :311.2                       \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI        \n Length:2156        Length:2156         Min.   : 0.000   Min.   : 9.435  \n Class :character   Class :character    1st Qu.: 3.503   1st Qu.:21.712  \n Mode  :character   Mode  :character    Median : 4.873   Median :24.784  \n                                        Mean   : 4.863   Mean   :24.904  \n                                        3rd Qu.: 6.185   3rd Qu.:28.042  \n                                        Max.   :10.000   Max.   :42.630  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 34.36   Min.   : 65.71   Min.   : 44.30   Length:2156       \n 1st Qu.: 62.88   1st Qu.:109.74   1st Qu.: 73.23   Class :character  \n Median : 70.00   Median :119.27   Median : 80.19   Mode  :character  \n Mean   : 69.86   Mean   :119.68   Mean   : 80.11                     \n 3rd Qu.: 76.51   3rd Qu.:130.22   3rd Qu.: 86.94                     \n Max.   :102.94   Max.   :167.81   Max.   :113.85                     \n Heart_Attack_Occurrence   Age_Grp           Event_Grp        \n Length:2156             Length:2156        Length:2156       \n Class :character        Class :character   Class :character  \n Mode  :character        Mode  :character   Mode  :character  \n                                                              \n                                                              \n                                                              \n   BMI_cat             BP_cat          Cholesterol_Level_cat Stress_Level_cat  \n Length:2156        Length:2156        Length:2156           Length:2156       \n Class :character   Class :character   Class :character      Class :character  \n Mode  :character   Mode  :character   Mode  :character      Mode  :character  \n                                                                               \n                                                                               \n                                                                               \n Heart_Rate_cat     Heart_Attack_Occurrence_num   Gender_num    \n Length:2156        Min.   :1                   Min.   :0.0000  \n Class :character   1st Qu.:1                   1st Qu.:0.0000  \n Mode  :character   Median :1                   Median :1.0000  \n                    Mean   :1                   Mean   :0.5209  \n                    3rd Qu.:1                   3rd Qu.:1.0000  \n                    Max.   :1                   Max.   :1.0000  \n   Region_num     Smoking_History_num Diabetes_History_num\n Min.   :0.0000   Min.   :0.0000      Min.   :0.0000      \n 1st Qu.:0.0000   1st Qu.:0.0000      1st Qu.:0.0000      \n Median :1.0000   Median :0.0000      Median :0.0000      \n Mean   :0.6934   Mean   :0.3075      Mean   :0.2217      \n 3rd Qu.:1.0000   3rd Qu.:1.0000      3rd Qu.:0.0000      \n Max.   :1.0000   Max.   :1.0000      Max.   :1.0000      \n Hypertension_History_num Family_History_num Physical_Activity_num\n Min.   :0.0000           Min.   :0.0000     Min.   :0.0000       \n 1st Qu.:0.0000           1st Qu.:0.0000     1st Qu.:0.0000       \n Median :0.0000           Median :0.0000     Median :1.0000       \n Mean   :0.2648           Mean   :0.2917     Mean   :0.9717       \n 3rd Qu.:1.0000           3rd Qu.:1.0000     3rd Qu.:2.0000       \n Max.   :1.0000           Max.   :1.0000     Max.   :2.0000       \n Diet_Quality_num Alcohol_Consumption_num Stress_Level_cat_num  BMI_cat_num   \n Min.   :0.0000   Min.   :0.000           Min.   :0.000        Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:1.000           1st Qu.:1.000        1st Qu.:1.000  \n Median :1.0000   Median :2.000           Median :1.000        Median :1.000  \n Mean   :0.7713   Mean   :1.675           Mean   :1.103        Mean   :1.399  \n 3rd Qu.:1.0000   3rd Qu.:2.000           3rd Qu.:2.000        3rd Qu.:2.000  \n Max.   :2.0000   Max.   :3.000           Max.   :2.000        Max.   :2.000  \n Heart_Rate_cat_num   BP_cat_num    Cholesterol_Level_cat_num\n Min.   :0.0000     Min.   :0.000   Min.   :0.0000           \n 1st Qu.:0.0000     1st Qu.:0.000   1st Qu.:0.0000           \n Median :1.0000     Median :2.000   Median :0.0000           \n Mean   :0.5014     Mean   :1.544   Mean   :0.5742           \n 3rd Qu.:1.0000     3rd Qu.:2.000   3rd Qu.:1.0000           \n Max.   :2.0000     Max.   :3.000   Max.   :2.0000           \n\n\n\nggplot(data=HA,\n       aes(x =  Alcohol_Consumption, y = Cholesterol_Level, color =Heart_Attack_Occurrence )) +\n    geom_boxplot()+\n  facet_wrap(~ Age_Grp)\n\n\n\n\n\n\n\n\n\nggplot(data=HA_Yes,\n       aes(x = Diet_Quality, y = BMI, color = Age_Grp)) +\n    geom_boxplot()+\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n#| fig-width: 24 #to widen the space\n#| fig-height: 24 #to lengthen the graph\nggplot(HA_youth, \n       aes(x = Heart_Attack_Occurrence, y = BMI)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.01,\n               .width = 0.01,\n               point_colour = NA, \n               size=0.01) +\n  geom_boxplot(width = .05,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = .1)\n\n\n\n\n\n\n\n\n\nscatter_plot &lt;- ggplot(HA, aes(x = Cholesterol_Level, y = BMI, color = Heart_Attack_Occurrence)) +\n  geom_point(size = 2, alpha = 0.5) +  # Adjust point size and transparency\n  labs(title = \"Scatter Plot of Stress Levels vs. BMI\",\n       x = \"Stress Levels\",\n       y = \"BMI\",\n       color = \"Heart Attack Occurrence\") +  # Add title and labels\n  theme_minimal(base_size = 14) +  # Use a clean theme with larger base font size\n  scale_color_manual(values = c(\"blue\", \"red\"))  # Customize colors if needed\n\n# Print the scatter plot\nprint(scatter_plot)\n\n\n\n\n\n\n\n\n\n# Step 1: Select only continuous (numeric) columns along with Heart_Attack_Occurrence\ncontinuous_columns &lt;- HA %&gt;% \n  select(where(is.numeric), Heart_Attack_Occurrence)\n\n# Step 2: Reshape the data to long format\nlong_data &lt;- continuous_columns %&gt;%\n  pivot_longer(-Heart_Attack_Occurrence, names_to = \"Variable\", values_to = \"Value\")\n\n# Step 3: Create scatter plots for each continuous variable against Heart_Attack_Occurrence\nscatter_plot &lt;- ggplot(long_data, aes(x = Value, y = as.numeric(Heart_Attack_Occurrence), color = Heart_Attack_Occurrence)) +\n  geom_point(alpha = 0.7, size = 2) +  # Adjust point size and transparency\n  facet_wrap(~ Variable, scales = \"free\") +  # Create facets for each variable\n  labs(title = \"Scatter Plots of Continuous Variables by Heart Attack Occurrence\",\n       x = \"Value\",\n       y = \"Heart Attack Occurrence (0 = No, 1 = Yes)\") +  # y-axis represents occurrence\n  theme_minimal(base_size = 14) +  # Clean theme\n  scale_color_manual(values = c(\"blue\", \"red\"))  # Customize colors for heart attack occurrence\n\n# Print the scatter plots\nprint(scatter_plot)\n\n\n\n\n\n\n\n# Print the scatter plots\nprint(scatter_plot)\n\n\n\n\n\n\n\n\n\n\nInspiration\n\nggplot(HA, aes(x = Smoking_History, fill = Heart_Attack_Occurrence)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Smoking History vs Heart Attack Occurrence\", \n       y = \"Proportion\", \n       x = \"Smoking History\") +\n  scale_fill_manual(values = c(\"lightblue\", \"salmon\"), name = \"Heart Attack Occurrence\")\n\n\n\n\n\n\n\n\n\n\nAnova\n\nggbetweenstats(\n  data = HA_youth,\n  x = Heart_Attack_Occurrence, \n  y = Stress_Levels,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = HA_youth,\n  x = Heart_Attack_Occurrence, \n  y = BMI,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe very small effect size (Hedges’ g = 0.04) suggests that although the difference is statistically significant, it may not be practically relevant. In other words, the difference in means may not be large enough to have meaningful implications in a real-world context.\n\n\n\n\nModel building\n\nmodel &lt;- lm(Heart_Attack_Occurrence_num ~ Cholesterol_Level_cat + BMI_cat + Diabetes_History, data = HA_youth)\nmodel\n\n\nCall:\nlm(formula = Heart_Attack_Occurrence_num ~ Cholesterol_Level_cat + \n    BMI_cat + Diabetes_History, data = HA_youth)\n\nCoefficients:\n                   (Intercept)  Cholesterol_Level_catDesirable  \n                      0.100475                       -0.001157  \n     Cholesterol_Level_catHigh                    BMI_catObese  \n                     -0.021622                       -0.025376  \n             BMI_catOverweight              BMI_catUnderweight  \n                     -0.005984                       -0.015551  \n           Diabetes_HistoryYes  \n                      0.021381  \n\n\n\ncheck_n &lt;- check_normality(model)\nplot(check_n)\n\n\n\n\n\n\n\n\n\ncheck_h &lt;- check_heteroscedasticity(model)\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\ncheck_model(model)\n\n\n\n\n\n\n\n\n\nstr(HA_youth)\n\ntibble [8,603 × 39] (S3: tbl_df/tbl/data.frame)\n $ Age                        : num [1:8603] 32 25 28 28 20 19 19 29 33 32 ...\n $ Gender                     : chr [1:8603] \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ Region                     : chr [1:8603] \"Urban\" \"Rural\" \"Urban\" \"Rural\" ...\n $ Smoking_History            : chr [1:8603] \"No\" \"No\" \"No\" \"No\" ...\n $ Diabetes_History           : chr [1:8603] \"No\" \"No\" \"Yes\" \"Yes\" ...\n $ Hypertension_History       : chr [1:8603] \"No\" \"No\" \"Yes\" \"No\" ...\n $ Cholesterol_Level          : num [1:8603] 211 220 174 163 195 ...\n $ Physical_Activity          : chr [1:8603] \"Moderate\" \"Low\" \"Low\" \"Low\" ...\n $ Diet_Quality               : chr [1:8603] \"Good\" \"Good\" \"Average\" \"Good\" ...\n $ Alcohol_Consumption        : chr [1:8603] \"High\" \"High\" \"Low\" \"Moderate\" ...\n $ Stress_Levels              : num [1:8603] 6.01 8.21 1.6 3.13 4.04 ...\n $ BMI                        : num [1:8603] 23.7 20.2 24.4 25.2 32.3 ...\n $ Heart_Rate                 : num [1:8603] 55.1 67.7 75.3 62.1 78.9 ...\n $ Systolic_BP                : num [1:8603] 132 135 117 132 111 ...\n $ Diastolic_BP               : num [1:8603] 68.2 73.1 75.3 86.6 56.2 ...\n $ Family_History             : chr [1:8603] \"No\" \"No\" \"No\" \"No\" ...\n $ Heart_Attack_Occurrence    : chr [1:8603] \"No\" \"No\" \"No\" \"No\" ...\n $ Age_Grp                    : chr [1:8603] \"Youth\" \"Youth\" \"Youth\" \"Youth\" ...\n $ Event_Grp                  : chr [1:8603] \"Youth_No_HA\" \"Youth_No_HA\" \"Youth_No_HA\" \"Youth_No_HA\" ...\n $ BMI_cat                    : chr [1:8603] \"Normal\" \"Normal\" \"Normal\" \"Overweight\" ...\n $ BP_cat                     : chr [1:8603] \"High Stage 1\" \"High Stage 1\" \"Normal\" \"High Stage 1\" ...\n $ Cholesterol_Level_cat      : chr [1:8603] \"Borderline High\" \"Borderline High\" \"Desirable\" \"Desirable\" ...\n $ Stress_Level_cat           : chr [1:8603] \"High\" \"High\" \"Low\" \"Moderate\" ...\n $ Heart_Rate_cat             : chr [1:8603] \"Below Normal\" \"Below Normal\" \"Normal\" \"Below Normal\" ...\n $ Heart_Attack_Occurrence_num: num [1:8603] 0 0 0 0 0 0 0 0 0 0 ...\n $ Gender_num                 : num [1:8603] 0 0 0 0 0 1 1 0 1 1 ...\n $ Region_num                 : num [1:8603] 1 0 1 0 1 0 1 1 1 0 ...\n $ Smoking_History_num        : num [1:8603] 0 0 0 0 0 1 0 1 0 0 ...\n $ Diabetes_History_num       : num [1:8603] 0 0 1 1 0 0 0 0 0 0 ...\n $ Hypertension_History_num   : num [1:8603] 0 0 1 0 0 0 1 0 0 0 ...\n $ Family_History_num         : num [1:8603] 0 0 0 0 0 0 0 1 0 0 ...\n $ Physical_Activity_num      : num [1:8603] 1 2 2 2 2 2 1 1 1 0 ...\n $ Diet_Quality_num           : num [1:8603] 0 0 1 0 1 1 0 0 1 0 ...\n $ Alcohol_Consumption_num    : num [1:8603] 3 3 1 2 2 1 2 0 2 1 ...\n $ Stress_Level_cat_num       : num [1:8603] 2 2 0 1 1 2 1 2 0 2 ...\n $ BMI_cat_num                : num [1:8603] 1 1 1 2 2 2 0 2 2 1 ...\n $ Heart_Rate_cat_num         : num [1:8603] 0 0 1 0 1 0 0 0 0 1 ...\n $ BP_cat_num                 : num [1:8603] 2 2 0 2 0 2 2 0 2 1 ...\n $ Cholesterol_Level_cat_num  : num [1:8603] 1 1 0 0 0 0 0 0 0 0 ...\n\nsummary(HA_youth)\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:8603        Length:8603        Length:8603       \n 1st Qu.:22.00   Class :character   Class :character   Class :character  \n Median :27.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :26.65                                                           \n 3rd Qu.:31.00                                                           \n Max.   :35.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:8603        Length:8603          Min.   : 93.91    Length:8603       \n Class :character   Class :character     1st Qu.:179.47    Class :character  \n Mode  :character   Mode  :character     Median :199.35    Mode  :character  \n                                         Mean   :199.86                      \n                                         3rd Qu.:220.12                      \n                                         Max.   :303.94                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI        \n Length:8603        Length:8603         Min.   : 0.000   Min.   : 6.602  \n Class :character   Class :character    1st Qu.: 3.591   1st Qu.:21.644  \n Mode  :character   Mode  :character    Median : 4.923   Median :25.075  \n                                        Mean   : 4.966   Mean   :25.052  \n                                        3rd Qu.: 6.343   3rd Qu.:28.476  \n                                        Max.   :10.000   Max.   :42.157  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 30.59   Min.   : 67.49   Min.   : 42.92   Length:8603       \n 1st Qu.: 63.39   1st Qu.:109.80   1st Qu.: 73.06   Class :character  \n Median : 70.09   Median :119.83   Median : 79.90   Mode  :character  \n Mean   : 70.11   Mean   :120.15   Mean   : 79.91                     \n 3rd Qu.: 76.81   3rd Qu.:130.41   3rd Qu.: 86.76                     \n Max.   :104.15   Max.   :174.16   Max.   :117.66                     \n Heart_Attack_Occurrence   Age_Grp           Event_Grp        \n Length:8603             Length:8603        Length:8603       \n Class :character        Class :character   Class :character  \n Mode  :character        Mode  :character   Mode  :character  \n                                                              \n                                                              \n                                                              \n   BMI_cat             BP_cat          Cholesterol_Level_cat Stress_Level_cat  \n Length:8603        Length:8603        Length:8603           Length:8603       \n Class :character   Class :character   Class :character      Class :character  \n Mode  :character   Mode  :character   Mode  :character      Mode  :character  \n                                                                               \n                                                                               \n                                                                               \n Heart_Rate_cat     Heart_Attack_Occurrence_num   Gender_num    \n Length:8603        Min.   :0.0000              Min.   :0.0000  \n Class :character   1st Qu.:0.0000              1st Qu.:0.0000  \n Mode  :character   Median :0.0000              Median :0.0000  \n                    Mean   :0.0945              Mean   :0.4991  \n                    3rd Qu.:0.0000              3rd Qu.:1.0000  \n                    Max.   :1.0000              Max.   :1.0000  \n   Region_num     Smoking_History_num Diabetes_History_num\n Min.   :0.0000   Min.   :0.0000      Min.   :0.0000      \n 1st Qu.:0.0000   1st Qu.:0.0000      1st Qu.:0.0000      \n Median :1.0000   Median :0.0000      Median :0.0000      \n Mean   :0.6936   Mean   :0.2939      Mean   :0.2064      \n 3rd Qu.:1.0000   3rd Qu.:1.0000      3rd Qu.:0.0000      \n Max.   :1.0000   Max.   :1.0000      Max.   :1.0000      \n Hypertension_History_num Family_History_num Physical_Activity_num\n Min.   :0.000            Min.   :0.000      Min.   :0.000        \n 1st Qu.:0.000            1st Qu.:0.000      1st Qu.:0.000        \n Median :0.000            Median :0.000      Median :1.000        \n Mean   :0.251            Mean   :0.297      Mean   :1.002        \n 3rd Qu.:1.000            3rd Qu.:1.000      3rd Qu.:2.000        \n Max.   :1.000            Max.   :1.000      Max.   :2.000        \n Diet_Quality_num Alcohol_Consumption_num Stress_Level_cat_num  BMI_cat_num   \n Min.   :0.0000   Min.   :0.000           Min.   :0.000        Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:1.000           1st Qu.:1.000        1st Qu.:1.000  \n Median :1.0000   Median :2.000           Median :1.000        Median :2.000  \n Mean   :0.7952   Mean   :1.677           Mean   :1.143        Mean   :1.409  \n 3rd Qu.:1.0000   3rd Qu.:2.000           3rd Qu.:2.000        3rd Qu.:2.000  \n Max.   :2.0000   Max.   :3.000           Max.   :2.000        Max.   :2.000  \n Heart_Rate_cat_num   BP_cat_num    Cholesterol_Level_cat_num\n Min.   :0.0000     Min.   :0.000   Min.   :0.0000           \n 1st Qu.:0.0000     1st Qu.:0.000   1st Qu.:0.0000           \n Median :1.0000     Median :2.000   Median :0.0000           \n Mean   :0.5059     Mean   :1.538   Mean   :0.5834           \n 3rd Qu.:1.0000     3rd Qu.:2.000   3rd Qu.:1.0000           \n Max.   :2.0000     Max.   :3.000   Max.   :2.0000           \n\n\n\n\nLoistic Regression Model\n\n# Fit the model\nmodel_log_youth &lt;- glm(Heart_Attack_Occurrence_num  ~ Gender_num + Region_num + Smoking_History_num + Diabetes_History_num + Physical_Activity_num + Diet_Quality_num + Alcohol_Consumption_num + Stress_Level_cat_num + BMI_cat_num + Heart_Rate_cat_num + BP_cat_num +Family_History_num + Cholesterol_Level_cat_num, family = binomial(), data = HA_youth)\n\n# View the summary\nsummary(model_log_youth)\n\n\nCall:\nglm(formula = Heart_Attack_Occurrence_num ~ Gender_num + Region_num + \n    Smoking_History_num + Diabetes_History_num + Physical_Activity_num + \n    Diet_Quality_num + Alcohol_Consumption_num + Stress_Level_cat_num + \n    BMI_cat_num + Heart_Rate_cat_num + BP_cat_num + Family_History_num + \n    Cholesterol_Level_cat_num, family = binomial(), data = HA_youth)\n\nCoefficients:\n                           Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.902413   0.174884 -10.878  &lt; 2e-16 ***\nGender_num                 0.008399   0.073843   0.114  0.90944    \nRegion_num                -0.035929   0.079701  -0.451  0.65213    \nSmoking_History_num        0.027470   0.080680   0.340  0.73349    \nDiabetes_History_num       0.233031   0.086575   2.692  0.00711 ** \nPhysical_Activity_num     -0.022429   0.047622  -0.471  0.63765    \nDiet_Quality_num          -0.081135   0.049720  -1.632  0.10271    \nAlcohol_Consumption_num   -0.062754   0.040644  -1.544  0.12259    \nStress_Level_cat_num      -0.126628   0.054974  -2.303  0.02125 *  \nBMI_cat_num               -0.043197   0.055592  -0.777  0.43713    \nHeart_Rate_cat_num         0.049823   0.073476   0.678  0.49772    \nBP_cat_num                 0.017357   0.035005   0.496  0.62002    \nFamily_History_num        -0.061192   0.081671  -0.749  0.45371    \nCholesterol_Level_cat_num -0.069314   0.057217  -1.211  0.22573    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5382.6  on 8602  degrees of freedom\nResidual deviance: 5361.2  on 8589  degrees of freedom\nAIC: 5389.2\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nexp(coef(model_log_youth))\n\n              (Intercept)                Gender_num                Region_num \n                0.1492082                 1.0084343                 0.9647085 \n      Smoking_History_num      Diabetes_History_num     Physical_Activity_num \n                1.0278509                 1.2624204                 0.9778205 \n         Diet_Quality_num   Alcohol_Consumption_num      Stress_Level_cat_num \n                0.9220689                 0.9391745                 0.8810613 \n              BMI_cat_num        Heart_Rate_cat_num                BP_cat_num \n                0.9577223                 1.0510845                 1.0175082 \n       Family_History_num Cholesterol_Level_cat_num \n                0.9406425                 0.9330333 \n\n\n\nconfint.default(model_log_youth)\n\n                                2.5 %      97.5 %\n(Intercept)               -2.24517967 -1.55964576\nGender_num                -0.13633104  0.15312900\nRegion_num                -0.19214056  0.12028191\nSmoking_History_num       -0.13065991  0.18560021\nDiabetes_History_num       0.06334699  0.40271463\nPhysical_Activity_num     -0.11576681  0.07090856\nDiet_Quality_num          -0.17858543  0.01631467\nAlcohol_Consumption_num   -0.14241450  0.01690649\nStress_Level_cat_num      -0.23437441 -0.01888171\nBMI_cat_num               -0.15215615  0.06576123\nHeart_Rate_cat_num        -0.09418722  0.19383227\nBP_cat_num                -0.05125251  0.08596581\nFamily_History_num        -0.22126528  0.09888109\nCholesterol_Level_cat_num -0.18145736  0.04282853\n\n\n\nlibrary(ResourceSelection)\nlength(HA_youth$Heart_Attack_Occurrence_num)\n\n[1] 8603\n\nlength(fitted(model_log_youth))\n\n[1] 8603\n\nhoslem.test(HA_youth$Heart_Attack_Occurrence_num, fitted(model_log_youth))\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  HA_youth$Heart_Attack_Occurrence_num, fitted(model_log_youth)\nX-squared = 11.522, df = 8, p-value = 0.1738\n\n\n\nlibrary(caret)\npredictions &lt;- predict(model_log_youth, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconfusionMatrix(as.factor(predicted_classes), as.factor(HA_youth$Heart_Attack_Occurrence_num))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 7790  813\n         1    0    0\n                                          \n               Accuracy : 0.9055          \n                 95% CI : (0.8991, 0.9116)\n    No Information Rate : 0.9055          \n    P-Value [Acc &gt; NIR] : 0.5093          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.9055          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.9055          \n         Detection Rate : 0.9055          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n# Extract coefficients from the model\ncoef_data &lt;- as.data.frame(coef(summary(model_log_youth)))\ncoef_data$Feature &lt;- rownames(coef_data)\nrownames(coef_data) &lt;- NULL\n\n# Plot the coefficients\nlibrary(ggplot2)\nggplot(coef_data, aes(x = reorder(Feature, Estimate), y = Estimate)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Feature Importance (Logistic Regression Coefficients)\",\n       x = \"Feature\",\n       y = \"Coefficient Estimate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nlibrary(caret)\npredictions &lt;- predict(model_log_youth, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconfusionMatrix(as.factor(predicted_classes), as.factor(HA_youth$Heart_Attack_Occurrence_num))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 7790  813\n         1    0    0\n                                          \n               Accuracy : 0.9055          \n                 95% CI : (0.8991, 0.9116)\n    No Information Rate : 0.9055          \n    P-Value [Acc &gt; NIR] : 0.5093          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.9055          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.9055          \n         Detection Rate : 0.9055          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nset.seed(123)\ntrain_indices &lt;- sample(1:nrow(HA_youth), 0.7 * nrow(HA_youth))\ntrain_data &lt;- HA_youth[train_indices, ]\ntest_data &lt;- HA_youth[-train_indices, ]\n\n# Fit the model on the training data\nmodel_train &lt;- glm(Heart_Attack_Occurrence_num ~ Gender_num + Region_num + Smoking_History_num + Diabetes_History_num + Physical_Activity_num + Diet_Quality_num + Alcohol_Consumption_num + Stress_Level_cat_num + BMI_cat_num + Heart_Rate_cat_num + BP_cat_num + Family_History_num + Cholesterol_Level_cat_num, family = binomial(), data = train_data)\n\n# Predict on the test data\ntest_predictions &lt;- predict(model_train, newdata = test_data, type = \"response\")\ntest_predicted_classes &lt;- ifelse(test_predictions &gt; 0.5, 1, 0)\n\n# Evaluate the model on the test data\nconfusionMatrix(as.factor(test_predicted_classes), as.factor(test_data$Heart_Attack_Occurrence_num))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2338  243\n         1    0    0\n                                          \n               Accuracy : 0.9059          \n                 95% CI : (0.8939, 0.9168)\n    No Information Rate : 0.9059          \n    P-Value [Acc &gt; NIR] : 0.5171          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.9059          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.9059          \n         Detection Rate : 0.9059          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nggcoefstats(model_log_youth, \n            output = \"plot\")\n\n\n\n\n\n\n\n\n\nAIC(model_log_youth )\n\n[1] 5389.181\n\n\n\n\nConclusion:\nThe health crsis of heart attack is largely depended on the possibility"
  },
  {
    "objectID": "In-class_Exe/In-class_Ex04.html",
    "href": "In-class_Exe/In-class_Ex04.html",
    "title": "In-Class Ex 04",
    "section": "",
    "text": "pacman::p_load(haven, SmartEDA, tidyverse, tidymodels, ggdist, ggridges, ggthemes,\n               colorspace, gridExtra, ggstatsplot)\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\nggplot(data=exam, \n       aes(y=CLASS,\n           x=ENGLISH)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  #this part to create in ridgelines\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  ### x axis continous data\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\nrain drop plot.\nThe probabilty density plot will smooth out the the curve and the dot plot will be able to show the actual data points.\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\", #np is non parametric \n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_04.html",
    "href": "Hands-on_Exe/Hands-on_Ex_04.html",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "Visualising distribution is the first steps to understand and analyse the assigned data set,\n\n\nIn this Hands-on, we will be using the following packages:\n\nggdist: to plot ridglines, which will be useful to see the distribution across another dimension, such as changes in scores across Class.\nggthemes: for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse, gridExtra)\n\n\n\n\nThe same dataset exam data is being used for this exercise, as previously.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that in general 3A has the highest English score for when compared to other classes.\n\n\n\n\n\nCode for visualising English scores using geom_ridgeline_gradient(),\nThis function may not be useful and it may create confusion among the viewers.\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\nVisualising the Maths scores.\n\nggplot(exam, \n       aes(x = MATHS, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"Maths grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\necdf refers to the empirical; cumulative distribution. This is alternatives to visualise distribution and it could handle with continuous and categorical variables. the downside of this is that it will requires more training to accurately interpret.\nFor the could chunk below put the two charts side by side for easy visualization.\n\nECDF &lt;- ggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n \nEnglish &lt;- ggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\ngrid.arrange(English, ECDF, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\nThe probability is the highest at peak or the center of the the charts,\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20, ## syntax for boxplot.\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = GENDER, \n           y = MATHS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n2 &lt;- a + facet_grid(class())"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_04.html#visualising-distribution",
    "href": "Hands-on_Exe/Hands-on_Ex_04.html#visualising-distribution",
    "title": "Hands-On Exercise 04",
    "section": "",
    "text": "Visualising distribution is the first steps to understand and analyse the assigned data set,\n\n\nIn this Hands-on, we will be using the following packages:\n\nggdist: to plot ridglines, which will be useful to see the distribution across another dimension, such as changes in scores across Class.\nggthemes: for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse, gridExtra)\n\n\n\n\nThe same dataset exam data is being used for this exercise, as previously.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can see that in general 3A has the highest English score for when compared to other classes.\n\n\n\n\n\nCode for visualising English scores using geom_ridgeline_gradient(),\nThis function may not be useful and it may create confusion among the viewers.\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\nVisualising the Maths scores.\n\nggplot(exam, \n       aes(x = MATHS, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"Maths grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\necdf refers to the empirical; cumulative distribution. This is alternatives to visualise distribution and it could handle with continuous and categorical variables. the downside of this is that it will requires more training to accurately interpret.\nFor the could chunk below put the two charts side by side for easy visualization.\n\nECDF &lt;- ggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n \nEnglish &lt;- ggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\ngrid.arrange(English, ECDF, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\nThe probability is the highest at peak or the center of the the charts,\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20, ## syntax for boxplot.\n               outlier.shape = NA)\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam, \n       aes(x = GENDER, \n           y = MATHS)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n2 &lt;- a + facet_grid(class())"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_04.html#visual-statistical-analysis",
    "href": "Hands-on_Exe/Hands-on_Ex_04.html#visual-statistical-analysis",
    "title": "Hands-On Exercise 04",
    "section": "4.2 Visual Statistical Analysis",
    "text": "4.2 Visual Statistical Analysis\n\nLoading of libraries and packages\ntidyverse is loaded for this exercise.\n\npacman::p_load(ggstatsplot)\n\nOne-sample test: gghistostats() method\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\n\nTwo-sample mean test: ggbetweenstats()\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\nOneway ANOVA Test: ggbetweenstats() method\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\nSignificant Test of Correlation: ggscatterstats\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\nSignificant Test of Association (Depedence) : ggbarstats() methods\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\n\n\nVisualising Models\n\npacman::p_load(readxl, performance, parameters, see)\n\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_04.html#model-diagnostic-checking-for-multicolinearity",
    "href": "Hands-on_Exe/Hands-on_Ex_04.html#model-diagnostic-checking-for-multicolinearity",
    "title": "Hands-On Exercise 04",
    "section": "Model Diagnostic: checking for multicolinearity:",
    "text": "Model Diagnostic: checking for multicolinearity:\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n::: callout note There are two variables with high collinearity Age_08_04 and Mfg_Year. :::\n\nModel Diagnostic: checking normality assumption\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_n &lt;- check_normality(model1)\n\n\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\nModel Diagnostic: Check model for homogeneity of variances\n\n check_h &lt;- check_heteroscedasticity(model1)\n\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\nVisualising Regression Parameters: see methods\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\nVisualising Regression Parameters: ggcoefstats() methods\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_04.html#visualising-uncertainty",
    "href": "Hands-on_Exe/Hands-on_Ex_04.html#visualising-uncertainty",
    "title": "Hands-On Exercise 04",
    "section": "4.3 Visualising Uncertainty",
    "text": "4.3 Visualising Uncertainty\n\nLoading of Libraries\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\nLoading of Exam data\nAs the exam data has been loaded previously, there is no need to reload it the current exercise.\n\n\nVisualizing the uncertainty of point estimates: ggplot2 methods\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\nPlotting standard error bars of point estimates\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\n\nPlotting confidence interval of point estimates\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Uncertainty: ggdist package\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\nMean Point with 95% and 99% interval respectively.\n\nexam_mean_95 &lt;- exam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Mean Point + 95% Multiple-interval plot\")\n\n\nexam_mean_99 &lt;- exam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.99) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Mean Point + 99 % Multiple-interval plot\")\n\n\n\ngrid.arrange(exam_mean_95, exam_mean_99, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the uncertainty of point estimates: ggdist methods\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(\n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\nonly showing 95% confidencen Interval\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",\n    .width = 0.95,\n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\n\n\nVisualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n\n\n\nVisualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_04.html#funnel-plots-for-fair-comparisons",
    "href": "Hands-on_Exe/Hands-on_Ex_04.html#funnel-plots-for-fair-comparisons",
    "title": "Hands-On Exercise 04",
    "section": "4.4 Funnel Plots for Fair Comparisons",
    "text": "4.4 Funnel Plots for Fair Comparisons\n\nLoading of R packages\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #change in x axis range\n  yrange = c(0, 0.05),   #change in y axis range\n  title = 'Funnel plot using data_type = PR'\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  xrange = c(0, 6500),  #&lt;&lt; limiting the X axis range\n  yrange = c(0, 0.05),  #&lt;&lt; limiting the Y axis range\n  title = 'Funnel plot using data_type = SR'\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 1 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\nFunnelPlotR methods: Makeover 2\n\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\nFunnel Plot for Fair Visual Comparison: ggplot2 methods\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\nCalculate lower and upper limits for 95% and 99.9% CI\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\nPlotting a static funnel plot\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\nInteractive Funnel Plot: plotly + ggplot2\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_02.html",
    "href": "Hands-on_Exe/Hands-on_Ex_02.html",
    "title": "Hands-on Exercise 02",
    "section": "",
    "text": "In this hands-one exercise, 6 R packages are used. Beside tidyverse which were used in Hands-on Exercise 1 and dplyr which is used to transform the dataframe to create meaningful label. The following packages are introduced.\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse, gridExtra, dplyr \n               ) \n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_02.html#loading-of-the-required-libraries",
    "href": "Hands-on_Exe/Hands-on_Ex_02.html#loading-of-the-required-libraries",
    "title": "Hands-on Exercise 02",
    "section": "",
    "text": "In this hands-one exercise, 6 R packages are used. Beside tidyverse which were used in Hands-on Exercise 1 and dplyr which is used to transform the dataframe to create meaningful label. The following packages are introduced.\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse, gridExtra, dplyr \n               ) \n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_02.html#beyond-ggplot2-annotation-ggrepel",
    "href": "Hands-on_Exe/Hands-on_Ex_02.html#beyond-ggplot2-annotation-ggrepel",
    "title": "Hands-on Exercise 02",
    "section": "Beyond ggplot2 Annotation: ggrepel",
    "text": "Beyond ggplot2 Annotation: ggrepel\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\nData preparation for label and data to be plotted.\nTo make the above chart more meaningful, the following codes is to prepare the label so that the target audience of the chart could easily view the student IDs and the gender who scored higher for English and Chinese.\nIn this scenario, we are interested in students who scored above 80 for both subjects.\n\nexam_data &lt;- exam_data %&gt;%\n  mutate(ID_label=gsub(\"Student\", \"s_\",ID),\n         Gender_label= recode(GENDER, \"Female\"=\"F\", \"Male\" =\"M\"),\n         Label=str_c(ID_label,Gender_label,sep=\"\"))\n\nfiltered_data &lt;- exam_data %&gt;%\n  filter(MATHS &gt; 80, ENGLISH &gt; 80)\n\n\nggplot(data = filtered_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              size = 1) +  \n  geom_label(aes(label = Label, fill = GENDER,), \n             hjust = 0.1, \n             vjust = -0.1) +\n  scale_fill_manual(values = c(\"Female\" = \"pink\", \"Male\" = \"lightblue\")) +\n  coord_cartesian(xlim = c(80, 100),\n                  ylim = c(80, 100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3 for students above 80 \")+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nWorking with ggrepel\nPlotting of the best fit line with outliners being labelled.\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_02.html#beyond-ggplot2-themes",
    "href": "Hands-on_Exe/Hands-on_Ex_02.html#beyond-ggplot2-themes",
    "title": "Hands-on Exercise 02",
    "section": "Beyond ggplot2 Themes",
    "text": "Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, they are: theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\ntheme_gray()\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\nWorking with ggtheme package: theme_economist()\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\nWorking with hrbthems package\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_02.html#beyond-single-graph",
    "href": "Hands-on_Exe/Hands-on_Ex_02.html#beyond-single-graph",
    "title": "Hands-on Exercise 02",
    "section": "Beyond Single Graph",
    "text": "Beyond Single Graph\nWhen crafting the data story, it is common to represent the different dimensions of the data using multiple graphs and displaying them on the same page/slide to better illustrate and support the trends and observations.\nThis can be done using using function such as grid.arrange of the gridEXtra package (as used in Hands-on Exercise 1) and plot_grid() of cowplot package.\nIn this section, we will be using ggplot2 extension called pactchwork which is designed for combining seperate ggplot charts into one single image.\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\nUse of grid.arrange in gridExtra package\n\ngrid.arrange(p1,p2, p3, ncol=3)\n\n\n\n\n\n\n\n\n\n\nCreating Composite Graphics: pathwork methods\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\nCombining three ggplot2 graphs\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\n\n\n\nCreating figure with insert\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\nThe inset element can be modified to be at the bottom too by adjusting the numbers in the function. But it is important to take note that the insert chart should not cover any points of the graph.\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.01, \n                   right = 0.5, \n                   top = 0.3)\n\n\n\n\n\n\n\n\n\n\nCreating a composite figure by using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_00.html",
    "href": "Hands-on_Exe/Hands-on_Ex_00.html",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "",
    "text": "Load tidyverse into r environment by using the code chuck below.\n\npacman::p_load(tidyverse, psych)\n\nReading “read_csv” to create tibble dataframe"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_00.html#getting-started",
    "href": "Hands-on_Exe/Hands-on_Ex_00.html#getting-started",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "",
    "text": "Load tidyverse into r environment by using the code chuck below.\n\npacman::p_load(tidyverse, psych)\n\nReading “read_csv” to create tibble dataframe"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_00.html#importing-data",
    "href": "Hands-on_Exe/Hands-on_Ex_00.html#importing-data",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "Importing data",
    "text": "Importing data\n\nrealis_2019 &lt;- read_csv(\"data/REALIS2019.csv\")\n\n\nPivoting data\n\npopdata_fat&lt;-read_csv(\"data/PopData2019_fat.csv\")\n\n\npopdata_long &lt;- popdata_fat %&gt;%\n  pivot_longer(c(3:21),\n               names_to = \"Age Group\",\n               values_to = \"Population\")\n\n\nwrite_rds(popdata_long, \"data/rds/popdata_long.rds\")\n\n\nrealis2019_selected &lt;- realis_2019 %&gt;%\n  select(`Project Name`,\n  `Transacted Price ($)`,\n  `Property Type`)\nrealis2019_selected\n\n# A tibble: 19,515 × 3\n   `Project Name`           `Transacted Price ($)` `Property Type`    \n   &lt;chr&gt;                                     &lt;dbl&gt; &lt;chr&gt;              \n 1 PEIRCE VIEW                              840000 Condominium        \n 2 FLORIDA PARK                            3040000 Semi-Detached House\n 3 BULLION PARK                             860000 Condominium        \n 4 CASTLE GREEN                            1000000 Condominium        \n 5 HAPPY ESTATE                            7000000 Semi-Detached House\n 6 TEACHER'S HOUSING ESTATE                2880000 Terrace House      \n 7 THE PANORAMA                            1510000 Condominium        \n 8 THE PANORAMA                             710000 Condominium        \n 9 CHIP THYE GARDEN                        2800000 Terrace House      \n10 TEACHER'S HOUSING ESTATE                2300000 Terrace House      \n# ℹ 19,505 more rows\n\n\n\nrealis_2019_filtered &lt;- realis2019_selected %&gt;%\n  filter('Property Type'== \"Condominium\"|\n  'Property Type' == \"Apartment\") %&gt;% \n  filter('Type of Sales'== \"New Sales\") %&gt;%\n  filter('Unit Prfice ($ psm)'&lt;= 13000)\n\n\n\nPutting them together\n\nrealis_2019_filtered &lt;- realis_2019 %&gt;%\n  select(`Project Name`,\n  `Transacted Price ($)`,\n  `Property Type`) %&gt;%\n  filter('Property Type'== \"Condominium\"|\n  'Property Type' == \"Apartment\") %&gt;% \n  filter('Type of Sales'== \"New Sales\") %&gt;%\n  filter('Unit Prfice ($ psm)'&lt;= 13000)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html",
    "title": "Hands-on Exercise 01",
    "section": "",
    "text": "Loaded gridExtra library to display the charts side by side\n\npacman:: p_load(tidyverse, gridExtra )"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html#loading-of-the-required-libraries",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html#loading-of-the-required-libraries",
    "title": "Hands-on Exercise 01",
    "section": "",
    "text": "Loaded gridExtra library to display the charts side by side\n\npacman:: p_load(tidyverse, gridExtra )"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html#introducing-ggplot",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html#introducing-ggplot",
    "title": "Hands-on Exercise 01",
    "section": "Introducing ggplot",
    "text": "Introducing ggplot\n\nexam_data &lt;-read_csv(\"data/Exam_data.csv\")\n\n\nR Graphics VS ggplot\n\npar(mfrow = c(1, 3))\n\nHis_maths &lt;- hist(exam_data$MATHS)\n\nHis_eng &lt;- hist(exam_data$ENGLISH)\n\nHis_Sci &lt;- hist(exam_data$SCIENCE)\n\n\n\n\n\n\n\n\n\n\nGrammar of Graphics\n\nggplot(data = exam_data)\n\n\n\n\n\n\n\n\nEssential Grammatical Elements in ggplot2: Aesthetic mappings\n\nggplot(data=exam_data, \n      aes(x=MATHS))"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html#essential-grammatical-elements-in-ggplot2-geom",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html#essential-grammatical-elements-in-ggplot2-geom",
    "title": "Hands-on Exercise 01",
    "section": "Essential Grammatical Elements in ggplot2: geom",
    "text": "Essential Grammatical Elements in ggplot2: geom\nGeometric objects are the actual data marks being ploteed on the chat. Examples include:\n\n\n\nCopied from Prof Kam webpage: https://r4va.netlify.app/chap01#working-with-theme\n\n\nA plot must have at least one geom; there is no upper limit.\nYou can add a geom to a plot using the + operator. For complete list, please refer to here.\n\nGeometric Objects: geom_bar\n\nggplot(data=exam_data,\n       aes(x=RACE)) + \n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data,\n       aes(x=MATHS)) +\n  geom_dotplot(dotsize=0.5,fill=\"red\")\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data,\n       aes(x=MATHS)) + \n  geom_dotplot(binwidth = 2.5,\n               dotsize = 0.5) +\n  scale_y_continuous(NULL, breaks = NULL) \n\n\n\n\n\n\n\n## scale_y_continuous function is used to turn off the y-axis and the breaks = NULL is required to remove the inteval.\n\n\n\nGeoetirc Objects: geom_histogram ()\nPlotting histogram with bin set to 10, instad of using the default as 30.\n\nMaths_30 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS))+\n  geom_histogram()\n\nMaths_10 &lt;- ggplot(data=exam_data, \n       aes(x=MATHS))+\n  geom_histogram(binwidth=10)\n\ngrid.arrange(Maths_30, Maths_10, ncol=2)\n\n\n\n\n\n\n\n\n\n\nModifying a geometric object by changing aes ()\n\nggplot(data=exam_data,\n       aes(x=MATHS,\n           fill=GENDER)) +\n  geom_histogram(bin=20,\n                 color=\"grey30\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference:\n\n\n\nEnter \"grDevices::colors()\"under console to find out the colours. \n\n\n\n\n\nGeometric Objects: geom-density()\nUsing of geom-density() to computes and plots kernel density estimate, which is a smoothed version of the histogram.\nThe application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density\n\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density() \n\n\n\n\n\n\n\n\nusing the same data as\n\nMaths_gender_den &lt;- ggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\nMaths_gender_Hist &lt;-ggplot(data=exam_data,\n       aes(x=MATHS,\n           fill=GENDER)) +\n  geom_histogram(bin=20,\n                 color=\"grey30\")\n\ngrid.arrange(Maths_gender_den, Maths_gender_Hist, ncol=2)\n\n\n\n\n\n\n\n\n\n\nGeometric Objects: geom_boxplot\n\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\n\nGeometric Objects: geom_point()\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\nwith_jitter&lt;- ggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)\n\nno_jitter&lt;- ggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)\n\n\ngrid.arrange(with_jitter, no_jitter, ncol=2)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html#essential-grammatical-elements-in-ggplot2-stat",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html#essential-grammatical-elements-in-ggplot2-stat",
    "title": "Hands-on Exercise 01",
    "section": "Essential Grammatical Elements in ggplot2: stat",
    "text": "Essential Grammatical Elements in ggplot2: stat\nThe following Sections describe the show the statstictally transformation of data,\nThe Statistics functions statistically transform data, usually as some form of summary. For example:\nfrequency of values of a variable (bar graph) a mean a confidence limit There are two ways to use these functions: add a stat_() function and override the default geom, or add a geom_() function and override the default stat.\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html#working-with-stat",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html#working-with-stat",
    "title": "Hands-on Exercise 01",
    "section": "Working with stat()",
    "text": "Working with stat()\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun = \"mean\",         \n               colour =\"red\",\n               size=4)     \n\n\n\n\n\n\n\n\n\nBest Fit curve on a scatterplot\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\nEssential Grammatical Elements in ggplot2: Facets"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html#facet-wrap",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html#facet-wrap",
    "title": "Hands-on Exercise 01",
    "section": "Facet wrap",
    "text": "Facet wrap\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\nThe code chunk below plots a trellis plot using facet-wrap().\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\nfacet_grid() function\n\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)\n\n\n\n\n\n\n\n\n\n\nEssential Grammatical Elements in ggplot2: Coordinates\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nChanging the y- and x-axis range\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_01.html#essential-grammatical-elements-in-ggplot2-themes",
    "href": "Hands-on_Exe/Hands-on_Ex_01.html#essential-grammatical-elements-in-ggplot2-themes",
    "title": "Hands-on Exercise 01",
    "section": "Essential Grammatical Elements in ggplot2: Themes",
    "text": "Essential Grammatical Elements in ggplot2: Themes\nThemes control elements of the graph not related to the data. such as background colour, size of fonts, gridlines colour of labels Built-in themes include: - theme_gray() (default) - theme_bw() - theme_classic()\nA list of theme can be found at this link. Each theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\nPlotting with theme classic\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nplotting with theme minimal\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_minimal() # minimal effects\n\n\n\n\n\n\n\n\n\n\nPlotting with theme bw\n\nggplot (data=exam_data, \n        aes(x=RACE))+\n  geom_bar()+\n  coord_flip() +\n  theme_bw() +\n  ggtitle(\"Distribution of Races in Exam Data\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_03.html",
    "href": "Hands-on_Exe/Hands-on_Ex_03.html",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "The learning outcome of this exercise is to learn how to create interactive data visualistaion by using functions in ggiraph and plotlyr packages. The material of this page is referred to Prof Kam’s webpage allocated for Hands-on Exercise 03.\n\n\n\nggiraph package makes ‘ggplot’ graphics interactive.\nplotly is for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse is part of the family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork is for combining multiple ggplot2 graphs into one figure.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip (data label): a column of data-sets that contain tooltips or data label to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements. If it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides.\nRefer to this article for more detail explanation.\n\n\n\n\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomisation of Tooltips\n\n\n\nIf the desired Tool Tips or Data labels consisted of multiple fields, they can be customised using a few lines of codes with formatting so it will be shown up according to desired.\n\n\n\n# Cutomisation of Tooltips\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\n\n\n\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\ntooltip_css &lt;- \"background-color:white;\nfont-style:bold; color:black;\" \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css))\n)                     \n\n\n\n\n\n\n\n\n\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNoted that in the code argument, ” data_id= CLASS” so that students in the same class are being highlighted. In the earlier example, “tooltips = ID” is used to the student ID will be shown.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)\n\n\n\n\n\nStyling hover effect\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)          \n\n\n\n\n\nDifferent from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)             \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk below plotted the exam data for Maths and science in a scatter plot. Data points/ students in the same CLASS will be highlighted upon mouse over. At the same time, the tooltip will show the student and Class highlighted the students from the same class. The gender are plotted in different colours to allow viewers to make quick sense of data maths and science and maths scores in each class and if there are any gaps in gender.\nBased on the data below, it is observed that students from 3A has higher score in Maths and Science compared to students from class 3I.\n\n\n\ngg_scatter &lt;- ggplot(\n  data = exam_data,\n  mapping = aes(\n    x = MATHS, y = SCIENCE, color = GENDER,\n    # here we add iteractive aesthetics\n    tooltip = exam_data$tooltip, data_id = CLASS #using the tooltip or data label derived earlier part of the Hands-on Exercise\n  )) + geom_point_interactive(\n    size = 3, hover_nearest = TRUE)\n\ncss_default_hover &lt;- girafe_css_bicolor(primary = \"yellow\", secondary = \"red\")\n\nset_girafe_defaults(\n  opts_hover = opts_hover(css = css_default_hover),\n  opts_zoom = opts_zoom(min = 1, max =200 ),\n  opts_tooltip = opts_tooltip(css = \"padding:3px;background-color:#333333;color:white;\"),\n  opts_sizing = opts_sizing(rescale = TRUE),\n  opts_toolbar = opts_toolbar(saveaspng = FALSE, position = \"bottom\", delay_mouseout = 5000)\n)\ngirafe(ggobj = gg_scatter)\n\n\n\n\n\n\n\n\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618) \n\n\n\n\n\n\n\n\nCoordinated multiple views methods has been implemented in the data visualization below\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\nAppropriate interactive functions of ggiraph will be used to create the multiple views. patchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\n\n\n\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\nThere are two ways to create interactive graph by using plotly, they are:\nby using plot_ly(), and by using ggplotly()\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~GENDER)\n\n\n\n\n\n####creating an interactive scatter plot: ggplotly() method\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\nhighlight_key() of plotly package is used as shared data. two scatterplots will be created by using ggplot2 functions. lastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\n\n\nInteractive Data Table: DT package A wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n \ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk:\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_03.html#programming-interactive-data-visualisation-with-r",
    "href": "Hands-on_Exe/Hands-on_Ex_03.html#programming-interactive-data-visualisation-with-r",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "The learning outcome of this exercise is to learn how to create interactive data visualistaion by using functions in ggiraph and plotlyr packages. The material of this page is referred to Prof Kam’s webpage allocated for Hands-on Exercise 03.\n\n\n\nggiraph package makes ‘ggplot’ graphics interactive.\nplotly is for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse is part of the family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork is for combining multiple ggplot2 graphs into one figure.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip (data label): a column of data-sets that contain tooltips or data label to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements. If it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides.\nRefer to this article for more detail explanation.\n\n\n\n\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_03.html#by-hovering-the-mouse-pointer-on-an-data-point-of-interest-the-students-id-and-class-will-be-displayed.",
    "href": "Hands-on_Exe/Hands-on_Ex_03.html#by-hovering-the-mouse-pointer-on-an-data-point-of-interest-the-students-id-and-class-will-be-displayed.",
    "title": "Hands-on_Ex03",
    "section": "",
    "text": "Customisation of Tooltips\n\n\n\nIf the desired Tool Tips or Data labels consisted of multiple fields, they can be customised using a few lines of codes with formatting so it will be shown up according to desired.\n\n\n\n# Cutomisation of Tooltips\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\n\n\n\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\ntooltip_css &lt;- \"background-color:white;\nfont-style:bold; color:black;\" \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css))\n)                     \n\n\n\n\n\n\n\n\n\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNoted that in the code argument, ” data_id= CLASS” so that students in the same class are being highlighted. In the earlier example, “tooltips = ID” is used to the student ID will be shown.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)\n\n\n\n\n\nStyling hover effect\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)          \n\n\n\n\n\nDifferent from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)             \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk below plotted the exam data for Maths and science in a scatter plot. Data points/ students in the same CLASS will be highlighted upon mouse over. At the same time, the tooltip will show the student and Class highlighted the students from the same class. The gender are plotted in different colours to allow viewers to make quick sense of data maths and science and maths scores in each class and if there are any gaps in gender.\nBased on the data below, it is observed that students from 3A has higher score in Maths and Science compared to students from class 3I.\n\n\n\ngg_scatter &lt;- ggplot(\n  data = exam_data,\n  mapping = aes(\n    x = MATHS, y = SCIENCE, color = GENDER,\n    # here we add iteractive aesthetics\n    tooltip = exam_data$tooltip, data_id = CLASS #using the tooltip or data label derived earlier part of the Hands-on Exercise\n  )) + geom_point_interactive(\n    size = 3, hover_nearest = TRUE)\n\ncss_default_hover &lt;- girafe_css_bicolor(primary = \"yellow\", secondary = \"red\")\n\nset_girafe_defaults(\n  opts_hover = opts_hover(css = css_default_hover),\n  opts_zoom = opts_zoom(min = 1, max =200 ),\n  opts_tooltip = opts_tooltip(css = \"padding:3px;background-color:#333333;color:white;\"),\n  opts_sizing = opts_sizing(rescale = TRUE),\n  opts_toolbar = opts_toolbar(saveaspng = FALSE, position = \"bottom\", delay_mouseout = 5000)\n)\ngirafe(ggobj = gg_scatter)\n\n\n\n\n\n\n\n\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618) \n\n\n\n\n\n\n\n\nCoordinated multiple views methods has been implemented in the data visualization below\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nIn order to build a coordinated multiple views as shown in the example above, the following programming strategy will be used:\nAppropriate interactive functions of ggiraph will be used to create the multiple views. patchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\n\n\n\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\nThere are two ways to create interactive graph by using plotly, they are:\nby using plot_ly(), and by using ggplotly()\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~GENDER)\n\n\n\n\n\n####creating an interactive scatter plot: ggplotly() method\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\nhighlight_key() of plotly package is used as shared data. two scatterplots will be created by using ggplot2 functions. lastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\n\n\nInteractive Data Table: DT package A wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n \ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk:\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_03.html#programming-animated-statistical-graphics-with-r",
    "href": "Hands-on_Exe/Hands-on_Ex_03.html#programming-animated-statistical-graphics-with-r",
    "title": "Hands-on_Ex03",
    "section": "Programming Animated Statistical Graphics with R",
    "text": "Programming Animated Statistical Graphics with R\nThe 2nd part of the exercise is use animated graphic to attract the interest of the viewer and leave a deeper impression as compared to the static charts and graphs.\n\nLoading the R packages\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_03.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Exe/Hands-on_Ex_03.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on_Ex03",
    "section": "Animated Data Visualisation: gganimate methods",
    "text": "Animated Data Visualisation: gganimate methods\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young')"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_03.html#building-the-animated-bubble-plot",
    "href": "Hands-on_Exe/Hands-on_Ex_03.html#building-the-animated-bubble-plot",
    "title": "Hands-on_Ex03",
    "section": "Building the animated bubble plot",
    "text": "Building the animated bubble plot\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear') \n\n\n\n\n\n\n\n\n\nAnimated Data Visualisation: plotly\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_03.html#building-an-animated-bubble-plot-plot_ly-method",
    "href": "Hands-on_Exe/Hands-on_Ex_03.html#building-an-animated-bubble-plot-plot_ly-method",
    "title": "Hands-on_Ex03",
    "section": "Building an animated bubble plot: plot_ly() method",
    "text": "Building an animated bubble plot: plot_ly() method\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608 -Visual Analytics and Applications Course Website",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications\nIn this website, you will find my coursework prepared for this course.\nThe reference materials used in this course are found in Prof Kam Tin Seong’s course Webpage.\nhttps://isss608-ay2024-25jan.netlify.app/"
  },
  {
    "objectID": "index.html#words-of-encouragement-to-self",
    "href": "index.html#words-of-encouragement-to-self",
    "title": "ISSS608 -Visual Analytics and Applications Course Website",
    "section": "Words of encouragement to self",
    "text": "Words of encouragement to self"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_05.html",
    "href": "Hands-on_Exe/Hands-on_Ex_05.html",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "pacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_05.html#hands-on-exercise-5.1",
    "href": "Hands-on_Exe/Hands-on_Ex_05.html#hands-on-exercise-5.1",
    "title": "Hands-On Exercise 05",
    "section": "",
    "text": "pacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_05.html#visual-correlation-analysis",
    "href": "Hands-on_Exe/Hands-on_Ex_05.html#visual-correlation-analysis",
    "title": "Hands-On Exercise 05",
    "section": "5.2 Visual Correlation Analysis",
    "text": "5.2 Visual Correlation Analysis\nLoading fo libraries for this exercise\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\nBuilding Correlation Matrix: pairs() method\n\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\nIncluding with correlation coefficients\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\nVisualising Correlation Matrix: ggcormat()\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\n\nwine.cor &lt;- cor(wine[, 1:11])\n\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_05.html#heatmap-for-visualising-and-analysing-multivariate-data",
    "href": "Hands-on_Exe/Hands-on_Ex_05.html#heatmap-for-visualising-and-analysing-multivariate-data",
    "title": "Hands-On Exercise 05",
    "section": "5.3 Heatmap for Visualising and Analysing Multivariate Data",
    "text": "5.3 Heatmap for Visualising and Analysing Multivariate Data\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\nrow.names(wh) &lt;- wh$Country\n\n\nTransforming the data frame into a matrix\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\nheatmaply(mtcars)\n\n\n\n\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\nWorking with colour palettes\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\nThe finishing touch\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_05.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "href": "Hands-on_Exe/Hands-on_Ex_05.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "title": "Hands-On Exercise 05",
    "section": "5.4 Visual Multivariate Analysis with Parallel Coordinates Plot",
    "text": "5.4 Visual Multivariate Analysis with Parallel Coordinates Plot\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\nRotating x-axis text label\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\n\n\nThe basic plot\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_05.html#treemap-visualisation-with-r",
    "href": "Hands-on_Exe/Hands-on_Ex_05.html#treemap-visualisation-with-r",
    "title": "Hands-On Exercise 05",
    "section": "5.5 Treemap Visualisation with R",
    "text": "5.5 Treemap Visualisation with R\n\npacman::p_load(treemap, treemapify, tidyverse) \n\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nDesigning Treemap using treemapify Package\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\nDefining Hierarchy\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\nAdding boundary line\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\n\n\n\nDesigning Interactive Treemap using d3treeR\ninstall.packages(“devtools”)\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\nlibrary(d3treeR)\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "In-class_Exe/In-class_Ex05.html",
    "href": "In-class_Exe/In-class_Ex05.html",
    "title": "In-class 05",
    "section": "",
    "text": "pacman::p_load(tidyverse, readxl, SmartEDA, easystats,gtsummary, ggstatsplot)\n\n\nImporting Data\n\ncar_resale &lt;-\n  read_xls(\"data/In_Class_Ex05/ToyotaCorolla.xls\", \"data\")\n\n\nsummary(car_resale)\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\n\nlist(car_resale)\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\nsummary1  &lt;- car_resale %&gt;%\n  ExpData(type=1)\n\nmaking it into object class and further transform into a table in R\n\ncar_resale %&gt;% \n  ExpData(type=2)\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id       numeric     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month       numeric     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin     character     1436             0              0\n12    12           CC_bin     character     1436             0              0\n13    13            Doors       numeric     1436             0              0\n14    14            Gears       numeric     1436             0              0\n15    15        Cylinders       numeric     1436             0              0\n16    16        Fuel_Type     character     1436             0              0\n17    17            Color     character     1436             0              0\n18    18        Met_Color       numeric     1436             0              0\n19    19        Automatic       numeric     1436             0              0\n20    20    Mfr_Guarantee       numeric     1436             0              0\n21    21  BOVAG_Guarantee       numeric     1436             0              0\n22    22              ABS       numeric     1436             0              0\n23    23         Airbag_1       numeric     1436             0              0\n24    24         Airbag_2       numeric     1436             0              0\n25    25            Airco       numeric     1436             0              0\n26    26  Automatic_airco       numeric     1436             0              0\n27    27    Boardcomputer       numeric     1436             0              0\n28    28        CD_Player       numeric     1436             0              0\n29    29     Central_Lock       numeric     1436             0              0\n30    30  Powered_Windows       numeric     1436             0              0\n31    31   Power_Steering       numeric     1436             0              0\n32    32            Radio       numeric     1436             0              0\n33    33        Mistlamps       numeric     1436             0              0\n34    34      Sport_Model       numeric     1436             0              0\n35    35 Backseat_Divider       numeric     1436             0              0\n36    36     Metallic_Rim       numeric     1436             0              0\n37    37   Radio_cassette       numeric     1436             0              0\n38    38          Tow_Bar       numeric     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2\n\n\n\ncols &lt;- c(\"Mfg_Month\",\"HP_Bin\",\"CC_bin\",\"Doors\",\"Gears\",\"Cylinders\",\"Fuel_Type\",\"Color\",\"Met_Color\",\"Automatic\",\"Mfr_Guarantee\",\"BOVAG_Guarantee\",\"ABS\",\"Airbag_1\",\"Airbag_2\",\"Airco\",\"Automatic_airco\",\"Boardcomputer\",\"CD_Player\",\"Central_Lock\",\"Powered_Windows\",\"Power_Steering\",\"Radio\",\"Mistlamps\",\"Sport_Model\",\"Backseat_Divider\",\"Metallic_Rim\",\"Radio_cassette\",\"Tow_Bar\")\n\n\ncar_resale &lt;- read_xls (\"data/In_Class_Ex05/ToyotaCorolla.xls\",\n                        sheet = \"data\") %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_each(funs(factor(.)),cols)\n\n\ncar_resale %&gt;%\n  ExpNumViz(target=NULL,\n            nlim=10,\n            Page = c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## page(2 2,) is to define the number of chart to be added in one plot. \n\n\ncar_resale %&gt;%\n  ExpNumViz(target = \"Price\",\n            nlim=10,\n            Page=c(2,2))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\ncar_resale %&gt;%\n  ExpCatViz(target =NULL,\n            col=\"Sky blue\",\n            clim=10,\n            margin=2,\n            Page=c(4,4),\n            sample=16)\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n model &lt;- lm(Price ~ Age_08_04 + KM + Mfg_Year+\n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_c&lt;-check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n model1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\ncheck_normality(model1)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\nwe can use the checkc to plot it individually.\nfor homogeneity of variance, we can see there are two set of data, we should build two model instead of 1 model.\n\nsummary(model1)\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10249.4   -768.6    -15.4    738.5   6356.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -2.186e+03  9.722e+02  -2.248   0.0247 *  \nAge_08_04        -1.195e+02  2.760e+00 -43.292   &lt;2e-16 ***\nKM               -2.406e-02  1.201e-03 -20.042   &lt;2e-16 ***\nWeight            1.972e+01  8.379e-01  23.533   &lt;2e-16 ***\nGuarantee_Period  2.682e+01  1.261e+01   2.126   0.0336 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1413 on 1431 degrees of freedom\nMultiple R-squared:  0.8486,    Adjusted R-squared:  0.8482 \nF-statistic:  2005 on 4 and 1431 DF,  p-value: &lt; 2.2e-16\n\n\n\ntbl_regression(model1, intercept=TRUE)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n-2,186\n-4,093, -278\n0.025\n\n\nAge_08_04\n-119\n-125, -114\n&lt;0.001\n\n\nKM\n-0.02\n-0.03, -0.02\n&lt;0.001\n\n\nWeight\n20\n18, 21\n&lt;0.001\n\n\nGuarantee_Period\n27\n2.1, 52\n0.034\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\n\ntbl_regression(model1, \n               intercept=TRUE) %&gt;%\n  add_glance_source_note(\n    label=list(sigma~\"\\U03C3\"),\n    include = c(r.squared, adj.r.squared,\n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\np_model1&lt;-parameters(model1)\n\n\nplot(parameters (model1))\n\n\n\n\n\n\n\n\n\nggcoefstats(model1,\n            output=\"plot\")"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#importing-of-data-and-data-preparation.",
    "href": "Take-home_Exe/Take-home_Ex_01.html#importing-of-data-and-data-preparation.",
    "title": "Take-Home Exercise 01",
    "section": "2.0 Importing of Data and Data Preparation.",
    "text": "2.0 Importing of Data and Data Preparation.\n\n2.1 Importing data.\nAs the data is in csv file format, the data is loaded using read_csv function.\n\nheart &lt;- read_csv(\"data/Ex01/japan_heart_attack_dataset.csv\")\n\n\n\n2.2 Data Preparation\nAs the metadata did not specify the data in the extra column 1 to 15, hence we will not be able the data in the “Extra_column 1 to 15”\nThe first step of data preparation is to remove them first, and remove population who are above 64.\n\nheart &lt;- heart %&gt;% select (1:17) %&gt;% filter (Age &lt; 65)\n\nThe summary and head functions are used to view and examine the data before proceeding further.\n\nsummary(heart)\n\n      Age          Gender             Region          Smoking_History   \n Min.   :18.0   Length:22671       Length:22671       Length:22671      \n 1st Qu.:29.0   Class :character   Class :character   Class :character  \n Median :41.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   :40.9                                                           \n 3rd Qu.:53.0                                                           \n Max.   :64.0                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:22671       Length:22671         Min.   : 80.02    Length:22671      \n Class :character   Class :character     1st Qu.:179.45    Class :character  \n Mode  :character   Mode  :character     Median :199.72    Mode  :character  \n                                         Mean   :199.83                      \n                                         3rd Qu.:220.10                      \n                                         Max.   :311.24                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:22671       Length:22671        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.637   1st Qu.:21.64  \n Mode  :character   Mode  :character    Median : 4.973   Median :24.97  \n                                        Mean   : 4.988   Mean   :25.00  \n                                        3rd Qu.: 6.337   3rd Qu.:28.39  \n                                        Max.   :10.000   Max.   :44.12  \n   Heart_Rate      Systolic_BP     Diastolic_BP    Family_History    \n Min.   : 30.59   Min.   : 63.1   Min.   : 39.95   Length:22671      \n 1st Qu.: 63.23   1st Qu.:109.8   1st Qu.: 73.23   Class :character  \n Median : 69.92   Median :119.9   Median : 80.11   Mode  :character  \n Mean   : 69.93   Mean   :120.0   Mean   : 80.03                     \n 3rd Qu.: 76.63   3rd Qu.:130.2   3rd Qu.: 86.78                     \n Max.   :108.78   Max.   :176.6   Max.   :117.66                     \n Heart_Attack_Occurrence\n Length:22671           \n Class :character       \n Mode  :character       \n                        \n                        \n                        \n\n\n\nhead (heart)\n\n# A tibble: 6 × 17\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    46 Male   Rural  Yes             No               No                  \n3    32 Female Urban  No              No               No                  \n4    60 Female Rural  No              No               No                  \n5    25 Female Rural  No              No               No                  \n6    38 Female Urban  Yes             No               No                  \n# ℹ 11 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;\n\n\n\n\n2.3 Check for missing values\nis.na function is used to check if the data contain any missing values,if there are, they are being added.\n\ncolSums(is.na(heart))\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence \n                      0                       0 \n\n\nAfter removing the Extra columns, we continue by examining again the data using the summary and head functions to ensure that the extra columns are successfully removed.\n\nsummary(heart)\n\n      Age          Gender             Region          Smoking_History   \n Min.   :18.0   Length:22671       Length:22671       Length:22671      \n 1st Qu.:29.0   Class :character   Class :character   Class :character  \n Median :41.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   :40.9                                                           \n 3rd Qu.:53.0                                                           \n Max.   :64.0                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:22671       Length:22671         Min.   : 80.02    Length:22671      \n Class :character   Class :character     1st Qu.:179.45    Class :character  \n Mode  :character   Mode  :character     Median :199.72    Mode  :character  \n                                         Mean   :199.83                      \n                                         3rd Qu.:220.10                      \n                                         Max.   :311.24                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:22671       Length:22671        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.637   1st Qu.:21.64  \n Mode  :character   Mode  :character    Median : 4.973   Median :24.97  \n                                        Mean   : 4.988   Mean   :25.00  \n                                        3rd Qu.: 6.337   3rd Qu.:28.39  \n                                        Max.   :10.000   Max.   :44.12  \n   Heart_Rate      Systolic_BP     Diastolic_BP    Family_History    \n Min.   : 30.59   Min.   : 63.1   Min.   : 39.95   Length:22671      \n 1st Qu.: 63.23   1st Qu.:109.8   1st Qu.: 73.23   Class :character  \n Median : 69.92   Median :119.9   Median : 80.11   Mode  :character  \n Mean   : 69.93   Mean   :120.0   Mean   : 80.03                     \n 3rd Qu.: 76.63   3rd Qu.:130.2   3rd Qu.: 86.78                     \n Max.   :108.78   Max.   :176.6   Max.   :117.66                     \n Heart_Attack_Occurrence\n Length:22671           \n Class :character       \n Mode  :character       \n                        \n                        \n                        \n\n\n\nhead(heart)\n\n# A tibble: 6 × 17\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    46 Male   Rural  Yes             No               No                  \n3    32 Female Urban  No              No               No                  \n4    60 Female Rural  No              No               No                  \n5    25 Female Rural  No              No               No                  \n6    38 Female Urban  Yes             No               No                  \n# ℹ 11 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;\n\n\n\nheart &lt;- heart %&gt;%\n  mutate(Age_Grp = ifelse(Age &gt; 35, \"Adult\", \"Youth\"))\n\n\n\n2.4 Transforming the data to the 4 main sub group.\nIt was observed that the data consist two types of variables: namely continuous and categorical.\nAs mentioned in section 1.0, the data will be categorised into the 4 main group.\nGiven that the continuous data are consist of variables such as heart rate, cholesterol level, Systolic and Diastolic Blood Pressure, they are being categorized to established category . For this purpose of the analysis, we will use the range indicate in the following sites and with the help of Microsoft Co-pilot.\n\nBody Mass Index (BMI)\nBlood pressure (BP)\nCholesterol Levels\nHeart Rate\n\nUsing the following codes, the data are categorised into 4 main groups and the health indicators are categorized into the established data.\n\nHA &lt;- heart %&gt;%\n  mutate(Event_Grp = ifelse(Age_Grp == \"Adult\" & Heart_Attack_Occurrence == \"Yes\", \"Adult_HA\",\n                    ifelse(Age_Grp == \"Adult\" & Heart_Attack_Occurrence == \"No\", \"Adult_No_HA\",\n                    ifelse(Age_Grp == \"Youth\" & Heart_Attack_Occurrence == \"Yes\", \"Youth_HA\",\n                    ifelse(Age_Grp == \"Youth\" & Heart_Attack_Occurrence == \"No\", \"Youth_No_HA\",NA)))))\n\nHA &lt;- HA %&gt;%\n  mutate(BMI_cat = case_when( \n    BMI &lt; 18.5 ~ \"Underweight\",\n    BMI &gt;= 18.5 & BMI &lt; 25 ~ \"Normal\",\n    BMI &gt;= 25 & BMI &lt; 30 ~ \"Overweight\",\n    BMI &gt;= 30 ~ \"Obese\",\n    TRUE ~ NA_character_\n  ))\n\nHA &lt;- HA %&gt;%\n  mutate(BP_cat = case_when(\n    Systolic_BP &lt; 120 & Diastolic_BP &lt; 80 ~ \"Normal\",\n    (Systolic_BP &gt;= 120 & Systolic_BP &lt; 130) & Diastolic_BP &lt; 80 ~ \"Elevated\",\n    (Systolic_BP &gt;= 130 & Systolic_BP &lt; 140) | (Diastolic_BP &gt;= 80 & Diastolic_BP &lt; 90) ~ \"High Stage 1\",\n    Systolic_BP &gt;= 140 | Diastolic_BP &gt;= 90 ~ \"High Stage 2\",\n    TRUE ~ NA_character_\n  ))\n\nHA &lt;- HA %&gt;%\n  mutate(Cholesterol_Level_cat = case_when(\n    Cholesterol_Level &lt; 200 ~ \"Desirable\",\n    Cholesterol_Level &gt;= 200 & Cholesterol_Level &lt; 240~ \"Borderline High\",\n    Cholesterol_Level &gt;= 240 ~ \"High\",\n    TRUE ~ NA_character_\n  ))\n\nHA &lt;- HA %&gt;%\n  mutate(Stress_Level_cat = case_when(\n    Stress_Levels &lt; 3 ~ \"Low\",\n    Stress_Levels &gt;= 3 & Stress_Levels &lt; 6 ~ \"Moderate\",\n    Stress_Levels &gt;= 6 ~ \"High\",\n    TRUE ~ NA_character_\n  ))\n\n\nHA &lt;- HA %&gt;% \n  mutate(Heart_Rate_cat = case_when(\n    Heart_Rate &lt; 70 ~ \"Below Normal\",\n    Heart_Rate &gt;=  70 & Heart_Rate &lt;= 100 ~ \"Normal\",\n    Heart_Rate  &gt; 100 ~ \"Elevated\",\n    TRUE ~ NA_character_\n  ))\n\nExamine of missing data after recoding\nnote that there are 277 recids of BMI data and 108 of chelesterol records not being\n\nmissing_values &lt;- colSums(is.na(HA))\nprint(missing_values)\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence                 Age_Grp \n                      0                       0                       0 \n              Event_Grp                 BMI_cat                  BP_cat \n                      0                       0                       0 \n  Cholesterol_Level_cat        Stress_Level_cat          Heart_Rate_cat \n                      0                       0                       0 \n\n\n\nsummary(HA)\n\n      Age          Gender             Region          Smoking_History   \n Min.   :18.0   Length:22671       Length:22671       Length:22671      \n 1st Qu.:29.0   Class :character   Class :character   Class :character  \n Median :41.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   :40.9                                                           \n 3rd Qu.:53.0                                                           \n Max.   :64.0                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:22671       Length:22671         Min.   : 80.02    Length:22671      \n Class :character   Class :character     1st Qu.:179.45    Class :character  \n Mode  :character   Mode  :character     Median :199.72    Mode  :character  \n                                         Mean   :199.83                      \n                                         3rd Qu.:220.10                      \n                                         Max.   :311.24                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:22671       Length:22671        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.637   1st Qu.:21.64  \n Mode  :character   Mode  :character    Median : 4.973   Median :24.97  \n                                        Mean   : 4.988   Mean   :25.00  \n                                        3rd Qu.: 6.337   3rd Qu.:28.39  \n                                        Max.   :10.000   Max.   :44.12  \n   Heart_Rate      Systolic_BP     Diastolic_BP    Family_History    \n Min.   : 30.59   Min.   : 63.1   Min.   : 39.95   Length:22671      \n 1st Qu.: 63.23   1st Qu.:109.8   1st Qu.: 73.23   Class :character  \n Median : 69.92   Median :119.9   Median : 80.11   Mode  :character  \n Mean   : 69.93   Mean   :120.0   Mean   : 80.03                     \n 3rd Qu.: 76.63   3rd Qu.:130.2   3rd Qu.: 86.78                     \n Max.   :108.78   Max.   :176.6   Max.   :117.66                     \n Heart_Attack_Occurrence   Age_Grp           Event_Grp        \n Length:22671            Length:22671       Length:22671      \n Class :character        Class :character   Class :character  \n Mode  :character        Mode  :character   Mode  :character  \n                                                              \n                                                              \n                                                              \n   BMI_cat             BP_cat          Cholesterol_Level_cat Stress_Level_cat  \n Length:22671       Length:22671       Length:22671          Length:22671      \n Class :character   Class :character   Class :character      Class :character  \n Mode  :character   Mode  :character   Mode  :character      Mode  :character  \n                                                                               \n                                                                               \n                                                                               \n Heart_Rate_cat    \n Length:22671      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n2.5 Re-coding Catological data for Logistic Regression\n\nHA &lt;- HA %&gt;%\n  mutate(Heart_Attack_Occurrence_num = case_when(\n      Heart_Attack_Occurrence == \"Yes\" ~ 1,\n      Heart_Attack_Occurrence == \"No\" ~ 0,\n      TRUE ~ NA_real_),\n    Gender_num = case_when(\n      Gender == \"Male\" ~ 1,\n      Gender == \"Female\" ~ 0,\n      TRUE ~ NA_real_),\n    Region_num = case_when(\n      Region == \"Urban\" ~ 1,\n      Region == \"Rural\" ~ 0,\n      TRUE ~ NA_real_),\n    Smoking_History_num = case_when(\n      Smoking_History == \"Yes\" ~ 1,\n      Smoking_History == \"No\" ~ 0,\n      TRUE ~ NA_real_),\n    Diabetes_History_num = case_when(\n      Diabetes_History == \"Yes\" ~ 1,\n      Diabetes_History == \"No\" ~ 0,\n      TRUE ~ NA_real_),\n    Hypertension_History_num = case_when(\n      Hypertension_History == \"Yes\" ~ 1,\n      Hypertension_History == \"No\" ~ 0,\n      TRUE ~ NA_real_),\n    Family_History_num = case_when(\n      Family_History == \"Yes\" ~ 1,\n      Family_History == \"No\" ~ 0,\n      TRUE ~ NA_real_),\n    Physical_Activity_num = case_when(\n      Physical_Activity == \"High\" ~ 0,\n      Physical_Activity == \"Moderate\" ~ 1,\n      Physical_Activity == \"Low\" ~ 2,\n      TRUE ~ NA_real_),\n    Diet_Quality_num = case_when(\n      Diet_Quality == \"Good\" ~ 0,\n      Diet_Quality == \"Average\" ~ 1,\n      Diet_Quality == \"Poor\" ~ 2,\n      TRUE ~ NA_real_),\n    Alcohol_Consumption_num = case_when(\n      Alcohol_Consumption == \"None\" ~ 0,\n      Alcohol_Consumption == \"Low\" ~ 1,\n      Alcohol_Consumption == \"Moderate\" ~ 2,\n      Alcohol_Consumption == \"High\" ~ 3,\n      TRUE ~ NA_real_),\n    Stress_Level_cat_num = case_when(\n      Stress_Level_cat == \"Low\" ~ 0,\n      Stress_Level_cat == \"Moderate\" ~ 1,\n      Stress_Level_cat == \"High\" ~ 2,\n      TRUE ~ NA_real_),\n    BMI_cat_num = case_when(\n      BMI_cat == \"Underweight\" ~ 0,\n      BMI_cat == \"Normal\" ~ 1,\n      BMI_cat == \"Overweight\" ~ 2,\n      BMI_cat == \"Obese\" ~ 2,\n      TRUE ~ NA_real_),\n    Heart_Rate_cat_num = case_when(\n      Heart_Rate_cat  == \"Below Normal\" ~ 0,\n      Heart_Rate_cat  == \"Normal\" ~ 1,\n      Heart_Rate_cat  == \"Elevated\" ~ 2,\n      TRUE ~ NA_real_),\n    BP_cat_num = case_when(\n      BP_cat  == \"Normal\" ~ 0,\n      BP_cat  == \"Elevated\" ~ 1,\n      BP_cat  == \"High Stage 1\" ~ 2,\n      BP_cat  == \"High Stage 2\" ~ 3,\n      TRUE ~ NA_real_),\n    Cholesterol_Level_cat_num = case_when(\n      Cholesterol_Level_cat  == \"Desirable\" ~ 0,\n      Cholesterol_Level_cat == \"Borderline High\" ~ 1,\n      Cholesterol_Level_cat  == \"High\" ~ 2,\n      TRUE ~ NA_real_))\n\nChecking of data after re-coding.\n\nmissing_values &lt;- colSums(is.na(HA))\nprint(missing_values)\n\n                        Age                      Gender \n                          0                           0 \n                     Region             Smoking_History \n                          0                           0 \n           Diabetes_History        Hypertension_History \n                          0                           0 \n          Cholesterol_Level           Physical_Activity \n                          0                           0 \n               Diet_Quality         Alcohol_Consumption \n                          0                           0 \n              Stress_Levels                         BMI \n                          0                           0 \n                 Heart_Rate                 Systolic_BP \n                          0                           0 \n               Diastolic_BP              Family_History \n                          0                           0 \n    Heart_Attack_Occurrence                     Age_Grp \n                          0                           0 \n                  Event_Grp                     BMI_cat \n                          0                           0 \n                     BP_cat       Cholesterol_Level_cat \n                          0                           0 \n           Stress_Level_cat              Heart_Rate_cat \n                          0                           0 \nHeart_Attack_Occurrence_num                  Gender_num \n                          0                           0 \n                 Region_num         Smoking_History_num \n                          0                           0 \n       Diabetes_History_num    Hypertension_History_num \n                          0                           0 \n         Family_History_num       Physical_Activity_num \n                          0                           0 \n           Diet_Quality_num     Alcohol_Consumption_num \n                          0                           0 \n       Stress_Level_cat_num                 BMI_cat_num \n                          0                           0 \n         Heart_Rate_cat_num                  BP_cat_num \n                          0                           0 \n  Cholesterol_Level_cat_num \n                          0 \n\n\n\n\n2.6 Prepare of varies data frame for easy of infographic.\n\nHA_Yes &lt;- filter(HA, Heart_Attack_Occurrence == \"Yes\")\nHA_adult &lt;- filter(HA, Age_Grp == \"Adult\")\nHA_youth &lt;- filter(HA, Age_Grp == \"Youth\")\nHA_youth_yes &lt;- filter(HA_youth, Heart_Attack_Occurrence == \"Yes\")"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#required-task",
    "href": "Take-home_Exe/Take-home_Ex_01.html#required-task",
    "title": "Take-Home Exercise 01",
    "section": "Required Task",
    "text": "Required Task\nThe main task is to develop graphical visuals for a media company on an article for the possible factors leading to heart attack trends in Japanese Youth.\nThe plan is to carry out exploratory and confirmatory data analysis to confirm the observed trend before doing up the visuals required for the graphic.\nThe goal is to find out if there are any observable trends and health or lifestyle factors leading to the occurrence of heart attack in Japanese youth.\nFor the purpose of this analysis, I define Japanese youth as individuals under the age of 35, following the definition provided in the article on Youth Employment Policies in Japan. The data set is catogrised into 4 main groups as follows:\n\nYouths who have experienced a heart attack (interest group),\nYouths who have not experienced a heart attack,\nAdults who have experienced a heart attack,\nAdults who have not experienced a heart attack.\n\nPopulation with age above 65, they will be omitted from this study as age above 65 and above are considered as elderly based on the Wikipedia."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#exploratory-data-analysis-eda",
    "href": "Take-home_Exe/Take-home_Ex_01.html#exploratory-data-analysis-eda",
    "title": "Take-Home Exercise 01",
    "section": "3.0 Exploratory Data Analysis (EDA)",
    "text": "3.0 Exploratory Data Analysis (EDA)\nWe start off by understanding the distribution of the variables and examine if there were any observable trends to be tested out using confirmatory analysis.\n\n3.1 Visualising Distribution of the Categorical Variables\nThe aim of this task is to examine if there are any trends in categorical data in the occurrence of heart attack in Japanese population.\nThe code below is extract our the categorical data and summaries into the occurrence of heart attack by Heart occurrence Age Groups and Event Groups.\nThe data are processed using the following codes to allow flexibility when exploring the data set.Hence HA is being used instead of the subgroups. Those data sets would be used in the subsequent visuslisation.\n\ncategorical_columns &lt;- HA %&gt;% select(where(~is.character(.x) || is.factor(.x)))\n\ncount_occurrences &lt;- categorical_columns %&gt;%\n  pivot_longer(cols= - c(Heart_Attack_Occurrence, Age_Grp, Event_Grp), names_to = \"Category\", values_to = \"Value\") %&gt;%\n  group_by(Category, Value, Age_Grp, Heart_Attack_Occurrence, Event_Grp) %&gt;%\n  summarise(Occurrences = n(), .groups = 'drop')\n\n\nbar_plots &lt;- count_occurrences %&gt;%\n  ggplot(aes(x = Value, y = Occurrences, fill = Heart_Attack_Occurrence)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) +  \n  facet_wrap(~ Category, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Categorical Variables of The Study Population.\",\n       x = \"Unique Values\",\n       y = \"Count\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"#4682B4\"))\n\nbar_plots\n\n\n\n\n\n\n\n\n\nThe above chart shows the data distribution for the categorical variables of the data set.\nIt includes the entire data set of the risk factors with whom had have heart attack verse whom had not have attack in Japan in all age group.\nBefore visualizing the data, one would expect risk factors such as high cholesterol level, elevated heart rate and with smoking history, will likely result in more occurrence of heart attack in but solely by looking at the distribution of the data, this may not be true.\nThe main take away from the analysis is that it seems there is no sole risk factor which will lead to heart attack with the Japanese population sampled for this data set.\n\n\n3.1.1 Closer Examination on the Distribution Categorical Variables in different Age group in Youth vs Adult\n\ncount_occurrences_yes &lt;- filter(count_occurrences,  Heart_Attack_Occurrence == \"Yes\")\n\nbar_plots_yes &lt;- count_occurrences_yes %&gt;%\n  ggplot(aes(x = Value, y = Occurrences, fill = Age_Grp)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) + \n  facet_wrap(~ Category, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Categorical Variables of subjects with Heart Attack Across Age Groups\",\n       y = \"Count\") +\nscale_fill_manual(values = c(\"Youth\" = \"#FF1493\", \"Adult\" = \"#FFA07A\"))\n\nbar_plots_yes\n\n\n\n\n\n\n\n\n\nDue to the study population, contains more subjects with heart attack, it is good to visualise the distribution across the age groups with heart attack. This may provide some interesting insights.\nThe charts above show the distribution of the categorical variables for study subjects who had heart attack across different age groups.\nAt a glance, the general distribution across categorical variables may seem to be similar across both age groups but at a closer examination. the following observations could be seen:\n\nBP_cat “high stage 1” in adults has higher occurrence in heart attack when compared to youth.\nIn youth, the occurrence of heart attack across gender while in adult a slightly in male verse female.\nThere seems to be have more occurrence in heart attack occurring in urban setting than rural setting.\nIt seem to have association between BMI and heart attack with more slight more occurrence in overweight subjects as compared to other BMI category.\nIt seems that there is association with moderate stress level with high proportion in both age groups.\n\nFrom the distribution, it seems that there is no sole factor which stand out in the youth population which will result in heart attack, confirmatory analysis would be need to confirm the analysis or explanatory model using logistic regression could be carried out to further explain risk factors.\n\n\n\n3.1.2 Closer Examination on the Distribution Categorical Variables in the Youth Age group.\n\ncount_occurrences_youth &lt;- filter(count_occurrences,  Age_Grp == \"Youth\")\n\nbar_plots_yes &lt;- count_occurrences_youth %&gt;%\n  ggplot(aes(x = Value, y = Occurrences, fill = Heart_Attack_Occurrence)) + \n  geom_bar(stat = \"identity\", position = position_dodge()) + \n  facet_wrap(~ Category, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Categorical Variables of subjects with Heart Attack in Youth\",\n       y = \"Count\") +\nscale_fill_manual(values = c(\"Yes\" = \"#FF1493\", \"No\" = \"#87CEEB\"))\n\nbar_plots_yes\n\n\n\n\n\n\n\n\n\nSimilar to the bar charts in section 3.1, there is no sole risk factor which stands out in the categorical variables which lead to high occurrence in the youth population.\n\n\n\n\n3.2 Visualising Distribution of the Continuous Variables\nThe following codes is to prepared the data to plot histogram to visualise the continuous data.\nThe data is prepared this manner so that it will provide flexibility in showing data.\n\ncontin_col_HA &lt;- heart %&gt;% select(where(is.numeric), Heart_Attack_Occurrence, Age_Grp)\n\ncontin_col_HA_youth &lt;- filter(contin_col_HA, Age_Grp == \"Youth\")  \n\ncontin_col_HA_Yes &lt;- filter(contin_col_HA, Heart_Attack_Occurrence == \"Yes\")\n\ncontin_col_HA_Long&lt;-contin_col_HA %&gt;%\n  pivot_longer(cols = -c(Heart_Attack_Occurrence, Age_Grp), names_to = \"Field\", values_to = \"Value\")\n               \ncontin_col_HA_Long_Yes &lt;- filter(contin_col_HA_Long, Heart_Attack_Occurrence == \"Yes\")\n\ncontin_col_HA_Long_youth &lt;- filter(contin_col_HA_Long, Age_Grp == \"Youth\")  \n\n\nhistograms_HA &lt;- ggplot(contin_col_HA_Long, aes(x = Value, fill = Heart_Attack_Occurrence )) + \n  geom_histogram(binwidth = 1, alpha = 0.8) + \n  theme_minimal() + \n  labs(title = \"Histograms of Continuous Variables\", y = \"Frequency\")+\n  facet_wrap(~ Field, scales = \"free\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"#4682B4\"))\n\nhistograms_HA\n\n\n\n\n\n\n\n\n\nTo visualise the continuous variables, the continuous variables were plotted using the Heart data without re-coding. This is to allow us to be able to view this distribution of the continuous variables.\nBased on the histogram, the distribution of of the continuous variables for BMI, Cholesterol Level, Diastolic and Systolic blood pressure, stress levels and heart rate seems to be over normal distribution. The overall trend is similar between across the subjects regardless if they had or had not have a heart attack.\n\n\n3.2.1 Closer look and the Youth Age Group\n\nhistograms_HA_youth &lt;- ggplot(contin_col_HA_Long_youth, aes(x = Value, fill = Heart_Attack_Occurrence )) + \n  geom_histogram(binwidth = 1, alpha = 0.8) + \n  theme_minimal() + \n  labs(title = \"Histograms of Continuous Variables\", y = \"Frequency\")+\n  facet_wrap(~ Field, scales = \"free\") +\n  scale_fill_manual(values = c(\"Yes\" = \"#FF1493\", \"No\" = \"#87CEEB\"))\n\nhistograms_HA_youth\n\n\n\n\n\n\n\n\n\nComparing the occurrence of heart attack in youth, it follows the same distribution as the over all data. It was noted that similar to the overall data, there resembles the normality in the BMI, Cholesterol, Diastolic and systolic blood pressure, heart rate and stress levels.\nThis maybe logical as these variables are measure of bodily vitals or measurements, and due to homostasis the self regulating ability of the human body it will be normal to be peak in the center.\nHence to better understand variables, it may be more meaning for categorszed them into established bench marks such as what if the rest heart rate above 100 is considered as elevated.\n\n\n#| fig-width: 24 #to widen the space\n#| fig-height: 24 #to lengthen the graph\n\nHA_youth %&gt;%\n  ExpNumViz(target = \"Heart_Attack_Occurrence\",\n            nlim=10,\n            Page=c(2,4))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\nBesides histogram, given that the distribution is resemble normal distribution, boxplot could be useful type of chart to visualise if there are any differences across the variables. It was noted that there maybe slight differences in stress levels between the youth with heart attack verse youth without heart attack."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#visualising-distribution-of-the-continuous-variables",
    "href": "Take-home_Exe/Take-home_Ex_01.html#visualising-distribution-of-the-continuous-variables",
    "title": "Take-Home Exercise 01",
    "section": "3.2 Visualising Distribution of the Continuous Variables",
    "text": "3.2 Visualising Distribution of the Continuous Variables\nThe following codes is to prepared the data to plot histogram to visualise the continuous data.\nThe data is prepared this manner so that it will provide flexibility in showing data.\n\ncontin_col_HA &lt;- heart %&gt;% select(where(is.numeric), Heart_Attack_Occurrence, Age_Grp)\n\ncontin_col_HA_youth &lt;- filter(contin_col_HA, Age_Grp == \"Youth\")  \n\ncontin_col_HA_Yes &lt;- filter(contin_col_HA, Heart_Attack_Occurrence == \"Yes\")\n\ncontin_col_HA_Long&lt;-contin_col_HA %&gt;%\n  pivot_longer(cols = -c(Heart_Attack_Occurrence, Age_Grp), names_to = \"Field\", values_to = \"Value\")\n               \ncontin_col_HA_Long_Yes &lt;- filter(contin_col_HA_Long, Heart_Attack_Occurrence == \"Yes\")\n\ncontin_col_HA_Long_youth &lt;- filter(contin_col_HA_Long, Age_Grp == \"Youth\")  \n\n\nhistograms_HA &lt;- ggplot(contin_col_HA_Long, aes(x = Value, fill = Heart_Attack_Occurrence )) + \n  geom_histogram(binwidth = 1, alpha = 0.8) + \n  theme_minimal() + \n  labs(title = \"Histograms of Continuous Variables\", y = \"Frequency\")+\n  facet_wrap(~ Field, scales = \"free\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"#4682B4\"))\n\nhistograms_HA\n\n\n\n\n\n\n\n\n\nTo visualise the continuous variables, the continuous variables were plotted using the Heart data without re-coding. This is to allow us to be able to view this distribution of the continuous variables.\nBased on the histogram, the distribution of of the continuous variables for BMI, Cholesterol Level, Diastolic and Systolic blood pressure, stress levels and heart rate seems to be over normal distribution. The overall trend is similar between across the subjects regardless if they had or had not have a heart attack.\n\n\n3.2.1 Closer look and the Youth Age Group\n\nhistograms_HA_youth &lt;- ggplot(contin_col_HA_Long_youth, aes(x = Value, fill = Heart_Attack_Occurrence )) + \n  geom_histogram(binwidth = 1, alpha = 0.8) + \n  theme_minimal() + \n  labs(title = \"Histograms of Continuous Variables\", y = \"Frequency\")+\n  facet_wrap(~ Field, scales = \"free\") +\n  scale_fill_manual(values = c(\"Yes\" = \"#FF1493\", \"No\" = \"#87CEEB\"))\n\nhistograms_HA_youth\n\n\n\n\n\n\n\n\n\nComparing the occurrence of heart attack in youth, it follows the same distribution as the over all data. It was noted that similar to the overall data, there resembles the normality in the BMI, Cholesterol, Diastolic and systolic blood pressure, heart rate and stress levels.\nThis maybe logical as these variables are measure of bodily vitals or measurements, and due to homostasis the self regulating ability of the human body it will be normal to be peak in the center.\nHence to better understand variables, it may be more meaning for categorszed them into established bench marks such as what if the rest heart rate above 100 is considered as elevated.\n\n\n#| fig-width: 24 #to widen the space\n#| fig-height: 24 #to lengthen the graph\n\nHA_youth %&gt;%\n  ExpNumViz(target = \"Heart_Attack_Occurrence\",\n            nlim=10,\n            Page=c(2,4))\n\n$`0`\n\n\n\n\n\n\n\n\n\n\nBesides histogram, given that the distribution is resemble normal distribution, boxplot could be useful type of chart to visualise if there are any differences across the variables. It was noted that there maybe slight differences in stress levels between the youth with heart attack verse youth without heart attack."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#confirmatory-data-analysis-cda",
    "href": "Take-home_Exe/Take-home_Ex_01.html#confirmatory-data-analysis-cda",
    "title": "Take-Home Exercise 01",
    "section": "4.0 Confirmatory Data Analysis (CDA)",
    "text": "4.0 Confirmatory Data Analysis (CDA)\n\n4.1 Chi Square test for youth data\nDue to there is no trend observed by examining the barcharts, the chi square was ran using all variables.\nAfter running the above associative test, the following code chunk to carry out the chi-square test to find our variables with association between heart attacks and categorical variables. The following codes on how to perform chi-square is generated using co-pilot.\n\nanalyze_association &lt;- function(data, x, y) {\n  # Perform Chi-Square test\n  table &lt;- table(data[[x]], data[[y]])\n  chi_test &lt;- chisq.test(table)\n  \n  # Print Chi-Square test results\n  print(paste(\"Chi-Square Test between\", x, \"and\", y))\n  print(chi_test)\n}\n\n# Analyze association for each categorical column with Heart_Attack_Occurrence\nfor (column in colnames(categorical_columns)) {\n  if (!(column %in% c(\"Heart_Attack_Occurrence\", \"Event_Grp\", \"Age_Grp\")))  \n    analyze_association(HA_youth, column, \"Heart_Attack_Occurrence\")\n  }\n\n[1] \"Chi-Square Test between Gender and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.058062, df = 1, p-value = 0.8096\n\n[1] \"Chi-Square Test between Region and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.25033, df = 1, p-value = 0.6168\n\n[1] \"Chi-Square Test between Smoking_History and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.16193, df = 1, p-value = 0.6874\n\n[1] \"Chi-Square Test between Diabetes_History and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 7.4232, df = 1, p-value = 0.006439\n\n[1] \"Chi-Square Test between Hypertension_History and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 1.0644, df = 1, p-value = 0.3022\n\n[1] \"Chi-Square Test between Physical_Activity and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.19892, df = 2, p-value = 0.9053\n\n[1] \"Chi-Square Test between Diet_Quality and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 3.2215, df = 2, p-value = 0.1997\n\n[1] \"Chi-Square Test between Alcohol_Consumption and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 3.487, df = 3, p-value = 0.3224\n\n[1] \"Chi-Square Test between Family_History and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.16224, df = 1, p-value = 0.6871\n\n[1] \"Chi-Square Test between BMI_cat and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 8.7346, df = 3, p-value = 0.03304\n\n[1] \"Chi-Square Test between BP_cat and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 3.0918, df = 3, p-value = 0.3777\n\n[1] \"Chi-Square Test between Cholesterol_Level_cat and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 3.9627, df = 2, p-value = 0.1379\n\n[1] \"Chi-Square Test between Stress_Level_cat and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 4.9121, df = 2, p-value = 0.08577\n\n\n[1] \"Chi-Square Test between Heart_Rate_cat and Heart_Attack_Occurrence\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.78781, df = 2, p-value = 0.6744\n\n\n\n\n4.2 Significant Test of Association (Dependence) : ggbarstats() methods\n\nA_HA_youth_BMI &lt;- ggbarstats(HA_youth, \n           x=BMI_cat,\n           y=Heart_Attack_Occurrence)\n\nA_HA_youth_Dia &lt;- ggbarstats(HA_youth, \n           x=Diabetes_History,\n           y=Heart_Attack_Occurrence)\n\n\n#| eval: false\n#| fig-width: 120 #to widen the space\n#| fig-height: 120 #to lengthen the graph\n\nPatchwork_sig &lt;- (A_HA_youth_Dia | A_HA_youth_BMI)\n\n\nPatchwork_sig \n\n\n\n\n\n\n\n\n\nUsing the ggbarstats, we are able to view the associations between diabetes history and BMI verse heart attack occurrence in youth.\nFor Diabetes history, it is clear that youth with diabetes history have higher association or may have high risk with heart attack as compared to youth without diabetes in the past.\nFor the BMI category, while the statistical test seems that BMI category may be significant, however it seems like the proportion of youth with normal BMI may result in high proportion of youth has heart attack. While it is consistent with general knowledge, it is noted the youth who are underweight has slightly less association of having heart attack.\nFurther analysis using logistic regression would be required to further understand the relationship.\n\n\n\n4.3 Chi Square for subjects with heart attack of the two different age group\n\nanalyze_association &lt;- function(data, x, y) {\n  # Perform Chi-Square test\n  table &lt;- table(data[[x]], data[[y]])\n  chi_test &lt;- chisq.test(table)\n  \n  # Print Chi-Square test results\n  print(paste(\"Chi-Square Test between\", x, \"and\", y))\n  print(chi_test)\n}\n\n# Analyze association for each categorical column with Heart_Attack_Occurrence\nfor (column in colnames(categorical_columns)) {\n  if (!(column %in% c(\"Heart_Attack_Occurrence\", \"Event_Grp\", \"Age_Grp\")))  \n    analyze_association(HA_Yes, column, \"Age_Grp\")\n}\n\n[1] \"Chi-Square Test between Gender and Age_Grp\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 1.4995, df = 1, p-value = 0.2207\n\n[1] \"Chi-Square Test between Region and Age_Grp\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.31905, df = 1, p-value = 0.5722\n\n[1] \"Chi-Square Test between Smoking_History and Age_Grp\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.53203, df = 1, p-value = 0.4658\n\n[1] \"Chi-Square Test between Diabetes_History and Age_Grp\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 4.7694, df = 1, p-value = 0.02897\n\n[1] \"Chi-Square Test between Hypertension_History and Age_Grp\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.012936, df = 1, p-value = 0.9094\n\n[1] \"Chi-Square Test between Physical_Activity and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.93543, df = 2, p-value = 0.6264\n\n[1] \"Chi-Square Test between Diet_Quality and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 1.4703, df = 2, p-value = 0.4794\n\n[1] \"Chi-Square Test between Alcohol_Consumption and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 2.6902, df = 3, p-value = 0.4419\n\n[1] \"Chi-Square Test between Family_History and Age_Grp\"\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table\nX-squared = 0.084369, df = 1, p-value = 0.7715\n\n[1] \"Chi-Square Test between BMI_cat and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.53648, df = 3, p-value = 0.9108\n\n[1] \"Chi-Square Test between BP_cat and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 5.6781, df = 3, p-value = 0.1284\n\n[1] \"Chi-Square Test between Cholesterol_Level_cat and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 1.1245, df = 2, p-value = 0.5699\n\n[1] \"Chi-Square Test between Stress_Level_cat and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.51442, df = 2, p-value = 0.7732\n\n\n[1] \"Chi-Square Test between Heart_Rate_cat and Age_Grp\"\n\n    Pearson's Chi-squared test\n\ndata:  table\nX-squared = 0.81758, df = 2, p-value = 0.6645\n\n\n\nBased on chi square test, there no statistically significant variables for the between the categorical variables and age group. Hence the earlier observations in section 3.1.1 are not valid.\n\n\n\n4.5 Anova performed on the youth group.\nAnova is performed to test the differences in true mean across continous variables for the youth with heart attack and without heart attack. The following code is steamlined with the help of Microsoft co-pilot.\n\nperform_anova &lt;- function(data, group_var, continuous_vars) {\n  results &lt;- list()\n  \n  for (var in continuous_vars) {\n    print(paste(\"Performing ANOVA for\", var, \"by\", group_var))\n    anova_result &lt;- summary(aov(as.formula(paste(var, \"~\", group_var)), data = data))\n    results[[var]] &lt;- anova_result\n    print(anova_result)\n  }\n  \n  return(results)\n}\n\n# List of continuous variables\ncontinuous_vars &lt;- c(\"BMI\", \"Cholesterol_Level\", \"Stress_Levels\", \"Heart_Rate\", \"Systolic_BP\", \"Diastolic_BP\")\n\n# Perform ANOVA for each continuous variable in the Youth group\nanova_results &lt;- perform_anova(contin_col_HA_youth, \"Heart_Attack_Occurrence\", continuous_vars)\n\n[1] \"Performing ANOVA for BMI by Heart_Attack_Occurrence\"\n                          Df Sum Sq Mean Sq F value Pr(&gt;F)\nHeart_Attack_Occurrence    1     36   35.69   1.415  0.234\nResiduals               8764 221033   25.22               \n[1] \"Performing ANOVA for Cholesterol_Level by Heart_Attack_Occurrence\"\n                          Df  Sum Sq Mean Sq F value Pr(&gt;F)\nHeart_Attack_Occurrence    1     707   707.2   0.786  0.375\nResiduals               8764 7883042   899.5               \n[1] \"Performing ANOVA for Stress_Levels by Heart_Attack_Occurrence\"\n                          Df Sum Sq Mean Sq F value Pr(&gt;F)   \nHeart_Attack_Occurrence    1     27   26.77    6.71 0.0096 **\nResiduals               8764  34970    3.99                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n[1] \"Performing ANOVA for Heart_Rate by Heart_Attack_Occurrence\"\n                          Df Sum Sq Mean Sq F value Pr(&gt;F)\nHeart_Attack_Occurrence    1      0    0.24   0.002  0.961\nResiduals               8764 882302  100.67               \n[1] \"Performing ANOVA for Systolic_BP by Heart_Attack_Occurrence\"\n                          Df  Sum Sq Mean Sq F value Pr(&gt;F)\nHeart_Attack_Occurrence    1     202   202.0   0.896  0.344\nResiduals               8764 1977029   225.6               \n[1] \"Performing ANOVA for Diastolic_BP by Heart_Attack_Occurrence\"\n                          Df Sum Sq Mean Sq F value Pr(&gt;F)\nHeart_Attack_Occurrence    1      3    2.54   0.025  0.874\nResiduals               8764 886533  101.16               \n\n\n\n4.5.1 Oneway ANOVA Test: ggbetweenstats() method\n\nstress &lt;- ggbetweenstats(\n  data = HA_youth,\n  x = Heart_Attack_Occurrence, \n  y = Stress_Levels,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\nstress\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBased on the anova results, there is statistically significant differences in the stress levels between youth with heart attack verse youth whom have not had heart attack.\nHowever, it is noted that there small effect size (Hedges’ g = 0.09) suggests that although the difference is statistically significant, but the small effect may not may not be large enough to have meaningful implications in a real-world context.\nAs the stress level maybe derived from Likert kind of survey, it maybe meaningful to categorise it to the different levels and analyse using logistic regression model. This had been done in section 2.6"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#final-charts-and-conclusion",
    "href": "Take-home_Exe/Take-home_Ex_01.html#final-charts-and-conclusion",
    "title": "Take-Home Exercise 01",
    "section": "6.0 Final charts and conclusion",
    "text": "6.0 Final charts and conclusion\n\nWith acknowledge of the how the variables related to the occurrence of heart attack, in this section, we will do up the charts to visually show the relationship between the variables and heart attack in youth.\nSince the earlier charts has already compared the main group of of interest youth with heart attack, with adults with heart attack and youth without heart attack. This section is dedicated to show visualise the relationship of the variables within the group\n\n\nP1 &lt;-ggplot(data=HA_youth_yes, \n            aes(x= BMI, \n                y= Stress_Levels, \n                color=Diabetes_History)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(5,40),\n                  ylim=c(0,11))\n\nP2&lt;- ggMarginal(P1, type = \"boxplot\",\n           groupColour = TRUE, \n           groupFill = TRUE)+ theme_minimal()\n\n\nP3 &lt;-ggplot(data=HA_youth_yes,\n       aes(x = Diabetes_History , y = Stress_Levels, color = Diet_Quality )) +\n    geom_boxplot()+\n  facet_wrap(~ BMI_cat)+ theme_minimal()\n\n\nP4&lt;-ggplot(HA_youth_yes, \n       aes(x = Diabetes_History, y = Stress_Levels)) +\n  stat_halfeye(adjust = 0.5,\n               justification = 0.1,\n               .width = 0,\n               point_colour = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.1, \n            binwidth = .5,\n            dotsize = 0.05)\n  #coord_flip() +\n  theme_economist()\n\nList of 39\n $ line                :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                :List of 5\n  ..$ fill         : Named chr NA\n  .. ..- attr(*, \"names\")= chr NA\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"black\"\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text           :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 10points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 10points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 10points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks          :List of 6\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ axis.ticks.y        : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.length   : 'simpleUnit' num -5points\n  ..- attr(*, \"unit\")= int 8\n $ axis.line           :List of 6\n  ..$ colour       : NULL\n  ..$ linewidth    : 'rel' num 0.8\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ axis.line.y         : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.background   :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : num 0\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ legend.spacing      : 'simpleUnit' num 15points\n  ..- attr(*, \"unit\")= int 8\n $ legend.key          :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : num 0\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ legend.key.size     : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height   : NULL\n $ legend.key.width    : NULL\n $ legend.text         :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.25\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.position     : chr \"top\"\n $ legend.direction    : NULL\n $ legend.justification: chr \"center\"\n $ panel.background    :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : NULL\n  ..$ linewidth    : NULL\n  ..$ linetype     : num 0\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ panel.border        : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ panel.spacing       : 'simpleUnit' num 0.25lines\n  ..- attr(*, \"unit\")= int 3\n $ panel.grid.major    :List of 6\n  ..$ colour       : chr \"white\"\n  ..$ linewidth    : 'rel' num 1.75\n  ..$ linetype     : NULL\n  ..$ lineend      : NULL\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ panel.grid.minor    : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ plot.background     :List of 5\n  ..$ fill         : Named chr \"#d5e4eb\"\n  .. ..- attr(*, \"names\")= chr \"blue-gray\"\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ plot.title          :List of 11\n  ..$ family       : NULL\n  ..$ face         : chr \"bold\"\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.5\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ plot.margin         : 'simpleUnit' num [1:4] 12points 10points 12points 10points\n  ..- attr(*, \"unit\")= int 8\n $ strip.background    :List of 5\n  ..$ fill         : Named chr NA\n  .. ..- attr(*, \"names\")= chr NA\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : num 0\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ strip.text          :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 1.25\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.x        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ strip.text.y        :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ panel.grid.major.x  : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n#| fig-width: 24 #to widen the space\n#| fig-height: 24 #to lengthen the graph\n\nPatchwork &lt;- P4 |P3\n\nPatchwork\n\n\n\n\n\n\n\n\n\nAfter establishing the following:\n-Stress Level and Diabetes History have statistically significant related to incidents of heart attack in youth using the logistic regression model. Although diet history may not have statistically significant, but is worthwhile to explore its relationship with other variables.\n-there is statistically significant differences in true group mean in stress level between youth whom had heart attack and without heart attack.\n-the various BMI categories seem to have statically significant associations in relation of heart attack, proven using chi square.\nThe 4 variables of the youth whom had heart attack plotted using the half eye chart, box plot across the BMI categories to further analyse the relationships among the mentioned variables.\nObservation\nAs mentioned in section 5, that the logistic regression, that the stress level is by Likert scale as the data is discrete instead of continuous as shown in the dot stats chart. Based on the chart, we can see the youth with higher stress level had diabetes before as compared to those who has not have diabetes before.\nThis trend could be further showed by the box plot on the right. The boxplot is facet across the various BMI categories.\nAt a quick glance, we could see obese youth whom had heart attack, has highest stress level. This particular group of youth also had the poor diet quality and diabetes before.\nIt was also noted there are overall higher stress level in overweight and obese youths as compared to those with normal BMI or underweight.\nAmong the youths, it was also noted. that two groups with the highest stress level are the obese and overweight youths with poor diet quality.\nFurther analysis could be carried out to further explore characteristic of Japanese Youth whom had heart attack. It is important to note that more information on the data set would be require before further conclusion could be drawn\nConclusion\nLearning points from this take home exercise is that data visualising is not direct. It requires student or analyst to have a throughout understanding of the data set before the analyst could make use of the tools to present the data in a accurate and simplify manner which allow viewers to be able to see the trend of the data set.\n\n\n7.0 Other Charts and visualisation tools used.\nAs part of the learning journey, there are may other charts tried to visualise the data. However due to the assignment limit, they are not added to the main part of the assessment.\n\nHistogram to compare subjects with heart attack in youth and adults\n\nhistograms &lt;- ggplot(contin_col_HA_Long_Yes, aes(x = Value, fill = Age_Grp )) + \n  geom_histogram(binwidth = 1, alpha = 0.8) + \n  theme_minimal() + \n  labs(title = \"Histograms of Continuous Fields\", y = \"Frequency\")+\n  facet_wrap(~ Field, scales = \"free\") \n\n# Print the histograms\nprint(histograms)\n\n\n\n\n\n\n\n\n\n\nvisualizing using density plot to further understand stress level\n\n# Adding density plot\nggplot(HA_youth, aes(x = Stress_Levels)) +\n  geom_histogram(aes(y = ..density..), binwidth = 1, fill = \"blue\", alpha = 0.5) +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram and Density Plot of Stress Levels\", x = \"Stress Levels\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\nCorrelation plots\n\nHA.cor &lt;- cor(HA_youth_yes[, c(7, 11:15)], use = \"complete.obs\")\n\n\ncorrplot(HA.cor)\n\n\n\n\n\n\n\n\n\nggplot(HA, aes(x = Stress_Levels, fill = Heart_Attack_Occurrence)) +  # Map fill to Heart_Attack_Occurrence\n  geom_histogram(aes(y = ..density..), binwidth = 1, alpha = 0.5, position = \"identity\") +  # Adjust position for overlap\n  geom_density(color = \"red\", size = 1, alpha = 0.3) +  # Density plot with transparency\n  labs(title = \"Histogram and Density Plot of Stress Levels\",\n       x = \"Stress Levels\",\n       y = \"Density\") +\n  theme_minimal(base_size = 14) +  # Improve overall theme and font size\n  scale_fill_brewer(palette = \"Set2\", name = \"Heart Attack Occurrence\") +  # Use a color palette for filling\n  theme(legend.position = \"top\")  # Position the legend at the top\n\n\n\n\n\n\n\n\n\nscatter_plot &lt;- ggplot(HA, aes(x = Stress_Levels, y = BMI, color = Heart_Attack_Occurrence)) +\n  geom_point(size = 1, alpha = 0.4) +  # Adjust point size and transparency\n  labs(title = \"Scatter Plot of Stress Levels vs. BMI\",\n       x = \"Stress Levels\",\n       y = \"BMI\",\n       color = \"Heart Attack Occurrence\") +  # Add title and labels\n  theme_minimal(base_size = 14) +  # Use a clean theme with larger base font size\n  scale_color_manual(values = c(\"blue\", \"red\"))  # Customize colors if needed\n\n# Print the scatter plot\nprint(scatter_plot)\n\n\n\n\n\n\n\n\n\nggplot(HA, aes(x = Smoking_History, fill = Heart_Attack_Occurrence)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Smoking History vs Heart Attack Occurrence\", \n       y = \"Proportion\", \n       x = \"Smoking History\") +\n  scale_fill_manual(values = c(\"lightblue\", \"salmon\"), name = \"Heart Attack Occurrence\")\n\n\n\n\n\n\n\n\n\n\nDisclaimer\nThis take home exercise was done with reference to methods in Prof Kam’s e text book https://r4va.netlify.app/ , materials on R available online and the use of Microsoft co-pilot to troubleshoot syntax errors and streamline codes."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_01.html#model-building",
    "href": "Take-home_Exe/Take-home_Ex_01.html#model-building",
    "title": "Take-Home Exercise 01",
    "section": "5.0 Model building",
    "text": "5.0 Model building\nDue to the lack of observable trends from CDA and EDA, explanatory model is build to further analysis if there are other variables, besides Stress level, BMI categories and Diabetes history, a logistic model is built to further analyse the data set for trends.\n\n5.1 Logistic Regression Model\n\n# Fit the model\nmodel_log_youth &lt;- glm(Heart_Attack_Occurrence_num  ~ Gender_num + Region_num + Smoking_History_num + Diabetes_History_num + Physical_Activity_num + Diet_Quality_num + Alcohol_Consumption_num + Stress_Level_cat_num + BMI_cat_num + Heart_Rate_cat_num + BP_cat_num +Family_History_num + Cholesterol_Level_cat_num, family = binomial(), data = HA_youth)\n\n# View the summary\nsummary(model_log_youth)\n\n\nCall:\nglm(formula = Heart_Attack_Occurrence_num ~ Gender_num + Region_num + \n    Smoking_History_num + Diabetes_History_num + Physical_Activity_num + \n    Diet_Quality_num + Alcohol_Consumption_num + Stress_Level_cat_num + \n    BMI_cat_num + Heart_Rate_cat_num + BP_cat_num + Family_History_num + \n    Cholesterol_Level_cat_num, family = binomial(), data = HA_youth)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -1.93838    0.17306 -11.201  &lt; 2e-16 ***\nGender_num                 0.02215    0.07293   0.304  0.76134    \nRegion_num                -0.04617    0.07862  -0.587  0.55700    \nSmoking_History_num        0.03464    0.07954   0.436  0.66320    \nDiabetes_History_num       0.23270    0.08552   2.721  0.00651 ** \nPhysical_Activity_num     -0.01854    0.04703  -0.394  0.69335    \nDiet_Quality_num          -0.08240    0.04909  -1.678  0.09327 .  \nAlcohol_Consumption_num   -0.05436    0.04016  -1.354  0.17585    \nStress_Level_cat_num      -0.11934    0.05435  -2.196  0.02810 *  \nBMI_cat_num               -0.04134    0.05512  -0.750  0.45321    \nHeart_Rate_cat_num         0.05784    0.07258   0.797  0.42557    \nBP_cat_num                 0.02014    0.03454   0.583  0.55997    \nFamily_History_num        -0.03353    0.08030  -0.418  0.67629    \nCholesterol_Level_cat_num -0.07304    0.05659  -1.291  0.19681    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5509.8  on 8765  degrees of freedom\nResidual deviance: 5488.9  on 8752  degrees of freedom\nAIC: 5516.9\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nexp(coef(model_log_youth))\n\n              (Intercept)                Gender_num                Region_num \n                0.1439365                 1.0223990                 0.9548764 \n      Smoking_History_num      Diabetes_History_num     Physical_Activity_num \n                1.0352488                 1.2620005                 0.9816281 \n         Diet_Quality_num   Alcohol_Consumption_num      Stress_Level_cat_num \n                0.9209048                 0.9470903                 0.8875062 \n              BMI_cat_num        Heart_Rate_cat_num                BP_cat_num \n                0.9595019                 1.0595403                 1.0203396 \n       Family_History_num Cholesterol_Level_cat_num \n                0.9670273                 0.9295625 \n\n\n\n# Check collinearity\ncheck_c &lt;- check_collinearity(model_log_youth)\n\n# Print the collinearity check results\nprint(check_c)\n\n# Plot collinearity diagnostics\nplot(check_c)\n\n\nconfint.default(model_log_youth)\n\n                                2.5 %      97.5 %\n(Intercept)               -2.27756591 -1.59920051\nGender_num                -0.12079445  0.16509808\nRegion_num                -0.20026248  0.10791582\nSmoking_History_num       -0.12126241  0.19054601\nDiabetes_History_num       0.06507627  0.40032007\nPhysical_Activity_num     -0.11071131  0.07362587\nDiet_Quality_num          -0.17862059  0.01382343\nAlcohol_Consumption_num   -0.13307122  0.02434948\nStress_Level_cat_num      -0.22586001 -0.01281956\nBMI_cat_num               -0.14936657  0.06668465\nHeart_Rate_cat_num        -0.08442686  0.20009715\nBP_cat_num                -0.04757126  0.08784222\nFamily_History_num        -0.19091804  0.12386083\nCholesterol_Level_cat_num -0.18395596  0.03787357\n\n\n\nlibrary(caret)\npredictions &lt;- predict(model_log_youth, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconfusionMatrix(as.factor(predicted_classes), as.factor(HA_youth$Heart_Attack_Occurrence_num))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 7932  834\n         1    0    0\n                                          \n               Accuracy : 0.9049          \n                 95% CI : (0.8985, 0.9109)\n    No Information Rate : 0.9049          \n    P-Value [Acc &gt; NIR] : 0.5092          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.9049          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.9049          \n         Detection Rate : 0.9049          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n5.2 Variables important\n\n# Extract coefficients from the model\ncoef_data &lt;- as.data.frame(coef(summary(model_log_youth)))\ncoef_data$Feature &lt;- rownames(coef_data)\nrownames(coef_data) &lt;- NULL\n\n# Plot the coefficients\nlibrary(ggplot2)\nggplot(coef_data, aes(x = reorder(Feature, Estimate), y = Estimate)) +\n  geom_bar(stat = \"identity\", fill = \"#FF1493\") +\n  coord_flip() +\n  labs(title = \"Feature Importance (Logistic Regression Coefficients)\",\n       x = \"Feature\",\n       y = \"Coefficient Estimate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe model results show that stress level and diabetes history are statically significant in leading to higher risk of heart attack in youth.\nBesides two two variables, it seems that Diet quality may also play a role in leading heart attack in Japanese youth, although the significant level at 0.1 which is more than the standard 0.05.\n\n\n5.2.1 other codes in looking into model parameters (should not be include in the assignment counts)\n\nlibrary(caret)\npredictions &lt;- predict(model_log_youth, type = \"response\")\npredicted_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)\nconfusionMatrix(as.factor(predicted_classes), as.factor(HA_youth$Heart_Attack_Occurrence_num))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 7932  834\n         1    0    0\n                                          \n               Accuracy : 0.9049          \n                 95% CI : (0.8985, 0.9109)\n    No Information Rate : 0.9049          \n    P-Value [Acc &gt; NIR] : 0.5092          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.9049          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.9049          \n         Detection Rate : 0.9049          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nset.seed(123)\ntrain_indices &lt;- sample(1:nrow(HA_youth), 0.7 * nrow(HA_youth))\ntrain_data &lt;- HA_youth[train_indices, ]\ntest_data &lt;- HA_youth[-train_indices, ]\n\n# Fit the model on the training data\nmodel_train &lt;- glm(Heart_Attack_Occurrence_num ~ Gender_num + Region_num + Smoking_History_num + Diabetes_History_num + Physical_Activity_num + Diet_Quality_num + Alcohol_Consumption_num + Stress_Level_cat_num + BMI_cat_num + Heart_Rate_cat_num + BP_cat_num + Family_History_num + Cholesterol_Level_cat_num, family = binomial(), data = train_data)\n\n# Predict on the test data\ntest_predictions &lt;- predict(model_train, newdata = test_data, type = \"response\")\ntest_predicted_classes &lt;- ifelse(test_predictions &gt; 0.5, 1, 0)\n\n# Evaluate the model on the test data\nconfusionMatrix(as.factor(test_predicted_classes), as.factor(test_data$Heart_Attack_Occurrence_num))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2373  257\n         1    0    0\n                                          \n               Accuracy : 0.9023          \n                 95% CI : (0.8903, 0.9134)\n    No Information Rate : 0.9023          \n    P-Value [Acc &gt; NIR] : 0.5166          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.9023          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.9023          \n         Detection Rate : 0.9023          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nggcoefstats(model_log_youth, \n            output = \"plot\")\n\n\nAIC(model_log_youth )\n\n[1] 5516.885"
  },
  {
    "objectID": "Project/Project_Data_Prep.html",
    "href": "Project/Project_Data_Prep.html",
    "title": "Project data preparation",
    "section": "",
    "text": "pacman::p_load(tidyverse)\n\n\nweather&lt;- read_csv(\"data/combined_weather_data.csv\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_07.html",
    "href": "Hands-on_Exe/Hands-on_Ex_07.html",
    "title": "Hands-on Ex 07",
    "section": "",
    "text": "Visualising and Analysing Time-oriented Data\n\npacman::p_load(rootSolve, lmom, expm, Exact, gld, productplots, libcoin, inum, \n                 DescTools, ggmosaic, partykit, CGPfunctions, knitr, tidyverse,\n               data.table, dplyr,ggthemes, readxl )\n\n\n\nPlotting Calender Heatmap\n\nattacks &lt;- read.csv(\"data/eventlog.csv\")\n\n\n\nExamining the data structure\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12T15:59:16.718901Z\nCN\nAsia/Shanghai\n\n\n2015-03-12T16:00:48.841746Z\nFR\nEurope/Paris\n\n\n2015-03-12T16:02:26.731256Z\nCN\nAsia/Shanghai\n\n\n2015-03-12T16:02:38.469907Z\nUS\nAmerica/Chicago\n\n\n2015-03-12T16:03:22.201903Z\nCN\nAsia/Shanghai\n\n\n2015-03-12T16:03:45.984616Z\nCN\nAsia/Shanghai\n\n\n\n\n\n\n\nData Preparation\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nBuilding Multiple Calendar Heatmaps\n\nUsing different from code from prof kam\n\ntop4 &lt;- attacks %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  group_by(source_country) %&gt;%\n  summarise(total_attacks = sum(n)) %&gt;%\n  top_n(4, total_attacks) %&gt;%\n  arrange(desc(total_attacks))\n\ngrouped_top4 &lt;- attacks %&gt;%\n  count(source_country, wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit() %&gt;%\n  semi_join(top4, by = \"source_country\")\n\n\nggplot(grouped_top4, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nfacet_wrap(~source_country)+\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      axis.text.x = element_text(size = 7),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) ) \n\n\n\n\n\n\n\n\n\n\n\nPlotting Cycle Plot\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_economist_white()\n\n\n\n\n\n\n\n\n\n\nPlotting Slopegraph\nStep 1: Data Import\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Referred to codes by: Dr. Kam Tin Seong\")"
  },
  {
    "objectID": "In-class_Exe/In-class_Ex07.html",
    "href": "In-class_Exe/In-class_Ex07.html",
    "title": "In-Class Ex 07",
    "section": "",
    "text": "pacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\n\nts_data &lt;- read_csv(\n  \"data/visitor_arrivals_by_air.csv\")\n\n\nts_data$`Month-Year` &lt;- dmy(\n  ts_data$`Month-Year`)\n\n\nts_dataa_ts &lt;- ts(ts_data) #indicate as time series data frame, is not tsclassibble dataframe\nhead (ts_data)\n\n# A tibble: 6 × 34\n  `Month-Year` `Republic of South Africa` Canada   USA Bangladesh Brunei China\n  &lt;date&gt;                            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 2008-01-01                         3680   6972 31155       6786   3729 79599\n2 2008-02-01                         1662   6056 27738       6314   3070 82074\n3 2008-03-01                         3394   6220 31349       7502   4805 72546\n4 2008-04-01                         3337   4764 26376       7333   3096 76112\n5 2008-05-01                         2089   4460 26788       7988   3586 64808\n6 2008-06-01                         2515   3888 29725       8301   5284 55238\n# ℹ 27 more variables: `Hong Kong SAR (China)` &lt;dbl&gt;, India &lt;dbl&gt;,\n#   Indonesia &lt;dbl&gt;, Japan &lt;dbl&gt;, `South Korea` &lt;dbl&gt;, Kuwait &lt;dbl&gt;,\n#   Malaysia &lt;dbl&gt;, Myanmar &lt;dbl&gt;, Pakistan &lt;dbl&gt;, Philippines &lt;dbl&gt;,\n#   `Saudi Arabia` &lt;dbl&gt;, `Sri Lanka` &lt;dbl&gt;, Taiwan &lt;dbl&gt;, Thailand &lt;dbl&gt;,\n#   `United Arab Emirates` &lt;dbl&gt;, Vietnam &lt;dbl&gt;, `Belgium & Luxembourg` &lt;dbl&gt;,\n#   Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Italy &lt;dbl&gt;, Netherlands &lt;dbl&gt;,\n#   Spain &lt;dbl&gt;, Switzerland &lt;dbl&gt;, `United Kingdom` &lt;dbl&gt;, Australia &lt;dbl&gt;, …\n\n\n\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = `Month`)\n\n\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\n\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;% #this could be one of the parameters. \n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")\n\n\ntsibble_longer %&gt;%\n  filter(Country == \"Italy\" |\n         Country == \"Vietnam\" |\n         Country == \"United Kingdom\" |\n         Country == \"Germany\") %&gt;% \n  gg_season(Arrivals)\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals) #the subseries function is a cycle plot\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  ACF(Arrivals) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\" |\n         `Country` == \"Italy\" |\n         `Country` == \"United Kingdom\" |\n         `Country` == \"China\") %&gt;%\n  PACF(Arrivals) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_02.html",
    "href": "Take-home_Exe/Take-home_Ex_02.html",
    "title": "Take Home Exercise 02",
    "section": "",
    "text": "This page document the data visualisation and analysis carried out for the Take-Home Exercise 02."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_02.html#scope",
    "href": "Take-home_Exe/Take-home_Ex_02.html#scope",
    "title": "Take Home Exercise 02",
    "section": "1.0 Scope:",
    "text": "1.0 Scope:\nThe tasks for this take home exercise are to:\n\ncomment on the pros and cons of three data visualisation on this page and provide sketches of the make-over.\nuse appropriate ggplot2 and others packages create the make-over of the three data visualisations commented above.\nanalyse the data with either time-series analysis or time-series forecasting methods by compliment the analysis with appropriate data visualisation methods and R packages."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_02.html#libraries",
    "href": "Take-home_Exe/Take-home_Ex_02.html#libraries",
    "title": "Take Home Exercise 02",
    "section": "2.0 Libraries:",
    "text": "2.0 Libraries:\n\nThe following libraries are used to complete the following exercise:\n\n\npacman::p_load(tidyverse, readxl,  tsibble, feasts, fable, seasonal, ggplot2, scales, ggiraph, reshape2, stringr,treemap, treemapify, gifski, gapminder,plotly, gganimate)"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_02.html#data-sets",
    "href": "Take-home_Exe/Take-home_Ex_02.html#data-sets",
    "title": "Take Home Exercise 02",
    "section": "3.0 Data Sets:",
    "text": "3.0 Data Sets:\nThe data sets used for this exercise are the “Merchandise Trade by Region/ Market” and “Merchandise Trade By Commodity Section/ Division” from Singstats which could be download from this page.\nIn Section 3, it details basic data preparation steps for the data sets: Merchandise Trade by Region/ Market, and the more specific steps in data process for specific types of data visualization are documented in the respective section.\n\n3.1 Importing Data sets for Merchandise Trade by Region/ Market\nThe data sets from “Merchandise Trade by Region/ Market” are imported using the function read_xlsx () from readxl package.\n\nimport_r &lt;-read_xlsx(\"data/Ex02/MerchandiseTradebyRegion.xlsx\", \"T1\")\n\nexport_r &lt;- read_xlsx(\"data/Ex02/MerchandiseTradebyRegion.xlsx\", \"T2\")\n\nre_export_r &lt;- read_xlsx(\"data/Ex02/MerchandiseTradebyRegion.xlsx\", \"T3\")\n\n\n\n3.2 Data Preparation Merchandise Trade by Region/ Market:\nThe following steps were done to transform the data sets for make-over and time series analysis.\nThe initial attempt was to streamline the data preparation so the transformed data set could be used for both the make-over visualisation and time series analysis. However, in the course of the doing the assignment, it was realised that it was not possible, hence the data preparation are split up for the each make over and time series analysis.\n\n\n3.2.1 Data Preparation for Make Over 1.\nThis section documents down the details of data preparation for make over 1. The steps may not be streamline and there may be seems to have redundant steps, this is part which should be removed. I have kept the codes, for future reference on how the the data could be transformed.\n\n\n3.2.1.1 Data Preparation for import data by market and region\nThe following section is on the data preparation for import data set from Merchandise Trade by Region/ Market.\n\nhead(import_r)\n\n# A tibble: 6 × 266\n  `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n  &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Total All Markets       54746.    56136.      51802.    51416.     49068. \n2 America                  6923.     7874.       7880.     8078.      9112  \n3 Antigua And Barbuda         0         0           0         0          0  \n4 Argentina                   4        12.5       116.        4.1        8.1\n5 Bahamas                     0         8.1         0         0          0  \n6 Bermuda                     0         0           0         0          0  \n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;,\n#   `2022 Sep` &lt;dbl&gt;, `2022 Aug` &lt;dbl&gt;, `2022 Jul` &lt;dbl&gt;, `2022 Jun` &lt;dbl&gt;, …\n\n\n\n# Ensure 'Data Series' is character type, to make sure that the 'Data Series'could be come row names. \nimport_r$`Data Series` &lt;- as.character(import_r$`Data Series`)\n\n# Creating a new data frame with 'Data Series' as row names\nimport &lt;- import_r %&gt;% column_to_rownames(var = \"Data Series\")\n\ntransposed_import &lt;- as.data.frame(t(import))\n\n\n\n3.2.1.1.1 Transposed_import data set\nThe transposed data has Year_month as rowname the steps below is the add column so that the usual row name will be in running numbers\nAfter preparing the data set, this will be used to carry out time series analysis in section 5.1\n\n# Add the row names as a new column so that a running numbers could be replaced. \ntransposed_import &lt;- transposed_import %&gt;% rownames_to_column(var = \"Year_Month\")\n\n# Set sequential row names\nrownames(transposed_import) &lt;- seq_len(nrow(transposed_import ))\n\n\n# Convert the 'Year_Month' column to date-time format\ntransposed_import$`Year_Month` &lt;- parse_date_time(transposed_import$`Year_Month`, orders = \"ym\")\n\ntransposed_import$`Year_Month` &lt;- format(transposed_import$`Year_Month`, \"%b %Y\")\n\ntransposed_import &lt;- transposed_import %&gt;%\n  rename(Month_Year = Year_Month, Total_imports = `Total All Markets`)\n\nThe following steps is to summarize the data to yearly.\n\nyearly_import&lt;- transposed_import\n\nyearly_import$Month_Year &lt;- parse_date_time(yearly_import$Month_Year, orders = \"my\")\n\n# Convert 'Month_Year' to a date format and filter incomplete data, 2003 and 2025 does not have 12 months of data, hence they will not be used.\nyearly_import &lt;- yearly_import %&gt;%\n  filter(`Month_Year` &lt;= ymd(\"2024-12-01\") & `Month_Year` &gt;= ymd(\"2004-01-01\"))\n\n# Extract the year and add it as a new column\nyearly_import$Year &lt;- year(yearly_import$`Month_Year`)\n\n# Summarize the values by year\nyearly_import &lt;- yearly_import %&gt;%\n  group_by(Year) %&gt;%\n  summarise(across(-Month_Year, sum, na.rm = TRUE))\n\nhead(yearly_import)\n\n# A tibble: 6 × 161\n   Year Total_imports America `Antigua And Barbuda` Argentina Bahamas Bermuda\n  &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;                 &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1  2004       294178.  38847                    0        32.3    45.1     0  \n2  2005       334314.  43741.                  18.9      38.6    41.2     0  \n3  2006       380842.  53460                    0.1      69.2    93       0  \n4  2007       398404   55411                    0.4      65.4   584.      0.4\n5  2008       455460.  63848.                   0       105     764.      0  \n6  2009       358374.  52205.                   8.1      79.8   635.      0  \n# ℹ 154 more variables: Brazil &lt;dbl&gt;, Canada &lt;dbl&gt;, Chile &lt;dbl&gt;,\n#   Colombia &lt;dbl&gt;, `Costa Rica` &lt;dbl&gt;, Cuba &lt;dbl&gt;, `Dominican Rep` &lt;dbl&gt;,\n#   Ecuador &lt;dbl&gt;, `El Salvador` &lt;dbl&gt;, Guatemala &lt;dbl&gt;, Guyana &lt;dbl&gt;,\n#   Honduras &lt;dbl&gt;, Jamaica &lt;dbl&gt;, Mexico &lt;dbl&gt;, `Netherlands Antilles` &lt;dbl&gt;,\n#   Panama &lt;dbl&gt;, Paraguay &lt;dbl&gt;, Peru &lt;dbl&gt;, `Puerto Rico` &lt;dbl&gt;,\n#   `St. Vincent And The Grenadines` &lt;dbl&gt;, `Trinidad And Tobago` &lt;dbl&gt;,\n#   `United States` &lt;dbl&gt;, `United States Virgin Islands` &lt;dbl&gt;, …\n\n\n\n\n3.2.1.2 Data Preparation for Export Data by market and region\nThe total export by regions and years include the both amount from the export data from worksheet “T2”- Export and T3”- re-export.\nThe following code check is to select the numeric figures and sum them accordingly if they are of the same dimension. This is assuming that the arrangement of the values are the same.\n\nif (all(dim(export_r) == dim(re_export_r))) {\n  summed_df &lt;- export_r %&gt;%\n    mutate(across(where(is.numeric), ~ . + re_export_r[[cur_column()]]))\n  non_numeric_cols &lt;- export_r %&gt;%\n    select(where(~!is.numeric(.)))\n  \n combined_export &lt;- bind_cols(non_numeric_cols, summed_df %&gt;% select(where(is.numeric)))\n  print(combined_export)\n} else {\n  cat(\"The data frames do not have the same dimensions and cannot be summed element-wise.\\n\")\n}\n\n# A tibble: 160 × 266\n   `Data Series`       `2025 Jan` `2024 Dec` `2024 Nov` `2024 Oct` `2024 Sep`\n   &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Total All Markets      59408.     60143.     58330.     56110.     54444. \n 2 America                 7373.      7501.      6657.      6324.      6491. \n 3 Antigua And Barbuda       10.7        7.2        8.3        7.9        8.2\n 4 Argentina                 31         28.3       23.3       30.2       30.1\n 5 Bahamas                   61.2       49.4       64.2       40.4       67.4\n 6 Bermuda                    2.8        7.7        0.5        5.5        0.7\n 7 Brazil                   244.       207.       187.       217.       233. \n 8 Canada                   113.       103.       155.        97.2      141. \n 9 Chile                     14         13.7       10.7       13.2       29.8\n10 Colombia                  10.7       16.4       12.5       14.1       13.6\n# ℹ 150 more rows\n# ℹ 260 more variables: `2024 Aug` &lt;dbl&gt;, `2024 Jul` &lt;dbl&gt;, `2024 Jun` &lt;dbl&gt;,\n#   `2024 May` &lt;dbl&gt;, `2024 Apr` &lt;dbl&gt;, `2024 Mar` &lt;dbl&gt;, `2024 Feb` &lt;dbl&gt;,\n#   `2024 Jan` &lt;dbl&gt;, `2023 Dec` &lt;dbl&gt;, `2023 Nov` &lt;dbl&gt;, `2023 Oct` &lt;dbl&gt;,\n#   `2023 Sep` &lt;dbl&gt;, `2023 Aug` &lt;dbl&gt;, `2023 Jul` &lt;dbl&gt;, `2023 Jun` &lt;dbl&gt;,\n#   `2023 May` &lt;dbl&gt;, `2023 Apr` &lt;dbl&gt;, `2023 Mar` &lt;dbl&gt;, `2023 Feb` &lt;dbl&gt;,\n#   `2023 Jan` &lt;dbl&gt;, `2022 Dec` &lt;dbl&gt;, `2022 Nov` &lt;dbl&gt;, `2022 Oct` &lt;dbl&gt;, …\n\n\n\n\n# Ensure 'Data Series' is character type\ncombined_export$`Data Series` &lt;- as.character(export_r$`Data Series`)\n\n# Creating a new column ith 'Data Series' as row names\ncombined_export&lt;- combined_export %&gt;% column_to_rownames(var = \"Data Series\")\n\n# Transpose the data frame. \ntransposed_export &lt;- as.data.frame(t(combined_export))\n\n# Add the row names as a new column\ntransposed_export &lt;- transposed_export  %&gt;% rownames_to_column(var = \"Year_Month\")\n\n# Set sequential row names in running numbers\nrownames(transposed_export ) &lt;- seq_len(nrow(transposed_export))\n\n# Convert the 'Year_Month' column to date-time format\ntransposed_export$`Year_Month` &lt;- parse_date_time(transposed_export$`Year_Month`, orders = \"ym\")\n\n# set the format to be the month year\ntransposed_export$`Year_Month` &lt;- format(transposed_export$`Year_Month`, \"%b %Y\") \n\n# rename. \ntransposed_export &lt;- transposed_export %&gt;%\n  rename(Month_Year = Year_Month, Total_exports = `Total All Markets`)\n\nThe following code chunk is to rename and prepare for the yearly export data.\n\nyearly_export&lt;- transposed_export\n\nyearly_export$Month_Year &lt;- parse_date_time(yearly_export$Month_Year, orders = \"my\")\n\n# Convert 'Month_Year' to a date format and filter incomplete data, 2003 and 2025 does not have 12 months of data, hence they will not be used.\nyearly_export &lt;- yearly_export %&gt;%\n  filter(`Month_Year` &lt;= ymd(\"2024-12-01\") & `Month_Year` &gt;= ymd(\"2004-01-01\"))\n\n# Extract the year and add it as a new column\nyearly_export$Year &lt;- year(yearly_export$`Month_Year`)\n\n# Summarize the values by year and remove the month_year column. \nyearly_export &lt;- yearly_export %&gt;%\n  group_by(Year) %&gt;%\n  summarise(across(-Month_Year, sum, na.rm = TRUE))\n\n\n\n3.2.1.3 Preparation for Total Merchandise Trade Data Frame\n\n# Extract the 'Total_imports' column\ntotal_imports_summary &lt;- yearly_import %&gt;% select(Year, Total_imports)\n\n\n# combining total import and export column, they are rounded and convert to billions. \ncombined_summary &lt;- yearly_export %&gt;% \n  select(Year, Total_exports) %&gt;% \n  inner_join(total_imports_summary, by = \"Year\") %&gt;%\n  mutate(\n    Total_imports_B = round(Total_imports / 1000, 2),\n    Total_exports_B = round(Total_exports / 1000, 2)\n  ) %&gt;%\n  select(Year, Total_imports_B, Total_exports_B)\n\nThe following code is to calculate year on year percent change.\n\n# Add a 'Total Trade' column to the combined summary\ncombined_summary &lt;- combined_summary %&gt;%\n  mutate(Total_Trade = Total_imports_B + Total_exports_B)\n\n\n#Calculate the YoY percentage change for 'Total_imports', 'Total_exports', and 'Total Trade'\ncombined_summary &lt;- combined_summary %&gt;%\n  arrange(Year) %&gt;%\n  mutate(\n    YoY_imports_change = round((Total_imports_B / lag(Total_imports_B) - 1) * 100, 2),\n    YoY_exports_change = round((Total_exports_B / lag(Total_exports_B) - 1) * 100, 2),\n    YoY_trade_change = round((Total_Trade / lag(Total_Trade) - 1) * 100, 2)\n  )\n\n\n\n3.3.Data preparation for the Merchandise Trade by commodity\nThe following codes are to prepare for the data set for plotting the tree map for the trade by commodity.\n\ncommodity &lt;-read_xlsx(\"data/Ex02/MechandiseTradebycommodity.xlsx\", \"T1\")\n\nfiltered_commodities &lt;- commodity %&gt;%\n  filter(!(Data_Series %in% c(\"Oil\", \"Petroleum\", \"Oil Bunkers\")))\n\n\n# Add the Trade_Type column based on row numbers\nfiltered_commodities &lt;- filtered_commodities %&gt;%\n  mutate(Trade_Type = case_when(\n    row_number() %in% 1 :11 ~ \"Total_Trade\",  # Adjust the row numbers and trade types as needed\n    row_number() %in% 12:22 ~ \"Total_Import\",\n    row_number() %in% 23:33~ \"Total_Export\",\n    row_number() %in% 34:44~ \"Total_Domestic_Exports\",\n    row_number() %in% 45:55~ \"Total_Re_exports\",\n    TRUE ~ \"Other\"\n  )) %&gt;%\n  select(Trade_Type, everything())\n\n\nfiltered_commodities &lt;- filtered_commodities %&gt;%\n  select(-starts_with(\"19\"), -starts_with(\"2000\"), -starts_with(\"2001\"), -starts_with(\"2002\"), -starts_with(\"2003\"))\n\n\n# Pivot the data to a longer format\nfiltered_commodities_long &lt;- filtered_commodities %&gt;%\n  pivot_longer(\n    cols = starts_with(\"20\"),  # Select columns that start with \"202\"\n    names_to = \"Month_Year\",\n    values_to = \"Value\"\n  )\n\n\n# Convert the 'Year_Month' column to date-time format\nfiltered_commodities_long$Month_Year &lt;- parse_date_time(filtered_commodities_long$Month_Year, orders = \"ym\")\n\nfiltered_commodities_long$Month_Year &lt;- format(filtered_commodities_long$Month_Year, \"%b %Y\")\n\n\nfiltered_commodities_long$Month_Year &lt;- parse_date_time(filtered_commodities_long$Month_Year, orders = \"my\")\n\n# Extract the year and add it as a new column\nfiltered_commodities_long$Year &lt;- year(filtered_commodities_long$`Month_Year`)\n\n# Summarize the values by year\nfiltered_commodities_long&lt;- filtered_commodities_long%&gt;%\n  group_by(Year, Trade_Type, Data_Series) %&gt;%\n  summarise(across(-Month_Year, sum, na.rm = TRUE))\n\n\nhead(filtered_commodities_long)\n\n# A tibble: 6 × 4\n# Groups:   Year, Trade_Type [1]\n   Year Trade_Type             Data_Series                              Value\n  &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;                                    &lt;dbl&gt;\n1  2004 Total_Domestic_Exports Animal & Vegetable Oils Fats & Waxes   404240.\n2  2004 Total_Domestic_Exports Beverages & Tobacco                    338106.\n3  2004 Total_Domestic_Exports Chemicals & Chemical Products        30995464.\n4  2004 Total_Domestic_Exports Crude Materials (Excl Fuels)           963438.\n5  2004 Total_Domestic_Exports Food & Live Animals                   1972162 \n6  2004 Total_Domestic_Exports Machinery & Transport Equipment      88130650."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_02.html#make-over-1",
    "href": "Take-home_Exe/Take-home_Ex_02.html#make-over-1",
    "title": "Take Home Exercise 02",
    "section": "Make-over 1:",
    "text": "Make-over 1:\n1 ) while the chart is colorful and looks nice however it is difficult to see the rate of change in total trade.\na simpler graph could be used and with more least sophisticated graphic could be used. to deliver the key message that there were 6.6% increase in the total trade amount for year 2024.\n\n\ncombined_summary_melt &lt;- melt(combined_summary, id.vars = \"Year\", measure.vars = c(\"Total_imports\", \"Total_exports\"))\ncombined_summary_melt$tooltip &lt;- c(paste0(combined_summary_melt$Year,\" ,\" ,combined_summary_melt$value))\n\ncombined_summary$tooltip &lt;- c(paste0(combined_summary$Year,\" ,\" ,combined_summary_melt$'YoY Trade Change', \"%\"))\n\n\np &lt;- ggplot() +\n  geom_bar_interactive(data = combined_summary_melt, aes(x = Year, y = value, fill = variable, tooltip = tooltip), stat = \"identity\") +\n  geom_line_interactive(data = combined_summary, aes(x = Year, y = YoY_trade_change * 0.5 * max(combined_summary$Total_Trade) / 100, group = 1, color = \"YoY Trade Change\", tooltip = tooltip), size = 1) +\n  geom_point_interactive(data = combined_summary, aes(x = Year, y = YoY_trade_change * 0.5 * max(combined_summary$Total_Trade) / 100, color = \"YoY Trade Change\", tooltip = tooltip), size = 2) +\n  scale_fill_manual(values = c(\"Total_imports\" = \"skyblue\", \"Total_exports\" = \"orange\"), name = \"Trade Type\") +\n  scale_color_manual(values = c(\"YoY Trade Change\" = \"red\"), name = \"Percentage Change\") +\n  scale_y_continuous(\n    labels = comma,\n    name = \"Total Value\",\n    sec.axis = sec_axis(~ . * 200 / max(combined_summary$Total_Trade), name = \"YoY Percentage Change\")\n  ) +\n  labs(\n    title = \"Stacked Bar Chart of Import and Export with YoY Percentage Change\",\n    x = \"Year\",\n    fill = \"Trade Type\",\n    color = \"Percentage Change\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.y.right = element_text(vjust = -0.5),  # Adjust vertical position of right y-axis title\n    axis.title.y = element_text(vjust = 1.5)          # Adjust vertical position of left y-axis title\n  )\n\n# Render the interactive plot\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8 * 0.618\n)"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_02.html#make-over-of-visualization",
    "href": "Take-home_Exe/Take-home_Ex_02.html#make-over-of-visualization",
    "title": "Take Home Exercise 02",
    "section": "4.0 Make Over of Visualization",
    "text": "4.0 Make Over of Visualization\nThe first task of this assignment is to make over of the existing graphical representation.\n\n4.1 Make-over 1:\n\n\n\n\n\n\nNote\n\n\n\nPros:\n\nThe graph looks captivating and it draws the attention of the viewer\nIt clearly shows the import and export amount in the past 5 years.\n\nCons:\n\nIt is difficult to see the rate of change in total trade across the years with % increase. The author has indicated percent change at the end of the page.\nWhile the graph shows the the amount in trade across the years, it is difficult to visually see the trend and percentage changes across the years. For example, there has been increase in total trade amount from 2020 to 2022, while there was a dip in 2023. Despite of the dip in total trade amount in year 2023, it is still higher as compared to pre-Covid. It is important to note that the total trade amount quickly recover in 2024 leading to the 6.6 % indicated in the original graph.\nWhile colourful chart is attention grabbing, and however it makes it visually difficult to compare across the years and it may dilute the key message the graph to be delivered.\n\n\n\n\n\nCreation of tool tips\nThe following codes are to create the tool tips to plot the chart.\nAs mentioned in section 3 in the data preparation, the data processing will have to be made further prepare the data for the make over. the following code uses the melt () function to create the data required for plotting of the stack bar chart.\n\ncombined_summary_melt &lt;- melt(combined_summary, id.vars = \"Year\", measure.vars = c(\"Total_imports_B\", \"Total_exports_B\"))\ncombined_summary_melt$tooltip &lt;- c(paste0(combined_summary_melt$Year,\" ,\" ,combined_summary_melt$value, \"B\"))\ncombined_summary$tooltip &lt;- c(paste0(combined_summary$Year,\" ,\" ,combined_summary$YoY_trade_change, \"%\"))\n\n\np1 &lt;- ggplot() +\n  geom_bar_interactive(data = combined_summary_melt, aes(x = Year, y = value, fill = variable, tooltip = tooltip), stat = \"identity\") +\n  scale_fill_manual(values = c(\"Total_imports_B\" = \"skyblue\", \"Total_exports_B\" = \"orange\"), name = \"Trade Type\") +\n  labs(\n    title = \"Stacked Bar Chart of Import and Export from 2004 to 2024\",\n    x = \"Year\",\n    y = \"Total Value in Billions\",\n    fill = \"Trade Types\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.y = element_text(vjust = 1.5)\n  )\n\ngirafe(\n  ggobj = p1,\n  width_svg = 8,\n  height_svg = 8 * 0.618\n)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe interactive chart above shows the export and import amount from 2004 from 2024, With the tooltip showing the total trade number across the years.\nIn the chart above, one could easily see the past 10 years trend which the total trade amount has increased since 2020 post-Covid. The mouse over tool tips will highlight the amount in the export and import of the particular year.\nAlthough in 2023, there was a dip in total trade amount, it is still higher than pre-Covid.\nTo show the year on year percentage change, a 2nd interactive plot is plotted to visualize this. Similar to the first plot, the line plot uses interactive tool tips to let viewer to mouse over to see the data points.\nThe advantage of doing two separate plots is that different indicator of the percentage change could be visualized. In the interactive line chart, it has the percentage of in year on year percentage change for Import, Export and Total Trade amount.\n\n\n\n# Line chart for YoY percentage changes\np2&lt;- ggplot() +\n  geom_line_interactive(data = combined_summary, aes(x = Year, y = YoY_imports_change, group = 1, color = \"YoY Imports Change\", tooltip = tooltip), size = 1) +\n  geom_line_interactive(data = combined_summary, aes(x = Year, y = YoY_exports_change, group = 1, color = \"YoY Exports Change\", tooltip = tooltip), size = 1) +\n  geom_line_interactive(data = combined_summary, aes(x = Year, y = YoY_trade_change, group = 1, color = \"YoY Trade Change\", tooltip = tooltip), size = 1) +\n  geom_point_interactive(data = combined_summary, aes(x = Year, y = YoY_imports_change, color = \"YoY Imports Change\", tooltip = tooltip), size = 2) +\n  geom_point_interactive(data = combined_summary, aes(x = Year, y = YoY_exports_change, color = \"YoY Exports Change\", tooltip = tooltip), size = 2) +\n  geom_point_interactive(data = combined_summary, aes(x = Year, y = YoY_trade_change, color = \"YoY Trade Change\", tooltip = tooltip), size = 2) +\n  scale_color_manual(values = c(\"YoY Imports Change\" = \"blue\", \"YoY Exports Change\" = \"green\", \"YoY Trade Change\" = \"red\"), name = \"Percentage Change\") +\n  labs(\n    title = \"Year-over-Year Percentage Changes in Imports, Exports, and Total Trade\",\n    x = \"Year\",\n    y = \"Percentage Change\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.title.y = element_text(vjust = 1.5) \n  )\n\n\ngirafe(\n  ggobj = p2,\n  width_svg = 8,\n  height_svg = 8 * 0.618\n)\n\n\n\n\n\n\n\n\n4.2 Makeover 2: Bubble plots with the top 10 trading partners\n\n\n\n\n\n\n\nNote\n\n\n\nPros:\n\nThe above bubble chart shows the export and import of the major trading partners clearly.\n\nCos:\n\nThe above plot is stagnant, it does not show how the trend of the major trading partners over the years.\n\n\n\n\n\n4.2.1 Data preparation for bubble plot make over\nThe following code chucks are to prepare the data set required for the plotting of bubble plot.\n\n# Remove Regional Data \nyearly_import_long &lt;- yearly_import %&gt;%\n  select(-c(Total_imports, America, Asia, Europe, Oceania, Africa)) %&gt;%\n  pivot_longer(\n    cols = -Year,\n    names_to = \"Country\",\n    values_to = \"Import\"\n  )\n\n\n# Remove Regional Data \nyearly_export_long &lt;- yearly_export %&gt;%\n  select(-c(Total_exports, America, Asia, Europe, Oceania, Africa)) %&gt;%\n  pivot_longer(\n    cols = -Year,\n    names_to = \"Country\",\n    values_to = \"Export\"\n  )\n\n\n# Remove Regional Data and to select the top 10 trading partners\nyearly_trade_long &lt;- yearly_import_long %&gt;%\n  inner_join(yearly_export_long, by = c(\"Year\", \"Country\")) %&gt;%\n  mutate(Trade_Balance = Import - Export,\n         Total_Trade = Import + Export)%&gt;%\n  group_by(Year) %&gt;% # group the total trade by year to identify the top 10 major partners. \n  top_n(10, Total_Trade) %&gt;% \n  ungroup()\n\n\n\n4.2.2 Bubble Plots\n\n\n\n\n\n\nNote\n\n\n\nTo improve on the visualization, animated and interactive bubble plots are plotted.\nThe first plot is the animated bubble plots showing the transition across time of the top 10 major trading partners. This will allow the viewers to view the trend over time.\nThe second plot is the interactive bubble plot where the viewer could use the bar to view the transition of the trade between the major partners in the past 10 years at their own pace, pausing as and when they want.\n\n\nThe code below specified the colours of the each country codes.\n\ncountry_colors &lt;- c(\n  \"Taiwan\" = \"#FF6B6B\",\n  \"United States\" = \"#008080\",\n  \"Germany\" = \"#B0C4DE\",\n  \"Malaysia\" = \"#C7F464\",\n  \"China\" = \"#4ECDC4\",\n  \"Korea, Rep of\" = \"#C44D58\",\n  \"Japan\" = \"#FFCC5C\",\n  \"Hong Kong\" = \"#8A2BE2\",\n  \"Indonesia\" = \"#556270\",\n  \"Thailand\" = \"#6A4A3C\"\n)\n\n\nggplot(yearly_trade_long, aes(x = Export, y = Import, \n                      size = Total_Trade,, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = TRUE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = 'Total Export', \n       y = 'Total Import') +\n  transition_time(Year) +       \n  ease_aes('linear') \n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(yearly_trade_long, \n       aes(x = Import, \n           y = Export, \n           size = Total_Trade, \n           colour = Country)) +\n  geom_point(aes(size = Total_Trade,\n                 frame = Year), alpha = 0.7, \n             show.legend = FALSE)+\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = 'Export', y = 'Import')\n\n\nggplotly (gg)\n\n\n\n\n\n\n\n4.3 Make-over 3:\n\n\n\n\n\n\nNote\n\n\n\nPros:\n\nThe infographics icons facilitate the users’ understanding the kind of goods or items in each commodity section is about.\nThe trade balance of each of the commodities could be estimated, even though visually is not very clear.\nUniform colours are being used for exports and imports across commodities which allows clearer picture.\n\nCos:\n\nSimilar to the 1st chart, it is not very clear on which commodity has a biggest proportion in total trade in dollars.\nIt doesn’t show the proportion of different types of export which are being catorgised to Re-export and Domestic Export.\n\n\n\n\n\n\n4.3.1 Tree maps\n\n\n\n\n\n\nNote\n\n\n\nThe codes chunk below is to plot the tree map the trade amount by commodities.\nWhile tree map is typically used to show hierarchical relationship, for this use case, we are using it to show the proportion of commodities by trade types which is namely total amount for domestic export and total amount for re-export.\nTo explore the different applications of the long and wide data sets were used to plot two tree maps.\nThe first tree map in section 4.3.1.1, was plotted using the wider data set. It does not have hierarchical relationship. It solely shows the proportion of total trade by across different commodities.\nThe second tree map in section 4.3.1.2, was plotted using long data set. It shows proper tree map showing the different proportion of each trade types for each of the commodities.\nAt one quick glance of the 2nd tree map in section 4.3.1.2, viewers will be able to identify that the largest trade is in machinery and transport equipment with good trade balance between import and export amount. Singapore being an international trading hub and land scarce to cater for heavy industrial on manufacturing of Machinery and Transport Equipment, the majority of the export equipment are in re-export rather than domestic export.\nHence using the above observation, is the supportive proof that tree map could be an improved version for the visuals.\n\n\n\n4.3.1.1 Tree Map Using The Wider Data\n\nfiltered_commodities_wider &lt;- filtered_commodities_long%&gt;%\n  pivot_wider(\n    names_from = Trade_Type,\n    values_from = Value\n  )\n\n\nfiltered_commodities_wider &lt;- filter(filtered_commodities_wider, \n                                         Year == 2024, \n                                         Data_Series != \"Non-Oil\", \n                                         Data_Series != \"Total Domestic Exports, (At Current Prices)\", \n                                         Data_Series != \"Total Merchandise Imports, (At Current Prices)\", \n                                         Data_Series != \"Total Merchandise Exports, (At Current Prices)\", \n                                         Data_Series != \"Total Re-Exports, (At Current Prices)\", \n                                         Data_Series != \"Total Merchandise Trade, (At Current Prices)\")\n\n\nhead(filtered_commodities_wider)\n\n# A tibble: 6 × 7\n# Groups:   Year [1]\n   Year Data_Series             Total_Domestic_Exports Total_Export Total_Import\n  &lt;dbl&gt; &lt;chr&gt;                                    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1  2024 Animal & Vegetable Oil…                174915.      271570.     3098804.\n2  2024 Beverages & Tobacco                    183274.     4600202.     4879537.\n3  2024 Chemicals & Chemical P…              44438593.    69516959.    42766218.\n4  2024 Crude Materials (Excl …               2673166.     4149311.     3038816.\n5  2024 Food & Live Animals                  11528731.    13785435.    13864241.\n6  2024 Machinery & Transport …              70379106.   363221996.   322073528.\n# ℹ 2 more variables: Total_Re_exports &lt;dbl&gt;, Total_Trade &lt;dbl&gt;\n\n\n\n# Plot the treemap\ntreemap(filtered_commodities_wider,\n        index = c(\"Data_Series\", \"Total_Trade\"),\n        vSize = \"Total_Trade\",\n        vColor = \"Data_Series\",\n        title = \"2024 No Oil Commodities by Total Trade Amount\",\n        title.legend = \"A\")\n\n\n\n\n\n\n\n\n\n\n4.3.1.2 Tree Map Using The Longer Data\n\nfiltered_commodities_longer_2024&lt;- filter(filtered_commodities_long, \n                                         Year == 2024, \n                                         Data_Series != \"Non-Oil\", \n                                         Data_Series != \"Total Domestic Exports, (At Current Prices)\", \n                                         Data_Series != \"Total Merchandise Imports, (At Current Prices)\", \n                                         Data_Series != \"Total Merchandise Exports, (At Current Prices)\", \n                                         Data_Series != \"Total Re-Exports, (At Current Prices)\", \n                                         Data_Series != \"Total Merchandise Trade, (At Current Prices)\", \n                                         Trade_Type != \"Total_Trade\",\n                                         Trade_Type != \"Total_Export\")\n\ntreemap(filtered_commodities_longer_2024,\n        index = c(\"Data_Series\", \"Trade_Type\"),\n        vSize = \"Value\",\n        vColor = \"Trade_Type\",\n        title = \"A\",\n        title.legend = \"A\")"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_02.html#time-series-analysis",
    "href": "Take-home_Exe/Take-home_Ex_02.html#time-series-analysis",
    "title": "Take Home Exercise 02",
    "section": "5.0 Time Series Analysis",
    "text": "5.0 Time Series Analysis\nThis section is to carry out time series analysis using the import dataset by countries.\nWe will focus the analysis on the major trading partners derived in section 4.2 when plotting the bubble plot.\n\n5.1 Time Series Data Preparation\n\ntop10 &lt;- c(\"Month_Year\",\"Taiwan\", \"United States\", \"Germany\", \"Malaysia\", \"China\", \"Korea, Rep of\", \n           \"Japan\", \"Hong Kong\", \"Indonesia\", \"Thailand\")\n\nexisting_columns &lt;- top10[top10 %in% colnames(transposed_import)]\n\ntransposed_import_1 &lt;- transposed_import[existing_columns]\n\n\nts_import &lt;-transposed_import_1\n\nts_import$Month_Year &lt;- parse_date_time(ts_import $Month_Year, orders = \"my\")\n\n\nts_import_ts &lt;- ts(ts_import)       \nhead(ts_import_ts)\n\n     Month_Year  Taiwan United States Germany Malaysia  China  Japan Hong Kong\n[1,] 1735689600 10152.7        5388.6   902.9   6445.1 6801.2 2600.1     696.0\n[2,] 1733011200  8847.7        6651.8  1108.3   6569.9 7168.0 2790.1     769.2\n[3,] 1730419200  7360.0        5988.7  1070.4   5778.2 6841.2 2879.8     442.4\n[4,] 1727740800  6976.4        6092.8  1103.5   5574.0 6070.1 2943.2     386.8\n[5,] 1725148800  7195.8        6449.3  1081.3   5462.0 5716.2 2381.9     476.6\n[6,] 1722470400  6813.1        7258.3  1163.3   5234.6 6249.9 2377.9     379.7\n     Indonesia Thailand\n[1,]    1765.7   1139.7\n[2,]    2006.9   1421.6\n[3,]    1940.8   1228.3\n[4,]    1973.1   1517.2\n[5,]    1650.2   1250.2\n[6,]    1826.1   1202.7\n\n\n\nts_tsibble &lt;- ts_import %&gt;%\n  mutate(Month = yearmonth(`Month_Year`)) %&gt;%\n  as_tsibble(index = `Month`)\n\n\n\n5.2 Visualizing time series\n\nts_longer &lt;- ts_import%&gt;%\n  pivot_longer(cols = c(2:10),\n               names_to = \"Country\",\n               values_to = \"Imports\")\n\n\nts_longer %&gt;%\n  filter(Country == \"United States\") %&gt;%\n  ggplot(aes(x = `Month_Year`, \n             y = Imports))+\n  geom_line(size = 0.5)\n\n\n\n\n\n\n\n\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:10),\n               names_to = \"Country\",\n               values_to = \"Imports\")\n\n\nggplot(data = ts_longer, \n       aes(x = `Month_Year`, \n           y = Imports,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\n\nggplot(data = ts_longer, \n       aes(x = `Month_Year`, \n           y = Imports))+\n  geom_line(size = 0.5) +\n  facet_wrap(~ Country,\n             ncol = 3,\n             scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n5.3 Visual Analysis of Time-series Data\n\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:10),\n               names_to = \"Country\",\n               values_to = \"Imports\")\n\n\ntsibble_longer %&gt;%\n  filter(Country == \"Thailand\" |\n         Country == \"Malaysia\" |\n         Country == \"Indonesia\" |\n         Country == \"Taiwan\"|\n         Country == \"China\"|\n         Country == \"United States\" |\n         Country == \"Germany\") %&gt;% \n  gg_season(Imports)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBased on the visualisation of the top 10 countries, it was observed, while there has been significant increase in the import from China, United States and Germany the recent years, it seems that there are no clear cycles across the months, hence we will focus our analysis on our nearest neighbours Malaysia and Thailand.\nWe will focus on the analysis on Singapore’s nearest neighbours Malaysia and Thailand, given there is no obvious cycle in imports from the rest of the countries.\n\n\n\ntsibble_longer %&gt;%\n  filter(Country == \"Thailand\" |\n         Country == \"Malaysia\") %&gt;% \n  autoplot(Imports) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;%\n  filter(Country == \"Thailand\" |\n         Country == \"Malaysia\") %&gt;% \n  gg_subseries(Imports)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is interesting to observe that for both countries, the importation of goods to Singapore is quite stable throughout the year. This is interesting as Singapore imports its food sources from these countries. Before carrying out this analysis, one would have expected to observe some seasonalities on the importation of food sources leading to spike in a certain months before the festival seasons or after harvest seasons.\n\n\n\n\n5.4 Time Series Decomposition\n\ntsibble_longer %&gt;%\n   filter(Country == \"Thailand\" |\n         Country == \"Malaysia\") %&gt;% \n  ACF(Imports) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;%\n filter(Country == \"Thailand\" |\n         Country == \"Malaysia\") %&gt;% \n  PACF(Imports) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBase on the charts, it was observed that there are significant results at lag -1 before 6 months. so there were some seasonality observed.\n\n\n\ntsibble_longer %&gt;% \n  filter(Country == \"Thailand\") %&gt;%\n  gg_tsdisplay(Imports)\n\n\n\n\n\n\n\n\n\ntsibble_longer %&gt;% \n  filter(Country == \"Malaysia\") %&gt;%\n  gg_tsdisplay(Imports)\n\n\n\n\n\n\n\n\n\n\n5.5 Visual STL Diagnostics\nBased on the time series decomposition, there are some seasonality observed which are significant in both countries. However due to the time restaints, we will carry out further forecasting using import data from Malaysia only using STL method.\nWe will hold out the hold the last 12 months of the data for testing while the rest for training.\n\nTotal_Import_TS &lt;- tsibble_longer %&gt;%\n  filter(Country == \"Malaysia\") %&gt;% \n  mutate(Type = if_else(\n    `Month_Year` &gt;= \"2024-01-01\", \n    \"Hold-out\", \"Training\"))\n\n\nTotal_Import_train &lt;- Total_Import_TS %&gt;%\n  filter(`Month_Year` &lt; \"2024-01-01\")\n\n\nTotal_Import_train %&gt;%\n  model(stl = STL(Imports)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n5.6 Visual Forecasting\n\nfit_ses &lt;- Total_Import_train %&gt;%\n  model(ETS(Imports ~ error(\"A\") \n            + trend(\"N\") \n            + season(\"N\")))\nfit_ses\n\n# A mable: 1 x 2\n# Key:     Country [1]\n  Country  `ETS(Imports ~ error(\"A\") + trend(\"N\") + season(\"N\"))`\n  &lt;chr&gt;                                                   &lt;model&gt;\n1 Malaysia                                           &lt;ETS(A,N,N)&gt;\n\n\n\ngg_tsresiduals(fit_ses)\n\n\n\n\n\n\n\n\n\nfit_ses %&gt;%\n  report()\n\nSeries: Imports \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.4836604 \n\n  Initial states:\n    l[0]\n 2941.07\n\n  sigma^2:  165349.9\n\n     AIC     AICc      BIC \n4425.395 4425.491 4435.983 \n\n\n\nTotal_Import_H &lt;- Total_Import_train %&gt;%\n  model(`Holt's method` = \n          ETS(Imports ~ error(\"A\") +\n                trend(\"A\") + \n                season(\"N\")))\nTotal_Import_H %&gt;% report()\n\nSeries: Imports \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.4798454 \n    beta  = 0.0001000284 \n\n  Initial states:\n     l[0]    b[0]\n 2918.213 9.40927\n\n  sigma^2:  166361.2\n\n     AIC     AICc      BIC \n4428.907 4429.151 4446.554 \n\n\n\ngg_tsresiduals(Total_Import_H)\n\n\n\n\n\n\n\n\n\nTotal_Import_WH &lt;- Total_Import_train %&gt;%\n  model(\n    Additive = ETS(Imports ~ error(\"A\") \n                   + trend(\"A\") \n                   + season(\"A\")),\n    Multiplicative = ETS(Imports ~ error(\"M\") \n                         + trend(\"A\") \n                         + season(\"M\"))\n    )\n\nTotal_Import_WH %&gt;% report()\n\n# A tibble: 2 × 10\n  Country  .model         sigma2 log_lik   AIC  AICc   BIC    MSE   AMSE     MAE\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Malaysia Additive      1.41e+5  -2182. 4399. 4401. 4459. 1.32e5 1.64e5 2.61e+2\n2 Malaysia Multiplicati… 6.81e-3  -2170. 4375. 4378. 4435. 1.43e5 1.79e5 5.97e-2\n\n\n\nfit_ETS &lt;- Total_Import_train %&gt;%\n  model(`SES` = ETS(Imports ~ error(\"A\") + \n                      trend(\"N\") + \n                      season(\"N\")),\n        `Holt`= ETS(Imports ~ error(\"A\") +\n                      trend(\"A\") +\n                      season(\"N\")),\n        `damped Holt` = \n          ETS(Imports ~ error(\"A\") +\n                trend(\"Ad\") + \n                season(\"N\")),\n        `WH_A` = ETS(\n          Imports ~ error(\"A\") + \n            trend(\"A\") + \n            season(\"A\")),\n        `WH_M` = ETS(Imports ~ error(\"M\") \n                         + trend(\"A\") \n                         + season(\"M\"))\n  )\n\n\nfit_ETS %&gt;%\n  tidy()\n\n# A tibble: 45 × 4\n   Country  .model      term     estimate\n   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;\n 1 Malaysia SES         alpha    0.484   \n 2 Malaysia SES         l[0]  2941.      \n 3 Malaysia Holt        alpha    0.480   \n 4 Malaysia Holt        beta     0.000100\n 5 Malaysia Holt        l[0]  2918.      \n 6 Malaysia Holt        b[0]     9.41    \n 7 Malaysia damped Holt alpha    0.480   \n 8 Malaysia damped Holt beta     0.000100\n 9 Malaysia damped Holt phi      0.968   \n10 Malaysia damped Holt l[0]  2862.      \n# ℹ 35 more rows\n\n\n\nfit_ETS %&gt;% \n  report()\n\n# A tibble: 5 × 10\n  Country  .model         sigma2 log_lik   AIC  AICc   BIC    MSE   AMSE     MAE\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Malaysia SES           1.65e+5  -2210. 4425. 4425. 4436. 1.64e5 2.00e5 3.01e+2\n2 Malaysia Holt          1.66e+5  -2209. 4429. 4429. 4447. 1.64e5 1.99e5 2.99e+2\n3 Malaysia damped Holt   1.67e+5  -2209. 4431. 4431. 4452. 1.63e5 1.99e5 2.99e+2\n4 Malaysia WH_A          1.41e+5  -2182. 4399. 4401. 4459. 1.32e5 1.64e5 2.61e+2\n5 Malaysia WH_M          6.81e-3  -2170. 4375. 4378. 4435. 1.43e5 1.79e5 5.97e-2\n\n\n\n\n5. 7 Forecasting Future Values\n\nfit_ETS %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(Total_Import_TS, \n           level = NULL)\n\n\n\n\n\n\n\n\n\nfit_autoETS &lt;- Total_Import_train%&gt;%\n  model(ETS(Imports))\nfit_autoETS %&gt;% report()\n\nSeries: Imports \nModel: ETS(M,A,A) \n  Smoothing parameters:\n    alpha = 0.4992295 \n    beta  = 0.0001020733 \n    gamma = 0.0004933497 \n\n  Initial states:\n     l[0]     b[0]     s[0]     s[-1]    s[-2]    s[-3]    s[-4]    s[-5]\n 3201.362 17.58228 18.96767 -16.42075 153.2938 96.81041 93.71874 157.8929\n    s[-6]     s[-7]     s[-8]    s[-9]    s[-10]    s[-11]\n 84.98671 -78.56033 -68.99226 168.0849 -429.0114 -180.7704\n\n  sigma^2:  0.0063\n\n     AIC     AICc      BIC \n4353.715 4356.330 4413.715 \n\n\n\nfit_autoETS %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(Total_Import_train)\n\n\n\n\n\n\n\n\n\nfc_autoETS &lt;- fit_autoETS %&gt;%\n  forecast(h = \"12 months\")\n\nTotal_Import_TS %&gt;%\n  ggplot(aes(x=`Month`, \n             y=Imports)) +\n  autolayer(fc_autoETS, \n            alpha = 0.6) +\n  geom_line(aes(\n    color = Type), \n    alpha = 0.8) + \n  geom_line(aes(\n    y = .mean, \n    colour = \"Forecast\"), \n    data = fc_autoETS) +\n  geom_line(aes(\n    y = .fitted, \n    colour = \"Fitted\"), \n    data = augment(fit_autoETS))\n\n\n\n\n\n\n\n\n\n\n5.8 Conclusion\n\n\n\n\n\n\nNote\n\n\n\nThe use of ETS models: fable method doesn’t yield good results, the ranges of both 90% and 80% levels are very wide. The forecast values are quite far from the hold out data. Due to the time limit of the assignment, other methods were not explored further to build better forecast model."
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_08qmd.html",
    "href": "Hands-on_Exe/Hands-on_Ex_08qmd.html",
    "title": "Hands-on Exercise 08",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\dewschan\\ISSS608\\Hands-on_Exe\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\npacman::p_load(sf, tmap, tidyverse)\n\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\ntmap_mode(\"plot\") # change to plot for smooth rendering\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\npacman::p_load(tmap, tidyverse, sf)\n\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")\n\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent, na.rm = TRUE) #modify the code, as there may be missing data in the database. \n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -60.50   0.00  12.25  34.00  60.75 133.50 278.00\n\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_08qmd.html#choropleth-mapping-with-r",
    "href": "Hands-on_Exe/Hands-on_Ex_08qmd.html#choropleth-mapping-with-r",
    "title": "Hands-on Exercise 08",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\dewschan\\ISSS608\\Hands-on_Exe\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\npacman::p_load(sf, tmap, tidyverse)\n\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\n\n\n\n\ntmap_mode(\"plot\") # change to plot for smooth rendering\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\npacman::p_load(tmap, tidyverse, sf)\n\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")\n\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent, na.rm = TRUE) #modify the code, as there may be missing data in the database. \n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -60.50   0.00  12.25  34.00  60.75 133.50 278.00\n\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "In-class_Exe/In-class_Ex08.html",
    "href": "In-class_Exe/In-class_Ex08.html",
    "title": "In-Class Exe 08- Modified codes using the new tmap package",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\dewschan\\ISSS608\\In-class_Exe\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "In-class_Exe/In-class_Ex08.html#choropleth-mapping-with-r",
    "href": "In-class_Exe/In-class_Ex08.html#choropleth-mapping-with-r",
    "title": "In-Class Exe 08- Modified codes using the new tmap package",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\dewschan\\ISSS608\\In-class_Exe\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html",
    "href": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html",
    "title": "Hands-o Exe 08- Modified codes using the new tmap package",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\dewschan\\ISSS608\\Hands-on_Exe\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n## Revised code with tmap version 4.0 \n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(fill = \"DEPENDENCY\", \n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(fill = \"DEPENDENCY\", \n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntmap_style(\"white\")\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#choropleth-mapping-with-r",
    "href": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#choropleth-mapping-with-r",
    "title": "Hands-o Exe 08- Modified codes using the new tmap package",
    "section": "",
    "text": "pacman::p_load(sf, tmap, tidyverse)\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\dewschan\\ISSS608\\Hands-on_Exe\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n## Revised code with tmap version 4.0 \n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(fill = \"DEPENDENCY\", \n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(fill = \"DEPENDENCY\", \n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\ntmap_style(\"white\")\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#visualising-geospatial-point-data",
    "href": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#visualising-geospatial-point-data",
    "title": "Hands-o Exe 08- Modified codes using the new tmap package",
    "section": "Visualising Geospatial Point Data",
    "text": "Visualising Geospatial Point Data\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThe tmap view is changed to plot to\n\ntmap_mode(\"plot\")\n\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"red\",\n           size = 1,\n           col = \"black\",\n           lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"red\",\n             size = \"Gp1Gp2 Winnings\",\n             col = \"black\",\n             lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"OUTLET TYPE\", \n             size = \"Gp1Gp2 Winnings\",\n             col = \"black\",\n             lwd = 1)\n\n\n\n\n\n\n\n\n\ntm_shape(sgpools_sf) + \n  tm_bubbles(fill = \"OUTLET TYPE\", \n             size = \"Gp1Gp2 Winnings\",\n             col = \"black\",\n             lwd = 1) + \n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#analytical-mapping",
    "href": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#analytical-mapping",
    "title": "Hands-o Exe 08- Modified codes using the new tmap package",
    "section": "Analytical Mapping",
    "text": "Analytical Mapping\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")\n\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_polygons(col = \"wp_functional\",\n              n = 10,\n              style = \"equal\",\n              palette = \"Blues\",\n              border.col = \"black\",\n              lwd = 0.1,\n              col_alpha = 1) +\n  tm_title(\"Distribution of functional water point by LGAs\") +\n  tm_layout(legend.outside = FALSE)\n\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_polygons(col = \"total_wp\",\n              n = 10,\n              style = \"equal\",\n              palette = \"Blues\",\n              border.col = \"black\",\n              lwd = 0.1,\n              col_alpha = 1) +\n  tm_title(\"Distribution of total water point by LGAs\") +\n  tm_layout(legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n\n\n\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#extreme-value-maps",
    "href": "Hands-on_Exe/Hands-on_Ex_08_newTmap.html#extreme-value-maps",
    "title": "Hands-o Exe 08- Modified codes using the new tmap package",
    "section": "Extreme Value Maps",
    "text": "Extreme Value Maps\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\npercentmap &lt;- function(vnam, df, legtitle = NA, mtitle = \"Percentile Map\") {\n  percent &lt;- c(0, 0.01, 0.1, 0.5, 0.9, 0.99, 1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  \n  tm_shape(df) +\n    tm_polygons() +\n    tm_shape(df) +\n    tm_polygons(col = vnam,\n                title = legtitle,\n                breaks = bperc,\n                palette = \"brewer.blues\",\n                labels = c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\")) +\n    tm_title(mtitle) +\n    tm_layout(title.position = c(\"right\", \"bottom\"))\n}\n\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle = NA,\n                   mtitle = \"Box Map\",\n                   mult = 1.5) {\n  var &lt;- get.var(vnam, df)\n  bb &lt;- boxbreaks(var)\n  tm &lt;- tm_shape(df) +\n    tm_polygons(\n      col = vnam,\n      fill.scale = tm_scale(\n        breaks = bb,\n        values = \"brewer.blues\",\n        labels = c(\n          \"lower outlier\", \n          \"&lt; 25%\", \n          \"25% - 50%\", \n          \"50% - 75%\",\n          \"&gt; 75%\", \n          \"upper outlier\"\n        )\n      ),\n      fill.legend = tm_legend(title = legtitle)\n    ) +\n    tm_title(mtitle) +\n    tm_layout(\n      legend.position = c(\"left\", \"top\")\n    )\n  return(tm)\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html",
    "title": "Hands-on Exe 09",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html#modelling-visualising-and-analysing-network-data-with-r",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html#modelling-visualising-and-analysing-network-data-with-r",
    "title": "Hands-on Exe 09",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html#importing-network-data-from-files",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html#importing-network-data-from-files",
    "title": "Hands-on Exe 09",
    "section": "Importing network data from files",
    "text": "Importing network data from files\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html#reviewing-the-imported-data",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html#reviewing-the-imported-data",
    "title": "Hands-on Exe 09",
    "section": "Reviewing the imported data",
    "text": "Reviewing the imported data\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html#wrangling-time",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html#wrangling-time",
    "title": "Hands-on Exe 09",
    "section": "Wrangling time",
    "text": "Wrangling time\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html#creating-network-objects-using-tidygraph",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on Exe 09",
    "section": "Creating network objects using tidygraph",
    "text": "Creating network objects using tidygraph\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday   Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;      &lt;int&gt;\n 1    40    41 Saturday      13\n 2    41    43 Monday        11\n 3    35    31 Tuesday       10\n 4    40    41 Monday        10\n 5    40    43 Monday        10\n 6    36    32 Sunday         9\n 7    40    43 Saturday       9\n 8    41    40 Monday         9\n 9    19    15 Wednesday      8\n10    35    38 Tuesday        8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on Exe 09",
    "section": "Plotting Static Network Graphs with ggraph package",
    "text": "Plotting Static Network Graphs with ggraph package\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_09.html#creating-facet-graphs",
    "href": "Hands-on_Exe/Hands-on_Ex_09.html#creating-facet-graphs",
    "title": "Hands-on Exe 09",
    "section": "Creating facet graphs",
    "text": "Creating facet graphs\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\nNetwork Metrics Analysis\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\n\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\n\nBuilding Interactive Network Graph with visNetwork\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") \n\n\n\n\n\n\nGAStech_nodes &lt;- GAStech_nodes %&gt;%\n  rename(group = Department)\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visEdges(arrows = \"to\", \n           smooth = list(enabled = TRUE, \n                         type = \"curvedCW\")) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\nvisNetwork(GAStech_nodes,\n           GAStech_edges_aggregated) %&gt;%\n  visIgraphLayout(layout = \"layout_with_fr\") %&gt;%\n  visOptions(highlightNearest = TRUE,\n             nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)"
  },
  {
    "objectID": "In-class_Exe/In-class_Ex09.html",
    "href": "In-class_Exe/In-class_Ex09.html",
    "title": "In-Class Exercise 09",
    "section": "",
    "text": "pacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nImportant for the date and tijme to be in numerical fields\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\nThis is the process the data to weekday field so that we can use a new column.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;% #different field name to differentiate the two different sent date. \n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "Hands-on_Exe/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Exe/data/geospatial/MPSZ-2019.html",
    "title": "ISSS608 Coursework",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_10.html",
    "href": "Hands-on_Exe/Hands-on_Ex_10.html",
    "title": "Hands-on 10",
    "section": "",
    "text": "pacman::p_load(RODBC,readr, tidyverse, ggthemes, gt, gtExtras, svglite, reactable, knitr, reactable,reactablefmtr )\n\n\ncoffeechain &lt;-read_rds(\"data/rds/CoffeeChain.rds\")\n\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n###sparklines in ggplot2\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_10.html#information-dashboard-design-r-methods",
    "href": "Hands-on_Exe/Hands-on_Ex_10.html#information-dashboard-design-r-methods",
    "title": "Hands-on 10",
    "section": "",
    "text": "pacman::p_load(RODBC,readr, tidyverse, ggthemes, gt, gtExtras, svglite, reactable, knitr, reactable,reactablefmtr )\n\n\ncoffeechain &lt;-read_rds(\"data/rds/CoffeeChain.rds\")\n\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n###sparklines in ggplot2\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_10.html#sparklines-gtextras-method",
    "href": "Hands-on_Exe/Hands-on_Ex_10.html#sparklines-gtextras-method",
    "title": "Hands-on 10",
    "section": "sparklines: gtExtras method",
    "text": "sparklines: gtExtras method\n\nreport_r &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\n\nreport_r %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\nreport_r%&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\nPlotting Coffechain Sales report\n\nreport_r %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K"
  },
  {
    "objectID": "Hands-on_Exe/Hands-on_Ex_10.html#adding-statistics",
    "href": "Hands-on_Exe/Hands-on_Ex_10.html#adding-statistics",
    "title": "Hands-on 10",
    "section": "Adding statistics",
    "text": "Adding statistics\n\nreport_r %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\nCombining the data.frame\n\nspark &lt;- report_r %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n\nsales &lt;- report_r %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\n\nsales_data = left_join(sales, spark)\n\n\n\nPlotting the updated data.table\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\nCombining bullet chart and sparklines\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\nremotes::install_github(\"timelyportfolio/dataui\")\n\n\nlibrary(dataui)\n\n\nreport &lt;- report_r %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\n\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_spark(report$`Monthly Sales`)\n    )\n  )\n)\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_03.html",
    "href": "Take-home_Exe/Take-home_Ex_03.html",
    "title": "Take Home Exercise 03",
    "section": "",
    "text": "The goal of this exercise is to:"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_03.html#data-sets",
    "href": "Take-home_Exe/Take-home_Ex_03.html#data-sets",
    "title": "Take Home Exercise 03",
    "section": "Data sets:",
    "text": "Data sets:\nThe following data sets were used for this exercise:\n\nThe climate data set was being scraped from Meteorological Service Website, Singapore using beautiful soup python package.\nThe scraped historical daily weather records in Singapore were then combined using python into one csv file and loaded in R Studio.\nThe dengue data sets were provided at the courtesy of the National Environment Agency (NEA), Singapore.\nThe data set is also publicly available on NEA website and Ministry of Health website."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_03.html#libraries",
    "href": "Take-home_Exe/Take-home_Ex_03.html#libraries",
    "title": "Take Home Exercise 03",
    "section": "Libraries:",
    "text": "Libraries:\nThe following libraries were used for this take home exercise.\n\npacman::p_load(ggstatsplot, plotly, DT, scales, tidyverse, readxl, SmartEDA, skimr, corrplot, readxl, ggdist, ggridges, ggthemes, colorspace, scales, nortest, easystats, tidymodels)"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_03.html#data-preparation-process",
    "href": "Take-home_Exe/Take-home_Ex_03.html#data-preparation-process",
    "title": "Take Home Exercise 03",
    "section": "Data Preparation Process:",
    "text": "Data Preparation Process:\nThe preliminary data preparation for the data sets were detailed in [project webpage] (https://isss608jan25group7.netlify.app/shiny/data_prep).\nUsing the pre-prepared data, this section will transform the weather data so that EDA and CDA could be performed in the subsequent steps.\nFor this part of the analysis, it will focus on the time period from 2014 to Jun 2024. This is to aligned to the availability of data time period from other data sets, the dengue data set is available from 2014 to 2024 and electricity consumption is available from 2005 to Jun 2024.\nHence the common period is from 2014 to June 2024. we would be using this common period.\nAlthough the weather station at Changi has the most completed dataset,"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_03.html#weather-data",
    "href": "Take-home_Exe/Take-home_Ex_03.html#weather-data",
    "title": "Take Home Exercise 03",
    "section": "Weather Data:",
    "text": "Weather Data:\nThe following code chuck is to read the weather data.\n\nweather &lt;- read_rds('data/Ex03/rds/weather.rds')\n\n\nskim(weather)\n\nThe list of the active weather stations are are found in this link: https://www.weather.gov.sg/wp-content/uploads/2024/08/Station_Records.pdf\nBased on the above summary table, we noted that there are a lot of missing data. with reference to the weather stations data records, 16 weather stations were selected. The names of the selected weather stations were listed in the code chunk below.\n\nfiltered_weather &lt;- filter(weather, \n                           Date &gt;= as.Date(\"2014-01-01\") & \n                           Date &lt;= as.Date(\"2024-06-30\") & \n                           Station %in% c('Admiralty','Ang Mo Kio', 'Changi', \n                                          'Choa Chu Kang (south)', 'Clementi', \n                                          'East Coast Parkway', 'Jurong (West)', \n                                          'Marina Barrage','Newton',\n                                          'Paya Lebar', 'Sembawang',\n                                          'Pasir Panjang', 'Seletar',\n                                          'Tai Seng','Tengah','Tuas South'))%&gt;%\n                            filter(MinTemperature &gt;= 15)\n\nUse summary function to view the filtered weather\n\nsummary (filtered_weather)\n\nThe following code chunk is used to aggreate the data together at monthly dimension.\n\nweather_Monthly &lt;- filtered_weather %&gt;%\n  group_by(Year, Month) %&gt;%\n  summarize(\n    AvgMeanTemp = mean(MeanTemperature, na.rm = TRUE),\n    MaxTemp = max(MaxTemperature, na.rm = TRUE),\n    MinTemp = min(MinTemperature, na.rm = TRUE),\n    total_rainfall=sum(DailyRainfall,na.rm=TRUE),\n    Highest30minRainfall=max(Highest30minRainfall,na.rm=TRUE),\n    Highest60minRainfall=max(Highest60minRainfall, na.rm=TRUE),\n    Highest120minRainfall=max(Highest120minRainfall, na.rm=TRUE), \n    DaysAbove35 = sum(MaxTemperature &gt;=35, na.rm = TRUE)\n  )\n\nThe following code chuck is to aggregate the yearly data together.\n2024 data are being dropped as the electricity consumption data for 2024 is incomplete\n\nweather_Yearly &lt;- filtered_weather %&gt;%\n  group_by(Year) %&gt;%  # Group by Year only\n  summarize(\n    AvgMeanTemp = mean(MeanTemperature, na.rm = TRUE),\n    MaxTemp = max(MaxTemperature, na.rm = TRUE),        \n    MinTemp = min(MinTemperature, na.rm = TRUE),      \n    total_rainfall = sum(DailyRainfall, na.rm = TRUE),\n    Highest30minRainfall = max(Highest30minRainfall, na.rm = TRUE),\n    Highest60minRainfall = max(Highest60minRainfall, na.rm = TRUE), \n    Highest120minRainfall = max(Highest120minRainfall, na.rm = TRUE), \n    DaysAbove35 = sum(MaxTemperature &gt;=35, na.rm = TRUE)\n  )%&gt;%\n  filter(Year != 2024)  \n\n\nData Preparation for the dengue data\nThe dengue data set was provided at the courtesy of the National Environment Agency (NEA), Singapore.\nThe data set is also publicly available on NEA website and Ministry of Health website.\n\nMonthly_dengue &lt;-read_xlsx(\"data/Ex03/Denguecases.xlsx\", \"Monthly\")\n\nThe following code is to transform the data to longer format, year and months columns to numeric.\n\nMonthly_dengue_longer &lt;- Monthly_dengue %&gt;%\n  pivot_longer(\n    cols = 2:12,\n    names_to = \"Year\",\n    values_to = \"denguecases\"\n  ) %&gt;%\n  filter(!str_detect(`Month/Year`, \"Total\")) %&gt;%\n  rename(Month = `Month/Year`) %&gt;%\n  mutate(\n    Year = as.numeric(Year),\n    Month = as.numeric(Month)\n  )\n\nThe following code chunk is to derive the yearly dengue cases.\n\nyearly_dengue &lt;- Monthly_dengue_longer %&gt;%\n  filter(Year != 2024) %&gt;%\n  group_by(Year) %&gt;%\n  summarise(denguecases=sum(denguecases))\n\ncolnames(yearly_dengue)\n\nThe population data is available at the following data link.\nBased on the National Centre for Infectious Diseases article, the dengue incidence rate is being normalised by population, and transformed to the number of dengue cases per 100, 000 population.\nThis is usually only applicable total number of dengue cases reported yearly.\n\npopulation &lt;- read_xlsx(\"data/Ex03/population.xlsx\", \"Population\")\npopulation$Year &lt;- as.numeric(population$Year)\npopulation$Population &lt;- as.numeric(population$Population)\n\nyearly_dengue &lt;- yearly_dengue %&gt;% left_join(population, by = \"Year\")\n\nyearly_dengue$dengueincidencerate &lt;-yearly_dengue$denguecases/yearly_dengue$Population*100000\n\n\n\nData Preparation for the Electricity consumption data.\nThe electricity consumption data set is available at this link.\n\nNormalizing the total electricity consumption per household.\nThe data set were extracted and joined with the number of Households in Singapore. The Households data is available at this link.\n\n# Load Monthly electricity data\nMonthly_electricity &lt;- read_xlsx(\"data/Ex03/electricityconsumption_DS.xlsx\", \"Monthly\")\nMonthly_electricity$Year &lt;- as.numeric(Monthly_electricity$Year)\nMonthly_electricity$Monthly_Elec_consump &lt;- as.numeric(Monthly_electricity$Monthly_Elec_consump)\n\n# Check column names\ncolnames(Monthly_electricity)\n\n[1] \"Year\"                 \"Month\"                \"Monthly_Elec_consump\"\n\n# Load Households data\nHouseholds &lt;- read_xlsx(\"data/Ex03/Households.xlsx\", \"Households\")\nHouseholds$Year &lt;- as.numeric(Households$Year)\nHouseholds$NoofHouseholds &lt;- as.numeric(Households$NoofHouseholds)\n\n# Check column names\ncolnames(Households)\n\n[1] \"Year\"           \"NoofHouseholds\"\n\n# Combine datasets and calculate electricity consumption per household\nMonthly_electricity &lt;- Monthly_electricity %&gt;%\n  left_join(Households, by = \"Year\") %&gt;%\n  mutate(Monthly_Elec_per_Household = Monthly_Elec_consump / NoofHouseholds)\n\n\nyearly_electricity &lt;-read_xlsx(\"data/Ex03/electricityconsumption_DS.xlsx\", \"Yearly\")\n\n\n# Load yearly electricity data\nyearly_electricity &lt;- read_xlsx(\"data/Ex03/electricityconsumption_DS.xlsx\", \"Yearly\")\nyearly_electricity$Year &lt;- as.numeric(yearly_electricity$Year)\nyearly_electricity$Yearly_Elec_consump &lt;- as.numeric(yearly_electricity$Yearly_Elec_consump)\n\n# Load households data\nHouseholds &lt;- read_xlsx(\"data/Ex03/Households.xlsx\", \"Households\")\nHouseholds$Year &lt;- as.numeric(Households$Year)\nHouseholds$NoofHouseholds &lt;- as.numeric(Households$NoofHouseholds)\n\n# Combine datasets and calculate yearly electricity consumption per household\nyearly_electricity &lt;- yearly_electricity %&gt;%\n  left_join(Households, by = \"Year\") %&gt;%\n  mutate(\n    Yearly_Elec_per_Household = Yearly_Elec_consump / NoofHouseholds\n  )\n\n\n\n\nCombining data sets\nThe code chunks below is for combining the various data sets, both the monthly and the year version will be written to rds.\n\ncombined_Monthly_data &lt;- Monthly_dengue_longer %&gt;% \n  left_join(weather_Monthly, by = c(\"Year\", \"Month\")) %&gt;%\n  left_join(Monthly_electricity, by = c(\"Year\", \"Month\"))%&gt;%\n  filter(!is.na(Monthly_Elec_consump))  %&gt;%\n  select(-NoofHouseholds)\n\n\nwrite_rds(combined_Monthly_data, file = \"data/Ex03/rds/combined_Monthly_data.rds\")\n\n\ncombined_Monthly_data &lt;- read_rds(\"data/Ex03/rds/combined_Monthly_data.rds\")\n\n\ncombined_Yearly_data &lt;- yearly_dengue %&gt;% \n  left_join(weather_Yearly, by = c(\"Year\")) %&gt;%\n  left_join(yearly_electricity, by = c(\"Year\"))%&gt;%\n  select(-Population, -NoofHouseholds)\n\n\nwrite_rds(combined_Yearly_data, file = \"data/Ex03/rds/combined_Yearly_data.rds\")\n\n\ncombined_Yearly_data &lt;- read_rds(\"data/Ex03/rds/combined_Yearly_data.rds\")"
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_03.html#exploratory-data-analysis-eda",
    "href": "Take-home_Exe/Take-home_Ex_03.html#exploratory-data-analysis-eda",
    "title": "Take Home Exercise 03",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nDistribution of of the data\n\nHistograms\n\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Ensure all numeric columns are selected (excluding non-numeric ones like \"Year\" or \"Month\")\nlong_data &lt;- combined_Monthly_data %&gt;%\n  pivot_longer(cols = where(is.numeric), names_to = \"Variable\", values_to = \"Value\")\n\n# Create faceted histogram for all continuous variables\nggplot(long_data, aes(x = Value)) + \n  geom_histogram(binwidth =1, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  theme_minimal() +\n  facet_wrap(~ Variable, scales = \"free\") +\n  labs(title = \"Histograms of All Continuous Variables\", x = \"Value\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = total_rainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$total_rainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$total_rainfall, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Total Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest30minRainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Highest30minRainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Highest30minRainfall\n                                                , na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Highest 30min Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest60minRainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Highest60minRainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Highest60minRainfall, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Highest 60min Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest120minRainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Highest120minRainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Highest120minRainfall, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Highest 120min Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Monthly_Elec_consump)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Monthly_Elec_consump, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Monthly_Elec_consump, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Monthly Electricity Consumption\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = denguecases)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$denguecases, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$denguecases, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Dengue cases\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nsummary (combined_Monthly_data)\n\n     Month            Year       denguecases      AvgMeanTemp   \n Min.   : 1.00   Min.   :2014   Min.   : 136.0   Min.   :25.94  \n 1st Qu.: 3.75   1st Qu.:2016   1st Qu.: 399.8   1st Qu.:27.55  \n Median : 6.50   Median :2018   Median : 890.0   Median :28.13  \n Mean   : 6.50   Mean   :2018   Mean   :1228.4   Mean   :28.02  \n 3rd Qu.: 9.25   3rd Qu.:2021   3rd Qu.:1447.2   3rd Qu.:28.56  \n Max.   :12.00   Max.   :2023   Max.   :8001.0   Max.   :29.25  \n    MaxTemp         MinTemp     total_rainfall  Highest30minRainfall\n Min.   :33.30   Min.   :20.0   Min.   :  330   Min.   :17.80       \n 1st Qu.:34.80   1st Qu.:21.6   1st Qu.: 2035   1st Qu.:37.70       \n Median :35.40   Median :22.0   Median : 3202   Median :44.80       \n Mean   :35.33   Mean   :21.9   Mean   : 3960   Mean   :44.28       \n 3rd Qu.:35.70   3rd Qu.:22.3   3rd Qu.: 5830   3rd Qu.:49.65       \n Max.   :37.90   Max.   :23.4   Max.   :13505   Max.   :67.00       \n Highest60minRainfall Highest120minRainfall Monthly_Elec_consump\n Min.   : 20.00       Min.   : 25.40        Min.   :488.7       \n 1st Qu.: 49.60       1st Qu.: 59.35        1st Qu.:594.1       \n Median : 62.70       Median : 69.60        Median :635.6       \n Mean   : 61.22       Mean   : 71.89        Mean   :636.4       \n 3rd Qu.: 71.85       3rd Qu.: 84.45        3rd Qu.:671.1       \n Max.   :102.60       Max.   :122.80        Max.   :885.0       \n Monthly_Elec_per_Household\n Min.   :0.0003859         \n 1st Qu.:0.0004514         \n Median :0.0004821         \n Mean   :0.0004800         \n 3rd Qu.:0.0005080         \n Max.   :0.0006448         \n\n\n\n\n\nCorrelation between the various variables.\n\nggstatsplot::ggcorrmat(\n  data = combined_Monthly_data, \n  cor.vars = 3:12)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = combined_Yearly_data, \n  cor.vars = 2:12)\n\n\n\n\n\n\n\n\n\n\nDistribution of the data\n\n\nRidgelines of variables\n\ncombined_Monthly_data$Year &lt;- as.factor(combined_Monthly_data$Year)\n\nggplot(combined_Monthly_data, \n       aes(x = AvgMeanTemp, \n           y = Year)) +  # Ensure 'Year' is correctly mapped to y-axis\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", 0.3),  # Adjust fill color\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Monthly Mean Temperature\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n# Perform both Shapiro-Wilk and Anderson-Darling tests for each year\nnormality_results &lt;- combined_Monthly_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Shapiro_p = shapiro.test(AvgMeanTemp)$p.value,  # Shapiro-Wilk test p-value\n    Anderson_p = ad.test(AvgMeanTemp)$p.value      # Anderson-Darling test p-value\n  ) %&gt;%\n  mutate(\n    Shapiro_Result = ifelse(Shapiro_p &gt; 0.05, \"Normal\", \"Not Normal\"),  # Interpretation\n    Anderson_Result = ifelse(Anderson_p &gt; 0.05, \"Normal\", \"Not Normal\")  # Interpretation\n  )\n\n# View combined results\nprint(normality_results)\n\n# A tibble: 10 × 5\n   Year  Shapiro_p Anderson_p Shapiro_Result Anderson_Result\n   &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;          \n 1 2014     0.753       0.690 Normal         Normal         \n 2 2015     0.274       0.358 Normal         Normal         \n 3 2016     0.408       0.529 Normal         Normal         \n 4 2017     0.200       0.179 Normal         Normal         \n 5 2018     0.156       0.266 Normal         Normal         \n 6 2019     0.0745      0.137 Normal         Normal         \n 7 2020     0.953       0.920 Normal         Normal         \n 8 2021     0.0612      0.127 Normal         Normal         \n 9 2022     0.889       0.637 Normal         Normal         \n10 2023     0.130       0.102 Normal         Normal         \n\n\n\nggplot(combined_Monthly_data, \n       aes(x = MaxTemp, \n           y = Year)) + \n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", 0.3),  \n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Monthly Max Temperature\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n# Perform both Shapiro-Wilk and Anderson-Darling tests for each year\nnormality_results &lt;- combined_Monthly_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Shapiro_p = shapiro.test(MaxTemp)$p.value,  # Shapiro-Wilk test p-value\n    Anderson_p = ad.test(MaxTemp)$p.value      # Anderson-Darling test p-value\n  ) %&gt;%\n  mutate(\n    Shapiro_Result = ifelse(Shapiro_p &gt; 0.05, \"Normal\", \"Not Normal\"),  # Interpretation\n    Anderson_Result = ifelse(Anderson_p &gt; 0.05, \"Normal\", \"Not Normal\")  # Interpretation\n  )\n\n# View combined results\nprint(normality_results)\n\n# A tibble: 10 × 5\n   Year  Shapiro_p Anderson_p Shapiro_Result Anderson_Result\n   &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;          \n 1 2014      0.770      0.763 Normal         Normal         \n 2 2015      0.499      0.388 Normal         Normal         \n 3 2016      0.696      0.579 Normal         Normal         \n 4 2017      0.915      0.936 Normal         Normal         \n 5 2018      0.988      0.927 Normal         Normal         \n 6 2019      0.530      0.353 Normal         Normal         \n 7 2020      0.878      0.837 Normal         Normal         \n 8 2021      0.300      0.127 Normal         Normal         \n 9 2022      0.801      0.808 Normal         Normal         \n10 2023      0.469      0.403 Normal         Normal         \n\n\n\nggplot(combined_Monthly_data, \n       aes(x = MinTemp, \n           y = Year)) + \n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", 0.3),  \n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Monthly Min Temperature\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n# Perform both Shapiro-Wilk and Anderson-Darling tests for each year\nnormality_results &lt;- combined_Monthly_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Shapiro_p = shapiro.test(MinTemp)$p.value,  # Shapiro-Wilk test p-value\n    Anderson_p = ad.test(MinTemp)$p.value      # Anderson-Darling test p-value\n  ) %&gt;%\n  mutate(\n    Shapiro_Result = ifelse(Shapiro_p &gt; 0.05, \"Normal\", \"Not Normal\"),  # Interpretation\n    Anderson_Result = ifelse(Anderson_p &gt; 0.05, \"Normal\", \"Not Normal\")  # Interpretation\n  )\n\n# View combined results\nprint(normality_results)\n\n# A tibble: 10 × 5\n   Year  Shapiro_p Anderson_p Shapiro_Result Anderson_Result\n   &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;          \n 1 2014     0.0432     0.0659 Not Normal     Normal         \n 2 2015     0.0151     0.0400 Not Normal     Not Normal     \n 3 2016     0.379      0.384  Normal         Normal         \n 4 2017     0.0836     0.125  Normal         Normal         \n 5 2018     0.634      0.673  Normal         Normal         \n 6 2019     0.644      0.586  Normal         Normal         \n 7 2020     0.330      0.361  Normal         Normal         \n 8 2021     0.976      0.848  Normal         Normal         \n 9 2022     0.217      0.135  Normal         Normal         \n10 2023     0.835      0.887  Normal         Normal         \n\n\n\n\nOne-Way Anova\n\ncombined_Monthly_data$total_rainfall &lt;- as.numeric(combined_Monthly_data$total_rainfall)\ncombined_Monthly_data &lt;- combined_Monthly_data %&gt;%\n  filter(!is.na(total_rainfall))\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = total_rainfall, \n  type = \"np\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = MaxTemp, \n  type = \"p\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = denguecases , \n  type = \"Np\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = Monthly_Elec_consump, \n  type = \"np\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\n\nMulti-linear regression\n\ncolnames(combined_Monthly_data)\n\n [1] \"Month\"                      \"Year\"                      \n [3] \"denguecases\"                \"AvgMeanTemp\"               \n [5] \"MaxTemp\"                    \"MinTemp\"                   \n [7] \"total_rainfall\"             \"Highest30minRainfall\"      \n [9] \"Highest60minRainfall\"       \"Highest120minRainfall\"     \n[11] \"Monthly_Elec_consump\"       \"Monthly_Elec_per_Household\"\n\n\n\nmodel_health &lt;- lm(denguecases ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall+ Highest60minRainfall + Highest120minRainfall , data = combined_Monthly_data)\nsummary(model_health)\n\n\nCall:\nlm(formula = denguecases ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest60minRainfall + Highest120minRainfall, \n    data = combined_Monthly_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1686.2  -777.6  -349.0   309.1  6266.0 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)           -4042.043   6810.808  -0.593  0.55405   \nMaxTemp                 -84.736    171.349  -0.495  0.62190   \nMinTemp                 -97.471    192.511  -0.506  0.61362   \nAvgMeanTemp             385.532    180.815   2.132  0.03516 * \nHighest30minRainfall    -70.668     23.995  -2.945  0.00392 **\nHighest60minRainfall     38.498     22.709   1.695  0.09278 . \nHighest120minRainfall     5.116     12.927   0.396  0.69302   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1270 on 113 degrees of freedom\nMultiple R-squared:  0.1037,    Adjusted R-squared:  0.05613 \nF-statistic: 2.179 on 6 and 113 DF,  p-value: 0.05003\n\n\n\ncheck_collinearity(model_health)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                  Term  VIF    VIF 95% CI Increased SE Tolerance\n               MaxTemp 1.21 [1.07,  1.64]         1.10      0.83\n               MinTemp 1.06 [1.00,  2.20]         1.03      0.94\n           AvgMeanTemp 1.26 [1.10,  1.68]         1.12      0.80\n  Highest30minRainfall 4.13 [3.14,  5.59]         2.03      0.24\n Highest120minRainfall 4.83 [3.64,  6.57]         2.20      0.21\n Tolerance 95% CI\n     [0.61, 0.94]\n     [0.45, 1.00]\n     [0.60, 0.91]\n     [0.18, 0.32]\n     [0.15, 0.27]\n\nModerate Correlation\n\n                 Term  VIF    VIF 95% CI Increased SE Tolerance\n Highest60minRainfall 9.19 [6.77, 12.62]         3.03      0.11\n Tolerance 95% CI\n     [0.08, 0.15]\n\n\n\ncheck_c_health &lt;- check_collinearity(model_health)\nplot(check_c_health)\n\n\n\n\n\n\n\n\n\nRemoved 60 mins Rainfll data die to multicollinearity\n\nmodel_health &lt;- lm(denguecases ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall + Highest120minRainfall , data = combined_Monthly_data)\nsummary(model_health)\n\n\nCall:\nlm(formula = denguecases ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest120minRainfall, data = combined_Monthly_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1426.3  -740.9  -317.0   214.7  6515.1 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)           -5341.726   6822.916  -0.783   0.4353  \nMaxTemp                 -63.337    172.282  -0.368   0.7138  \nMinTemp                 -74.587    193.609  -0.385   0.7008  \nAvgMeanTemp             383.419    182.291   2.103   0.0376 *\nHighest30minRainfall    -41.923     17.117  -2.449   0.0158 *\nHighest120minRainfall    21.606      8.584   2.517   0.0132 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1280 on 114 degrees of freedom\nMultiple R-squared:  0.08093,   Adjusted R-squared:  0.04062 \nF-statistic: 2.008 on 5 and 114 DF,  p-value: 0.08278\n\n\n\ncheck_model(model_health)\n\n\n\n\n\n\n\n\n\nmodel_elec &lt;- lm(Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall+ Highest60minRainfall + Highest120minRainfall , data = combined_Monthly_data)\n\nsummary(model_elec)\n\n\nCall:\nlm(formula = Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest60minRainfall + Highest120minRainfall, \n    data = combined_Monthly_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-116.93  -38.01   -5.09   24.84  232.05 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -687.0746   297.7437  -2.308   0.0228 *  \nMaxTemp                -10.1754     7.4907  -1.358   0.1770    \nMinTemp                 14.4465     8.4159   1.717   0.0888 .  \nAvgMeanTemp             46.3414     7.9046   5.863  4.6e-08 ***\nHighest30minRainfall    -0.6473     1.0490  -0.617   0.5384    \nHighest60minRainfall     0.1667     0.9928   0.168   0.8669    \nHighest120minRainfall    1.2033     0.5651   2.129   0.0354 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.51 on 113 degrees of freedom\nMultiple R-squared:  0.3323,    Adjusted R-squared:  0.2969 \nF-statistic: 9.374 on 6 and 113 DF,  p-value: 2.437e-08\n\n\n\ncheck_c_elec &lt;- check_collinearity(model_elec)\nplot(check_c_elec)\n\n\n\n\n\n\n\n\n\nmodel_elec &lt;- lm(Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall + Highest120minRainfall , data = combined_Monthly_data)\n\nsummary(model_elec)\n\n\nCall:\nlm(formula = Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest120minRainfall, data = combined_Monthly_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-117.63  -38.03   -5.61   24.57  233.13 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -692.7030   294.5877  -2.351 0.020418 *  \nMaxTemp                -10.0827     7.4385  -1.355 0.177943    \nMinTemp                 14.5456     8.3593   1.740 0.084550 .  \nAvgMeanTemp             46.3323     7.8706   5.887 4.04e-08 ***\nHighest30minRainfall    -0.5228     0.7390  -0.707 0.480776    \nHighest120minRainfall    1.2747     0.3706   3.439 0.000815 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.28 on 114 degrees of freedom\nMultiple R-squared:  0.3322,    Adjusted R-squared:  0.3029 \nF-statistic: 11.34 on 5 and 114 DF,  p-value: 6.835e-09\n\n\n\ncheck_model(model_elec)\n\n\n\n\n\n\n\n\n\n\n\nDisclaimer\nThis take home exercise was done with reference to methods in Prof Kam’s e text book https://r4va.netlify.app/ , materials on R available online and the use of Microsoft co-pilot to troubleshoot syntax errors and streamline codes."
  },
  {
    "objectID": "Take-home_Exe/Take-home_Ex_03.html#exploratory-data-analysis-eda-and-confirmatory-data-analysis-cda",
    "href": "Take-home_Exe/Take-home_Ex_03.html#exploratory-data-analysis-eda-and-confirmatory-data-analysis-cda",
    "title": "Take Home Exercise 03",
    "section": "Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA)",
    "text": "Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA)\n\nYearly Trend\nTo start of EDA, this exercise used the yearly data to explore the overall trends on the variables. This will be useful to provide the preliminary understanding on how climate change has impacted the local weather parameters, health (represented by dengue cases) and household consumption represented by electricity.\n\nYearly Trends on Temperature\n\n# Reshape the data for columns 3 to 5\nyearly_combined_data &lt;- combined_Yearly_data %&gt;%\n  select(Year, AvgMeanTemp, MaxTemp, MinTemp) %&gt;%\n  pivot_longer(cols = 2:4, names_to = \"Variable\", values_to = \"Value\")  # Convert to long format\n\n# Plot with trend lines for each variable\nggplot(yearly_combined_data, aes(x = Year, y = Value, color = Variable)) +\n  geom_point(alpha = 0.6, size = 3) +  # Scatter plot points\n  geom_smooth(method = \"lm\", linetype = \"dashed\", size = 1) +  # Linear trend line\n  labs(\n    title = \"Yearly Trends in Temperature Variables\",\n    x = \"Year\",\n    y = \"Value\",\n    color = \"Variable\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(combined_Yearly_data, aes(x = Year, y = MaxTemp)) +\n  geom_point(alpha = 0.6, size = 3) +  # Scatter plot points\n  geom_smooth(method = \"lm\", linetype = \"dashed\", size = 1) +  # Linear trend line\n  labs(\n    title = \"Yearly Trends in Max Temperature\",\n    x = \"Year\",\n    y = \"Max Temp\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Yearly_data, aes(x = Year, y = DaysAbove35)) +\n  geom_point(alpha = 0.6, size = 3) +  # Scatter plot points\n  geom_smooth(method = \"lm\", linetype = \"dashed\", size = 1) +  \n  labs(\n    title = \"Yearly Trends in No. of days above 35\",\n    x = \"Year\",\n    y = \"Days Above 35 degree\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nYearly Trends on Rainfall\n\nggplot(combined_Yearly_data, aes(x = Year, y = total_rainfall)) +\n  geom_point(alpha = 0.6, size = 3) +  # Scatter plot points\n  geom_smooth(method = \"lm\", linetype = \"dashed\", size = 1) + \n  labs(\n    title = \"Yearly Trends in Total Rainfall\",\n    x = \"Year\",\n    y = \"Total Rainfall (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Yearly_data, aes(x = Year, y = Highest30minRainfall)) +\n  geom_point(alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\", linetype = \"dashed\", size = 1) + \n  labs(\n    title = \"Yearly Trends in Highest Rainfall in 30 minutes\",\n    x = \"Year\",\n    y = \"Highest 30 min Rainfall\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDistribution of of the data\n\nHistograms\n\nlong_data &lt;- combined_Monthly_data %&gt;%\n  pivot_longer(cols = where(is.numeric), names_to = \"Variable\", values_to = \"Value\")\n\n# Create faceted histogram for all continuous variables\nggplot(long_data, aes(x = Value)) + \n  geom_histogram(binwidth =1, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  theme_minimal() +\n  facet_wrap(~ Variable, scales = \"free\") +\n  labs(title = \"Histograms of All Continuous Variables\", x = \"Value\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = total_rainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$total_rainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$total_rainfall, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Total Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest30minRainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Highest30minRainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Highest30minRainfall\n                                                , na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Highest 30min Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest60minRainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Highest60minRainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Highest60minRainfall, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Highest 60min Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest120minRainfall)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Highest120minRainfall, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Highest120minRainfall, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Highest 120min Rainfall\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Monthly_Elec_consump)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$Monthly_Elec_consump, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$Monthly_Elec_consump, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Monthly Electricity Consumption\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = denguecases)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, args = list(mean = mean(combined_Monthly_data$denguecases, na.rm = TRUE), \n                                         sd = sd(combined_Monthly_data$denguecases, na.rm = TRUE)), \n                color = \"red\", size = 1) +\n  labs(title = \"Histogram with Normal Curve\", x = \"Dengue cases\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nsummary (combined_Monthly_data)\n\n     Month            Year       denguecases      AvgMeanTemp   \n Min.   : 1.00   Min.   :2014   Min.   : 136.0   Min.   :25.94  \n 1st Qu.: 3.75   1st Qu.:2016   1st Qu.: 399.8   1st Qu.:27.55  \n Median : 6.50   Median :2018   Median : 890.0   Median :28.13  \n Mean   : 6.50   Mean   :2018   Mean   :1228.4   Mean   :28.02  \n 3rd Qu.: 9.25   3rd Qu.:2021   3rd Qu.:1447.2   3rd Qu.:28.56  \n Max.   :12.00   Max.   :2023   Max.   :8001.0   Max.   :29.25  \n    MaxTemp         MinTemp     total_rainfall  Highest30minRainfall\n Min.   :33.30   Min.   :20.0   Min.   :  330   Min.   :17.80       \n 1st Qu.:34.80   1st Qu.:21.6   1st Qu.: 2035   1st Qu.:37.70       \n Median :35.40   Median :22.0   Median : 3202   Median :44.80       \n Mean   :35.33   Mean   :21.9   Mean   : 3960   Mean   :44.28       \n 3rd Qu.:35.70   3rd Qu.:22.3   3rd Qu.: 5830   3rd Qu.:49.65       \n Max.   :37.90   Max.   :23.4   Max.   :13505   Max.   :67.00       \n Highest60minRainfall Highest120minRainfall  DaysAbove35    \n Min.   : 20.00       Min.   : 25.40        Min.   : 0.000  \n 1st Qu.: 49.60       1st Qu.: 59.35        1st Qu.: 0.000  \n Median : 62.70       Median : 69.60        Median : 2.000  \n Mean   : 61.22       Mean   : 71.89        Mean   : 7.417  \n 3rd Qu.: 71.85       3rd Qu.: 84.45        3rd Qu.: 6.000  \n Max.   :102.60       Max.   :122.80        Max.   :69.000  \n Monthly_Elec_consump Monthly_Elec_per_Household\n Min.   :488.7        Min.   :0.0003859         \n 1st Qu.:594.1        1st Qu.:0.0004514         \n Median :635.6        Median :0.0004821         \n Mean   :636.4        Mean   :0.0004800         \n 3rd Qu.:671.1        3rd Qu.:0.0005080         \n Max.   :885.0        Max.   :0.0006448         \n\n\n\n\n\nCorrelation between the various variables.\n\nggstatsplot::ggcorrmat(\n  data = combined_Monthly_data, \n  cor.vars = 3:13)\n\n\n\n\n\n\n\n\n\nggstatsplot::ggcorrmat(\n  data = combined_Yearly_data, \n  cor.vars = 2:13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBased on the correlation plot, it is noted for the yearly data there is no correlation between the yearly data across all variables.\n\n\n\n\nDistribution of the data\n\n\nRidge lines of variables\n\ncombined_Monthly_data$Year &lt;- as.factor(combined_Monthly_data$Year)\n\nggplot(combined_Monthly_data, \n       aes(x = AvgMeanTemp, \n           y = Year)) +  # Ensure 'Year' is correctly mapped to y-axis\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", 0.3),  # Adjust fill color\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Monthly Mean Temperature\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n# Perform both Shapiro-Wilk and Anderson-Darling tests for each year\nnormality_results &lt;- combined_Monthly_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Shapiro_p = shapiro.test(AvgMeanTemp)$p.value,  # Shapiro-Wilk test p-value\n    Anderson_p = ad.test(AvgMeanTemp)$p.value      # Anderson-Darling test p-value\n  ) %&gt;%\n  mutate(\n    Shapiro_Result = ifelse(Shapiro_p &gt; 0.05, \"Normal\", \"Not Normal\"),  # Interpretation\n    Anderson_Result = ifelse(Anderson_p &gt; 0.05, \"Normal\", \"Not Normal\")  # Interpretation\n  )\n\n# View combined results\nprint(normality_results)\n\n# A tibble: 10 × 5\n   Year  Shapiro_p Anderson_p Shapiro_Result Anderson_Result\n   &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;          \n 1 2014     0.753       0.690 Normal         Normal         \n 2 2015     0.274       0.358 Normal         Normal         \n 3 2016     0.408       0.529 Normal         Normal         \n 4 2017     0.200       0.179 Normal         Normal         \n 5 2018     0.156       0.266 Normal         Normal         \n 6 2019     0.0745      0.137 Normal         Normal         \n 7 2020     0.953       0.920 Normal         Normal         \n 8 2021     0.0612      0.127 Normal         Normal         \n 9 2022     0.889       0.637 Normal         Normal         \n10 2023     0.130       0.102 Normal         Normal         \n\n\n\nggplot(combined_Monthly_data, \n       aes(x = MaxTemp, \n           y = Year)) + \n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", 0.3),  \n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Monthly Max Temperature\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n# Perform both Shapiro-Wilk and Anderson-Darling tests for each year\nnormality_results &lt;- combined_Monthly_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Shapiro_p = shapiro.test(MaxTemp)$p.value,  # Shapiro-Wilk test p-value\n    Anderson_p = ad.test(MaxTemp)$p.value      # Anderson-Darling test p-value\n  ) %&gt;%\n  mutate(\n    Shapiro_Result = ifelse(Shapiro_p &gt; 0.05, \"Normal\", \"Not Normal\"),  # Interpretation\n    Anderson_Result = ifelse(Anderson_p &gt; 0.05, \"Normal\", \"Not Normal\")  # Interpretation\n  )\n\n# View combined results\nprint(normality_results)\n\n# A tibble: 10 × 5\n   Year  Shapiro_p Anderson_p Shapiro_Result Anderson_Result\n   &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;          \n 1 2014      0.770      0.763 Normal         Normal         \n 2 2015      0.499      0.388 Normal         Normal         \n 3 2016      0.696      0.579 Normal         Normal         \n 4 2017      0.915      0.936 Normal         Normal         \n 5 2018      0.988      0.927 Normal         Normal         \n 6 2019      0.530      0.353 Normal         Normal         \n 7 2020      0.878      0.837 Normal         Normal         \n 8 2021      0.300      0.127 Normal         Normal         \n 9 2022      0.801      0.808 Normal         Normal         \n10 2023      0.469      0.403 Normal         Normal         \n\n\n\nggplot(combined_Monthly_data, \n       aes(x = MinTemp, \n           y = Year)) + \n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", 0.3),  \n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Monthly Min Temperature\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n# Perform both Shapiro-Wilk and Anderson-Darling tests for each year\nnormality_results &lt;- combined_Monthly_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Shapiro_p = shapiro.test(MinTemp)$p.value,  # Shapiro-Wilk test p-value\n    Anderson_p = ad.test(MinTemp)$p.value      # Anderson-Darling test p-value\n  ) %&gt;%\n  mutate(\n    Shapiro_Result = ifelse(Shapiro_p &gt; 0.05, \"Normal\", \"Not Normal\"),  # Interpretation\n    Anderson_Result = ifelse(Anderson_p &gt; 0.05, \"Normal\", \"Not Normal\")  # Interpretation\n  )\n\n# View combined results\nprint(normality_results)\n\n# A tibble: 10 × 5\n   Year  Shapiro_p Anderson_p Shapiro_Result Anderson_Result\n   &lt;fct&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;          \n 1 2014     0.0432     0.0659 Not Normal     Normal         \n 2 2015     0.0151     0.0400 Not Normal     Not Normal     \n 3 2016     0.379      0.384  Normal         Normal         \n 4 2017     0.0836     0.125  Normal         Normal         \n 5 2018     0.634      0.673  Normal         Normal         \n 6 2019     0.644      0.586  Normal         Normal         \n 7 2020     0.330      0.361  Normal         Normal         \n 8 2021     0.976      0.848  Normal         Normal         \n 9 2022     0.217      0.135  Normal         Normal         \n10 2023     0.835      0.887  Normal         Normal         \n\n\n\n\nAnalysis of Variance: Parametric and Non-Parametric Methods\n\ncombined_Monthly_data$total_rainfall &lt;- as.numeric(combined_Monthly_data$total_rainfall)\ncombined_Monthly_data &lt;- combined_Monthly_data %&gt;%\n  filter(!is.na(total_rainfall))\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = total_rainfall, \n  type = \"np\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = MaxTemp, \n  type = \"p\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = DaysAbove35, \n  type = \"np\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = AvgMeanTemp, \n  type = \"p\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = denguecases , \n  type = \"Np\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = combined_Monthly_data,\n  x = Year, \n  y = Monthly_Elec_consump, \n  type = \"np\",  # Non-Parametric ANOVA, Parameteric - p\n  mean.ci = TRUE,  # Display mean and confidence interval\n  pairwise.comparisons = TRUE,  # Perform pairwise tests\n  pairwise.display = \"significant\",  # Show only significant pairs\n  p.adjust.method = \"fdr\",  # Adjust for multiple comparisons\n  messages = FALSE  # Suppress additional messages\n)\n\n\n\n\n\n\n\n\n\n\nMulti-Linear Regression (MLR)\n\nUsing MLR to understand the impact of climate change on health aspect\n\ncolnames(combined_Monthly_data)\n\n [1] \"Month\"                      \"Year\"                      \n [3] \"denguecases\"                \"AvgMeanTemp\"               \n [5] \"MaxTemp\"                    \"MinTemp\"                   \n [7] \"total_rainfall\"             \"Highest30minRainfall\"      \n [9] \"Highest60minRainfall\"       \"Highest120minRainfall\"     \n[11] \"DaysAbove35\"                \"Monthly_Elec_consump\"      \n[13] \"Monthly_Elec_per_Household\"\n\n\n\nmodel_health &lt;- lm(denguecases ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall+ Highest60minRainfall + Highest120minRainfall+DaysAbove35 , data = combined_Monthly_data)\nsummary(model_health)\n\n\nCall:\nlm(formula = denguecases ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest60minRainfall + Highest120minRainfall + \n    DaysAbove35, data = combined_Monthly_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1661.6  -764.4  -337.8   294.8  6276.8 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)           -2316.449   8590.478  -0.270  0.78792   \nMaxTemp                -118.360    199.650  -0.593  0.55449   \nMinTemp                -103.999    194.272  -0.535  0.59349   \nAvgMeanTemp             371.417    186.449   1.992  0.04880 * \nHighest30minRainfall    -70.100     24.151  -2.903  0.00446 **\nHighest60minRainfall     37.304     23.081   1.616  0.10886   \nHighest120minRainfall     5.405     13.007   0.416  0.67855   \nDaysAbove35               3.767     11.352   0.332  0.74063   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1275 on 112 degrees of freedom\nMultiple R-squared:  0.1046,    Adjusted R-squared:  0.04864 \nF-statistic: 1.869 on 7 and 112 DF,  p-value: 0.08121\n\n\n\ncheck_collinearity(model_health)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                  Term  VIF    VIF 95% CI Increased SE Tolerance\n               MaxTemp 1.63 [1.35,  2.13]         1.27      0.62\n               MinTemp 1.07 [1.01,  1.94]         1.04      0.93\n           AvgMeanTemp 1.33 [1.14,  1.75]         1.15      0.75\n  Highest30minRainfall 4.15 [3.16,  5.61]         2.04      0.24\n Highest120minRainfall 4.86 [3.67,  6.58]         2.20      0.21\n           DaysAbove35 1.74 [1.43,  2.28]         1.32      0.57\n Tolerance 95% CI\n     [0.47, 0.74]\n     [0.52, 0.99]\n     [0.57, 0.88]\n     [0.18, 0.32]\n     [0.15, 0.27]\n     [0.44, 0.70]\n\nModerate Correlation\n\n                 Term  VIF    VIF 95% CI Increased SE Tolerance\n Highest60minRainfall 9.42 [6.96, 12.91]         3.07      0.11\n Tolerance 95% CI\n     [0.08, 0.14]\n\n\n\ncheck_c_health &lt;- check_collinearity(model_health)\nplot(check_c_health)\n\n\n\n\n\n\n\n\n\nModel Refinement\nHighest Rainfall in 60 min data was removed due to multi-collinearity.\nBased on the outcome of the initial model, it shows that the increase in dengue cases could not be explained by weather parameters. The correlation plot shows that dengue cases has correlation with electricity consumption.\nHence fo r model refinement, it is added to the model as a variable. The model become significant with highest 30 mins rainfall explaining the increasing trend dengue cases.\n\nmodel_health &lt;- lm(denguecases ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall + Highest120minRainfall + Monthly_Elec_consump + DaysAbove35 , data = combined_Monthly_data)\n\nsummary(model_health)\n\n\nCall:\nlm(formula = denguecases ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest120minRainfall + Monthly_Elec_consump + \n    DaysAbove35, data = combined_Monthly_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1874.8  -725.0  -145.9   490.2  5013.7 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           5215.235   8170.506   0.638   0.5246    \nMaxTemp                -55.516    186.527  -0.298   0.7665    \nMinTemp               -223.714    183.207  -1.221   0.2246    \nAvgMeanTemp            -69.709    198.007  -0.352   0.7255    \nHighest30minRainfall   -37.981     15.931  -2.384   0.0188 *  \nHighest120minRainfall    9.539      8.397   1.136   0.2584    \nMonthly_Elec_consump     9.043      2.015   4.489 1.75e-05 ***\nDaysAbove35              9.158     10.458   0.876   0.3831    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1187 on 112 degrees of freedom\nMultiple R-squared:  0.2234,    Adjusted R-squared:  0.1749 \nF-statistic: 4.603 on 7 and 112 DF,  p-value: 0.0001488\n\n\n\ncheck_model(model_health)\n\n\n\n\n\n\n\n\n\n\n\nUsing MLR to understand the impact of climate change on electricity consumption\n\nmodel_elec &lt;- lm(Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall+ Highest60minRainfall + Highest120minRainfall+DaysAbove35, data = combined_Monthly_data)\n\nsummary(model_elec)\n\n\nCall:\nlm(formula = Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest60minRainfall + Highest120minRainfall + \n    DaysAbove35, data = combined_Monthly_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-117.907  -39.296   -5.111   25.925  231.190 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -824.4679   375.1164  -2.198   0.0300 *  \nMaxTemp                 -7.4983     8.7180  -0.860   0.3916    \nMinTemp                 14.9662     8.4832   1.764   0.0804 .  \nAvgMeanTemp             47.4653     8.1416   5.830 5.44e-08 ***\nHighest30minRainfall    -0.6925     1.0546  -0.657   0.5127    \nHighest60minRainfall     0.2618     1.0079   0.260   0.7955    \nHighest120minRainfall    1.1803     0.5680   2.078   0.0400 *  \nDaysAbove35             -0.2999     0.4957  -0.605   0.5464    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.67 on 112 degrees of freedom\nMultiple R-squared:  0.3345,    Adjusted R-squared:  0.2929 \nF-statistic: 8.042 on 7 and 112 DF,  p-value: 6.72e-08\n\n\n\ncheck_c_elec &lt;- check_collinearity(model_elec)\nplot(check_c_elec)\n\n\n\n\n\n\n\n\n\nmodel_elec &lt;- lm(Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp+ Highest30minRainfall + Highest120minRainfall + DaysAbove35 , data = combined_Monthly_data)\n\nsummary(model_elec)\n\n\nCall:\nlm(formula = Monthly_Elec_consump ~ MaxTemp + MinTemp + AvgMeanTemp + \n    Highest30minRainfall + Highest120minRainfall + DaysAbove35, \n    data = combined_Monthly_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-118.914  -39.111   -6.049   25.826  232.900 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -823.8967   373.5590  -2.206 0.029441 *  \nMaxTemp                 -7.5354     8.6808  -0.868 0.387203    \nMinTemp                 15.0833     8.4362   1.788 0.076468 .  \nAvgMeanTemp             47.3761     8.1007   5.848 4.91e-08 ***\nHighest30minRainfall    -0.4988     0.7424  -0.672 0.503054    \nHighest120minRainfall    1.2912     0.3728   3.463 0.000755 ***\nDaysAbove35             -0.2799     0.4876  -0.574 0.567149    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.44 on 113 degrees of freedom\nMultiple R-squared:  0.3341,    Adjusted R-squared:  0.2988 \nF-statistic:  9.45 on 6 and 113 DF,  p-value: 2.118e-08\n\n\n\ncheck_model(model_elec)\n\n\n\n\n\n\n\n\n\n\n\nConclusion and further visualization\nAlthough the MLR models may not be able to explain fully on the increase in the dengue cases and household electricity consumption over the year using the weather parameters, but the MLR models met the requirements for this exercise. The use of MLR technique is to use the weather parameters to explore if there were trends between changing weather patterns and its impact on our daily life.\n\nClimate Change on Health\nUsing the outcome of the 1st MLR model on health, although the P-value is at 0.05 which close to the conventional threshold of 0.05, this indicate the model is marginally significant, and suggest that the weather parameters used as a group does not provide a strong explanation of explaining the increase in dengue cases over the recent years.\nHowever it is important to note that among the weather parameters, Average Mean Temperature for the month and the highest 30 min rainfall are statically significant in the first model and only the highest rainfall 30 minutes is still statically significant for explaning the dengue trend after adding in the monthly total electricity consumption.\n\nggplot(combined_Monthly_data, aes(x = AvgMeanTemp, y = denguecases)) +\n  geom_point(color = \"blue\", alpha = 0.6, size = 3) +  # Scatter plot with points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Linear trend line\n  labs(\n    title = \"Relationship Between Dengue Cases \\nand Average Mean Temperature \",\n    x = \"Average Daily Mean Temperature (Monthly)\",\n    y = \"Dengue Cases\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest30minRainfall, y = denguecases)) +\n  geom_point(color = \"blue\", alpha = 0.6, size = 3) +  # Scatter plot with points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Linear trend line\n  labs(\n    title = \"Relationship Between Dengue Cases \\nand Highest 30 min Rainfall \",\n    x = \"Highest 30 min Rainfall\",\n    y = \"Dengue Cases\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Monthly_Elec_consump, y = denguecases)) +\n  geom_point(color = \"blue\", alpha = 0.6, size = 3) +  # Scatter plot with points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Linear trend line\n  labs(\n    title = \"Relationship Between Dengue Cases \\nand Electricity Consumption\",\n    x = \"Monthly Household Electricity Consumption\",\n    y = \"Dengue Cases\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nClimate Change on Electricity Consumption\nBased on the MLR regression models built for the Household Electricity Consumption, the statistically significant variables are Average Mean Temperature, Highest 120 min Rainfall and Minimum Temperature. Hence the following visualization is used to show their relationship.\n\nggplot(combined_Monthly_data, aes(x = AvgMeanTemp, y = Monthly_Elec_consump)) +\n  geom_point(color = \"blue\", alpha = 0.6, size = 3) +  # Scatter plot with points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Linear trend line\n  labs(\n    title = \"Relationship Between Electricity Consumption \\nand Average Mean Temperature \",\n    x = \"Average Daily Mean Temperature (Monthly)\",\n    y = \"Monthly Household Electricity Consumption\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = Highest120minRainfall, y = Monthly_Elec_consump)) +\n  geom_point(color = \"blue\", alpha = 0.6, size = 3) +  # Scatter plot with points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Linear trend line\n  labs(\n    title = \"Relationship Between Electricity Consumption \\nand Highest 120 min Rainfall\",\n    x = \"Highest 120 min Rainfall\",\n    y = \"Monthly Household Electricity Consumption\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(combined_Monthly_data, aes(x = MinTemp, y = Monthly_Elec_consump)) +\n  geom_point(color = \"blue\", alpha = 0.6, size = 3) +  # Scatter plot with points\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") +  # Linear trend line\n  labs(\n    title = \"Relationship Between Electricity Consumption \\nand Minimum Temperature\",\n    x = \"Minimum Temperature\",\n    y = \"Monthly Household Electricity Consumption\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nDisclaimer\nThis take home exercise was done with reference to methods in Prof Kam’s e textbook https://r4va.netlify.app/ , materials on R available online and the use of Microsoft co-pilot to troubleshoot syntax errors and streamline codes."
  }
]